{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uuid import uuid4\n",
    "import json\n",
    "import jmespath\n",
    "import weaviate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open json file from root folder\n",
    "with open('../___schema.json') as f:\n",
    "    schema_obj = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'class': 'Dataset', 'description': 'AWS Open Registry Dataset'},\n",
       " {'class': 'Publisher',\n",
       "  'description': 'Organization that publishes the dataset'},\n",
       " {'class': 'Tag', 'description': 'Related topics'},\n",
       " {'class': 'Resource', 'description': 'Dataset resources'},\n",
       " {'class': 'Tutorial',\n",
       "  'description': 'Tutorials for how to analyze the dataset'},\n",
       " {'class': 'Publication',\n",
       "  'description': 'Publications that mention the dataset'},\n",
       " {'class': 'ToolOrApplication',\n",
       "  'description': 'Tools or applications that use the dataset'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = [ { k:v for k, v in clss.items() if k != \"properties\" } for clss in schema_obj['classes'] ]\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://52.3.229.64:8080\n"
     ]
    },
    {
     "ename": "ConnectTimeout",
     "evalue": "HTTPConnectionPool(host='52.3.229.64', port=8080): Max retries exceeded with url: /v1/.well-known/openid-configuration (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x110f83310>, 'Connection to 52.3.229.64 timed out. (connect timeout=10)'))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/aws-open-data-registry-neural-search-4l16sXU8-py3.10/lib/python3.10/site-packages/urllib3/connection.py:174\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 174\u001b[0m     conn \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39;49mcreate_connection(\n\u001b[1;32m    175\u001b[0m         (\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dns_host, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mport), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mextra_kw\n\u001b[1;32m    176\u001b[0m     )\n\u001b[1;32m    178\u001b[0m \u001b[39mexcept\u001b[39;00m SocketTimeout:\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/aws-open-data-registry-neural-search-4l16sXU8-py3.10/lib/python3.10/site-packages/urllib3/util/connection.py:95\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[39mif\u001b[39;00m err \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 95\u001b[0m     \u001b[39mraise\u001b[39;00m err\n\u001b[1;32m     97\u001b[0m \u001b[39mraise\u001b[39;00m socket\u001b[39m.\u001b[39merror(\u001b[39m\"\u001b[39m\u001b[39mgetaddrinfo returns an empty list\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/aws-open-data-registry-neural-search-4l16sXU8-py3.10/lib/python3.10/site-packages/urllib3/util/connection.py:85\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     84\u001b[0m     sock\u001b[39m.\u001b[39mbind(source_address)\n\u001b[0;32m---> 85\u001b[0m sock\u001b[39m.\u001b[39;49mconnect(sa)\n\u001b[1;32m     86\u001b[0m \u001b[39mreturn\u001b[39;00m sock\n",
      "\u001b[0;31mTimeoutError\u001b[0m: timed out",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectTimeoutError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/aws-open-data-registry-neural-search-4l16sXU8-py3.10/lib/python3.10/site-packages/urllib3/connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    704\u001b[0m     conn,\n\u001b[1;32m    705\u001b[0m     method,\n\u001b[1;32m    706\u001b[0m     url,\n\u001b[1;32m    707\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[1;32m    708\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    709\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    710\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    711\u001b[0m )\n\u001b[1;32m    713\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    714\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    715\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    716\u001b[0m \u001b[39m# mess.\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/aws-open-data-registry-neural-search-4l16sXU8-py3.10/lib/python3.10/site-packages/urllib3/connectionpool.py:398\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    397\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 398\u001b[0m         conn\u001b[39m.\u001b[39;49mrequest(method, url, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhttplib_request_kw)\n\u001b[1;32m    400\u001b[0m \u001b[39m# We are swallowing BrokenPipeError (errno.EPIPE) since the server is\u001b[39;00m\n\u001b[1;32m    401\u001b[0m \u001b[39m# legitimately able to close the connection after sending a valid response.\u001b[39;00m\n\u001b[1;32m    402\u001b[0m \u001b[39m# With this behaviour, the received response is still readable.\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/aws-open-data-registry-neural-search-4l16sXU8-py3.10/lib/python3.10/site-packages/urllib3/connection.py:239\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[0;34m(self, method, url, body, headers)\u001b[0m\n\u001b[1;32m    238\u001b[0m     headers[\u001b[39m\"\u001b[39m\u001b[39mUser-Agent\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m _get_default_user_agent()\n\u001b[0;32m--> 239\u001b[0m \u001b[39msuper\u001b[39;49m(HTTPConnection, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mrequest(method, url, body\u001b[39m=\u001b[39;49mbody, headers\u001b[39m=\u001b[39;49mheaders)\n",
      "File \u001b[0;32m/usr/local/Cellar/python@3.10/3.10.8/Frameworks/Python.framework/Versions/3.10/lib/python3.10/http/client.py:1282\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1281\u001b[0m \u001b[39m\"\"\"Send a complete request to the server.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1282\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_request(method, url, body, headers, encode_chunked)\n",
      "File \u001b[0;32m/usr/local/Cellar/python@3.10/3.10.8/Frameworks/Python.framework/Versions/3.10/lib/python3.10/http/client.py:1328\u001b[0m, in \u001b[0;36mHTTPConnection._send_request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1327\u001b[0m     body \u001b[39m=\u001b[39m _encode(body, \u001b[39m'\u001b[39m\u001b[39mbody\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m-> 1328\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mendheaders(body, encode_chunked\u001b[39m=\u001b[39;49mencode_chunked)\n",
      "File \u001b[0;32m/usr/local/Cellar/python@3.10/3.10.8/Frameworks/Python.framework/Versions/3.10/lib/python3.10/http/client.py:1277\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1276\u001b[0m     \u001b[39mraise\u001b[39;00m CannotSendHeader()\n\u001b[0;32m-> 1277\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_output(message_body, encode_chunked\u001b[39m=\u001b[39;49mencode_chunked)\n",
      "File \u001b[0;32m/usr/local/Cellar/python@3.10/3.10.8/Frameworks/Python.framework/Versions/3.10/lib/python3.10/http/client.py:1037\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1036\u001b[0m \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_buffer[:]\n\u001b[0;32m-> 1037\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(msg)\n\u001b[1;32m   1039\u001b[0m \u001b[39mif\u001b[39;00m message_body \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1040\u001b[0m \n\u001b[1;32m   1041\u001b[0m     \u001b[39m# create a consistent interface to message_body\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/Cellar/python@3.10/3.10.8/Frameworks/Python.framework/Versions/3.10/lib/python3.10/http/client.py:975\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    974\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_open:\n\u001b[0;32m--> 975\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconnect()\n\u001b[1;32m    976\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/aws-open-data-registry-neural-search-4l16sXU8-py3.10/lib/python3.10/site-packages/urllib3/connection.py:205\u001b[0m, in \u001b[0;36mHTTPConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconnect\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 205\u001b[0m     conn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_new_conn()\n\u001b[1;32m    206\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_conn(conn)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/aws-open-data-registry-neural-search-4l16sXU8-py3.10/lib/python3.10/site-packages/urllib3/connection.py:179\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[39mexcept\u001b[39;00m SocketTimeout:\n\u001b[0;32m--> 179\u001b[0m     \u001b[39mraise\u001b[39;00m ConnectTimeoutError(\n\u001b[1;32m    180\u001b[0m         \u001b[39mself\u001b[39m,\n\u001b[1;32m    181\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mConnection to \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m timed out. (connect timeout=\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    182\u001b[0m         \u001b[39m%\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhost, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtimeout),\n\u001b[1;32m    183\u001b[0m     )\n\u001b[1;32m    185\u001b[0m \u001b[39mexcept\u001b[39;00m SocketError \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[0;31mConnectTimeoutError\u001b[0m: (<urllib3.connection.HTTPConnection object at 0x110f83310>, 'Connection to 52.3.229.64 timed out. (connect timeout=10)')",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/aws-open-data-registry-neural-search-4l16sXU8-py3.10/lib/python3.10/site-packages/requests/adapters.py:489\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m chunked:\n\u001b[0;32m--> 489\u001b[0m     resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    490\u001b[0m         method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[1;32m    491\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    492\u001b[0m         body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[1;32m    493\u001b[0m         headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    494\u001b[0m         redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    495\u001b[0m         assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    496\u001b[0m         preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    497\u001b[0m         decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    498\u001b[0m         retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[1;32m    499\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    500\u001b[0m     )\n\u001b[1;32m    502\u001b[0m \u001b[39m# Send the request.\u001b[39;00m\n\u001b[1;32m    503\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/aws-open-data-registry-neural-search-4l16sXU8-py3.10/lib/python3.10/site-packages/urllib3/connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    785\u001b[0m     e \u001b[39m=\u001b[39m ProtocolError(\u001b[39m\"\u001b[39m\u001b[39mConnection aborted.\u001b[39m\u001b[39m\"\u001b[39m, e)\n\u001b[0;32m--> 787\u001b[0m retries \u001b[39m=\u001b[39m retries\u001b[39m.\u001b[39;49mincrement(\n\u001b[1;32m    788\u001b[0m     method, url, error\u001b[39m=\u001b[39;49me, _pool\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m, _stacktrace\u001b[39m=\u001b[39;49msys\u001b[39m.\u001b[39;49mexc_info()[\u001b[39m2\u001b[39;49m]\n\u001b[1;32m    789\u001b[0m )\n\u001b[1;32m    790\u001b[0m retries\u001b[39m.\u001b[39msleep()\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/aws-open-data-registry-neural-search-4l16sXU8-py3.10/lib/python3.10/site-packages/urllib3/util/retry.py:592\u001b[0m, in \u001b[0;36mRetry.increment\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[39mif\u001b[39;00m new_retry\u001b[39m.\u001b[39mis_exhausted():\n\u001b[0;32m--> 592\u001b[0m     \u001b[39mraise\u001b[39;00m MaxRetryError(_pool, url, error \u001b[39mor\u001b[39;00m ResponseError(cause))\n\u001b[1;32m    594\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mIncremented Retry for (url=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m): \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m, url, new_retry)\n",
      "\u001b[0;31mMaxRetryError\u001b[0m: HTTPConnectionPool(host='52.3.229.64', port=8080): Max retries exceeded with url: /v1/.well-known/openid-configuration (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x110f83310>, 'Connection to 52.3.229.64 timed out. (connect timeout=10)'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectTimeout\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m uri \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhttp://\u001b[39m\u001b[39m{\u001b[39;00mhost\u001b[39m}\u001b[39;00m\u001b[39m:8080\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[39mprint\u001b[39m(uri)\n\u001b[0;32m----> 5\u001b[0m client \u001b[39m=\u001b[39m weaviate\u001b[39m.\u001b[39;49mClient(uri)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/aws-open-data-registry-neural-search-4l16sXU8-py3.10/lib/python3.10/site-packages/weaviate/client.py:121\u001b[0m, in \u001b[0;36mClient.__init__\u001b[0;34m(self, url, auth_client_secret, timeout_config, proxies, trust_env, additional_headers)\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    117\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mURL is expected to be string but is \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(url)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    118\u001b[0m     )\n\u001b[1;32m    119\u001b[0m url \u001b[39m=\u001b[39m url\u001b[39m.\u001b[39mstrip(\u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 121\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_connection \u001b[39m=\u001b[39m Connection(\n\u001b[1;32m    122\u001b[0m     url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    123\u001b[0m     auth_client_secret\u001b[39m=\u001b[39;49mauth_client_secret,\n\u001b[1;32m    124\u001b[0m     timeout_config\u001b[39m=\u001b[39;49mtimeout_config,\n\u001b[1;32m    125\u001b[0m     proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    126\u001b[0m     trust_env\u001b[39m=\u001b[39;49mtrust_env,\n\u001b[1;32m    127\u001b[0m     additional_headers\u001b[39m=\u001b[39;49madditional_headers,\n\u001b[1;32m    128\u001b[0m )\n\u001b[1;32m    129\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclassification \u001b[39m=\u001b[39m Classification(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_connection)\n\u001b[1;32m    130\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mschema \u001b[39m=\u001b[39m Schema(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_connection)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/aws-open-data-registry-neural-search-4l16sXU8-py3.10/lib/python3.10/site-packages/weaviate/connect/connection.py:89\u001b[0m, in \u001b[0;36mConnection.__init__\u001b[0;34m(self, url, auth_client_secret, timeout_config, proxies, trust_env, additional_headers)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_proxies \u001b[39m=\u001b[39m _get_proxies(proxies, trust_env)\n\u001b[1;32m     88\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mauthorization\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_headers:\n\u001b[0;32m---> 89\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_log_in()\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/aws-open-data-registry-neural-search-4l16sXU8-py3.10/lib/python3.10/site-packages/weaviate/connect/connection.py:102\u001b[0m, in \u001b[0;36mConnection._log_in\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_log_in\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     92\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[39m    Log in to the Weaviate server only if the Weaviate server has an OpenID configured.\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[39m        configured.\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 102\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_session\u001b[39m.\u001b[39;49mget(\n\u001b[1;32m    103\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49murl \u001b[39m+\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_api_version_path \u001b[39m+\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39m/.well-known/openid-configuration\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    104\u001b[0m         headers\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mcontent-type\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mapplication/json\u001b[39;49m\u001b[39m\"\u001b[39;49m},\n\u001b[1;32m    105\u001b[0m         timeout\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_timeout_config,\n\u001b[1;32m    106\u001b[0m         proxies\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_proxies,\n\u001b[1;32m    107\u001b[0m     )\n\u001b[1;32m    109\u001b[0m     \u001b[39mif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39m==\u001b[39m \u001b[39m200\u001b[39m:\n\u001b[1;32m    110\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_auth_client_secret, AuthCredentials):\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/aws-open-data-registry-neural-search-4l16sXU8-py3.10/lib/python3.10/site-packages/requests/sessions.py:600\u001b[0m, in \u001b[0;36mSession.get\u001b[0;34m(self, url, **kwargs)\u001b[0m\n\u001b[1;32m    592\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"Sends a GET request. Returns :class:`Response` object.\u001b[39;00m\n\u001b[1;32m    593\u001b[0m \n\u001b[1;32m    594\u001b[0m \u001b[39m:param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m    595\u001b[0m \u001b[39m:param \\*\\*kwargs: Optional arguments that ``request`` takes.\u001b[39;00m\n\u001b[1;32m    596\u001b[0m \u001b[39m:rtype: requests.Response\u001b[39;00m\n\u001b[1;32m    597\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    599\u001b[0m kwargs\u001b[39m.\u001b[39msetdefault(\u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m--> 600\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest(\u001b[39m\"\u001b[39;49m\u001b[39mGET\u001b[39;49m\u001b[39m\"\u001b[39;49m, url, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/aws-open-data-registry-neural-search-4l16sXU8-py3.10/lib/python3.10/site-packages/requests/sessions.py:587\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    582\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m    583\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[1;32m    584\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    585\u001b[0m }\n\u001b[1;32m    586\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 587\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[1;32m    589\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/aws-open-data-registry-neural-search-4l16sXU8-py3.10/lib/python3.10/site-packages/requests/sessions.py:701\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    698\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[1;32m    700\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    703\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    704\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/aws-open-data-registry-neural-search-4l16sXU8-py3.10/lib/python3.10/site-packages/requests/adapters.py:553\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(e\u001b[39m.\u001b[39mreason, ConnectTimeoutError):\n\u001b[1;32m    551\u001b[0m     \u001b[39m# TODO: Remove this in 3.0.0: see #2811\u001b[39;00m\n\u001b[1;32m    552\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(e\u001b[39m.\u001b[39mreason, NewConnectionError):\n\u001b[0;32m--> 553\u001b[0m         \u001b[39mraise\u001b[39;00m ConnectTimeout(e, request\u001b[39m=\u001b[39mrequest)\n\u001b[1;32m    555\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(e\u001b[39m.\u001b[39mreason, ResponseError):\n\u001b[1;32m    556\u001b[0m     \u001b[39mraise\u001b[39;00m RetryError(e, request\u001b[39m=\u001b[39mrequest)\n",
      "\u001b[0;31mConnectTimeout\u001b[0m: HTTPConnectionPool(host='52.3.229.64', port=8080): Max retries exceeded with url: /v1/.well-known/openid-configuration (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x110f83310>, 'Connection to 52.3.229.64 timed out. (connect timeout=10)'))"
     ]
    }
   ],
   "source": [
    "# create client; takes some time to start up\n",
    "host = \"52.3.229.64\"\n",
    "uri = f\"http://{host}:8080\"\n",
    "print(uri)\n",
    "client = weaviate.Client(uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_schema_obj = {\n",
    "    \"classes\": [ { k:v for k, v in clss.items() if k != \"properties\" } for clss in schema_obj['classes'] ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # delete existing schema\n",
    "# client.schema.delete_all()\n",
    "\n",
    "client.schema.create(full_schema_obj)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Database"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the data\n",
    "\n",
    "Every JSON value in a record has these possible datatypes:\n",
    "* primitive &rarr; 'field'\n",
    "* object &rarr; 'field.sub-field'\n",
    "* array of primitives &rarr; 'field[]'\n",
    "* array of objects &rarr; 'field[].sub-field'\n",
    "\n",
    "The mapping for a primitive object is easy:\n",
    "\"to_field\": \"field\"\n",
    "\n",
    "```python\n",
    "is_primitive = type(field) in [str, int, float, bool]\n",
    "is_object = type(field) == dict # named collection of objects, no arrays\n",
    "is_primitive_array = type(field) == list and len(field) > 0 and type(field[0]) in [str, int, float, bool] # collection of primitives\n",
    "is_object_array = type(field) == list and len(field) > 0 and type(field[0]) == dict # collection of objects\n",
    "```\n",
    "\n",
    "Weaviate schema for class objects:\n",
    "```python\n",
    "{\n",
    "    \"field\": \"value\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ready\n",
    "def from_object_or_primitive(class_name, data, map):\n",
    "    return [ { \n",
    "        \"id\": str(uuid4()),\n",
    "        \"class\": class_name,\n",
    "        \"data\": {\n",
    "        k: jmespath.search(v, data) for k, v in map.items() \n",
    "    }\n",
    "} ]\n",
    "\n",
    "# ready\n",
    "def from_primitive_array(class_name, data, map):\n",
    "    return [ { \n",
    "        \"id\": str(uuid4()), \n",
    "        \"class\": class_name,\n",
    "        \"data\": { k: i }\n",
    "    } for k, v in map.items() for i in jmespath.search(v, data) ]\n",
    "\n",
    "# works, maybe ready\n",
    "def from_object_array(class_name, data, key, map):\n",
    "    return [ {\n",
    "        \"id\": str(uuid4()), \n",
    "        \"class\": class_name, \n",
    "        \"data\": {\n",
    "            k: jmespath.search(v, item) for k, v in map.items() \n",
    "        }\n",
    "    } for item in data[key] ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../___data/output/aws-odr.json') as f:\n",
    "    data_objs = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Name': '1940 Census Population Schedules, Enumeration District Maps, and Enumeration District Descriptions',\n",
       " 'Description': 'The 1940 Census population schedules were created by the Bureau of the Census in an attempt to enumerate every person living in the United States on April 1, 1940, although some persons were missed. The 1940 census population schedules were digitized by the National Archives and Records Administration (NARA) and released publicly on April 2, 2012.\\nThe 1940 Census enumeration district maps contain maps of counties, cities, and other minor civil divisions that show enumeration districts, census tracts, and related boundaries and numbers used for each census. The coverage is nation wide and includes territorial areas.\\nThe 1940 Census enumeration district descriptions contain written descriptions of census districts, subdivisions, and enumeration districts.\\n',\n",
       " 'Documentation': 'https://www.archives.gov/developer/1940-census',\n",
       " 'Contact': 'public.dataset.program@nara.gov',\n",
       " 'ManagedBy': 'National Archives and Records Administration (NARA)',\n",
       " 'UpdateFrequency': 'Not updated',\n",
       " 'Collabs': {'ASDI': {'Tags': ['socioeconomic']}},\n",
       " 'Tags': ['nara',\n",
       "  'census',\n",
       "  'archives',\n",
       "  '1940 census',\n",
       "  'demography',\n",
       "  'aws-pds'],\n",
       " 'License': 'US Government work',\n",
       " 'Resources': [{'Description': '1940 Census',\n",
       "   'ARN': 'arn:aws:s3:::nara-1940-census',\n",
       "   'Region': 'us-east-2',\n",
       "   'Type': 'S3 Bucket'}],\n",
       " 'DataAtWork': {'Tutorials': [{'Title': '1940 Census on the AWS Registry of Open Data',\n",
       "    'URL': 'https://www.archives.gov/developer/1940-census',\n",
       "    'AuthorName': 'National Archives and Records Administration'}],\n",
       "  'Tools & Applications': [{'Title': 'National Archives 1940 Census',\n",
       "    'URL': 'https://1940census.archives.gov',\n",
       "    'AuthorName': 'National Archives and Records Administration',\n",
       "    'AuthorURL': 'https://www.archives.gov'}]}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data_objs[2]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'title': '1940 Census on the AWS Registry of Open Data', 'url': 'https://www.archives.gov/developer/1940-census', 'authorName': None, 'authorUrl': None}]\n"
     ]
    }
   ],
   "source": [
    "expr = 'map(&{title: @.Title, url: @.URL, authorName: @.Author.Name, authorUrl: @.Author.URL}, DataAtWork.Tutorials)'\n",
    "print(jmespath.search(expr, data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'Open City Model (OCM)', 'desc': 'Open City Model is an initiative to provide cityGML data for all the buildings in the United States.\\nBy using other open datasets in conjunction with our own code and algorithms it is our goal to provide 3D geometries for every US building.\\n'}]\n",
      "[{'name': 'aws-pds'}, {'name': 'events'}, {'name': 'cities'}, {'name': 'geospatial'}]\n",
      "[{'arn': 'arn:aws:s3:::opencitymodel', 'region': None}]\n"
     ]
    }
   ],
   "source": [
    "# values\n",
    "expr = \"[{name: Name, desc: Description}]\"\n",
    "print(jmespath.search(expr, data))\n",
    "\n",
    "# list of values\n",
    "expr = \"map(&{name: @}, Tags)\"\n",
    "print(jmespath.search(expr, data))\n",
    "\n",
    "# list of objects\n",
    "expr = \"map(&{arn: @.ARN, region: @.region}, Resources)\"\n",
    "print(jmespath.search(expr, data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '<uuid>', 'class': 'class1_name', 'data': {'property': 'value'}}"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mapping\n",
    "{\n",
    "    # option 1: rename object fields (works for object or object array)\n",
    "    \"class1_name\": {\n",
    "        \"object_property\": \"json_field_value\",\n",
    "    },\n",
    "    # option 2: keep field names (only works for values object)\n",
    "    \"class2_name\": \"json_field_array\"\n",
    "}\n",
    "\n",
    "# weaviate object\n",
    "{\n",
    "    \"id\": \"<uuid>\",\n",
    "    \"class\": \"class1_name\",\n",
    "    \"data\": {\n",
    "        \"property\": \"value\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mappings = {\n",
    "    \"Dataset\": {\n",
    "        \"name\" : \"Name\",\n",
    "        \"description\" : \"Description\",\n",
    "        \"documentation\" : \"Documentation\",\n",
    "        \"updateFrequency\" : \"UpdateFrequency\",\n",
    "        \"license\" : \"License\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "expr_map = {\n",
    "    \"values\": \"[{name: Name, desc: Description}]\",\n",
    "    \"list_of_values\": \"map(&{name: @}, Tags)\",\n",
    "    \"list_of_objects\": \"map(&{arn: @.ARN, region: @.region}, Resources)\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_weaviate_object(class_name, expr, data):\n",
    "    return [{\n",
    "        \"id\": str(uuid4()),\n",
    "        \"class\": class_name,\n",
    "        \"data\": item \n",
    "    } for item in jmespath.search(expr, data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': '5c972002-f671-4daa-9800-77399df518ef', 'class': 'Dataset', 'data': {'name': 'Open City Model (OCM)', 'desc': 'Open City Model is an initiative to provide cityGML data for all the buildings in the United States.\\nBy using other open datasets in conjunction with our own code and algorithms it is our goal to provide 3D geometries for every US building.\\n'}}] \n",
      "\n",
      "[{'id': '96c51555-f0a2-417c-8dc1-c3e1b3ee43ef', 'class': 'Tag', 'data': {'name': 'aws-pds'}}, {'id': '899ddf7b-85a4-4254-ac4a-b76ef575d59f', 'class': 'Tag', 'data': {'name': 'events'}}, {'id': 'db380e40-df47-4989-a1fa-cca2c0b4a40e', 'class': 'Tag', 'data': {'name': 'cities'}}, {'id': '4364e4a6-01e2-49c3-997c-2b8b2740a3f4', 'class': 'Tag', 'data': {'name': 'geospatial'}}] \n",
      "\n",
      "[{'id': '10d1655a-d1d3-44e2-baae-be28fbe3d416', 'class': 'Resource', 'data': {'arn': 'arn:aws:s3:::opencitymodel', 'region': None}}] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for class_name, expr in zip([\"Dataset\", \"Tag\", \"Resource\"], expr_map.values()):\n",
    "    print(build_weaviate_object(class_name, expr, data), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'aws-pds'}, {'name': 'events'}, {'name': 'cities'}, {'name': 'geospatial'}]\n"
     ]
    }
   ],
   "source": [
    "import jmespath\n",
    "\n",
    "data = {'Tags': ['aws-pds', 'events', 'cities', 'geospatial']}\n",
    "\n",
    "expression = \"map(&{name: @}, Tags)\"\n",
    "result = jmespath.search(expression, data)\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Tags': ['aws-pds', 'events', 'cities', 'geospatial']}"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# {\"Tags\": [ {\"name\": item } for item in  jmespath.search(\"Tags[]\", data)]}\n",
    "{ \"Tags\": jmespath.search(\"Tags[]\", data)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping structure:\n",
    "# \"output_field_name\": \"jmespath_expression\"\n",
    "\n",
    "payload = {}\n",
    "\n",
    "# dataset\n",
    "dataset_map = {\n",
    "    \"name\" : \"Name\",\n",
    "    \"description\" : \"Description\",\n",
    "    \"documentation\" : \"Documentation\",\n",
    "    \"updateFrequency\" : \"UpdateFrequency\",\n",
    "    \"license\" : \"License\"\n",
    "}\n",
    "\n",
    "payload[\"Dataset\"] = from_object_or_primitive(\"Dataset\", data, dataset_map)\n",
    "\n",
    "# publisher\n",
    "publisher_map = {\n",
    "    \"name\" : \"Name\",\n",
    "    \"contact\": \"Contact\",\n",
    "}\n",
    "payload[\"Publisher\"] = from_object_or_primitive(\"Publisher\", data, publisher_map)\n",
    "\n",
    "# resources\n",
    "resources_object_array_map = {\n",
    "    \"arn\": \"ARN\",\n",
    "    \"type\": \"Type\",\n",
    "    \"region\": \"Region\",\n",
    "    \"description\": \"Description\",\n",
    "}\n",
    "payload[\"Resource\"] = from_object_array(\"Resource\", data, \"Resources\", resources_object_array_map)\n",
    "\n",
    "# tags\n",
    "tags_prim_array_map = {\n",
    "    \"name\": \"Tags\"\n",
    "}\n",
    "payload[\"Tag\"] = from_primitive_array(\"Tag\", data, tags_prim_array_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': '880f2d16-16d6-4ca2-8616-494b7426f788',\n",
       "  'class': 'Resource',\n",
       "  'data': {'arn': 'arn:aws:s3:::opencitymodel',\n",
       "   'type': 'S3 Bucket',\n",
       "   'region': 'us-east-1',\n",
       "   'description': 'Project data files'}}]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # works, maybe ready\n",
    "# [ {\n",
    "#     \"id\": str(uuid4()), \n",
    "#     \"class\": \"Resource\", \n",
    "#     \"data\": {\n",
    "#         k: jmespath.search(v, item) for k, v in resources_object_array_map.items() \n",
    "#     }\n",
    "# } for item in data[\"Resources\"] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # works, maybe ready\n",
    "# def from_object_array(class_name, data, key, map):\n",
    "#     return [ {\n",
    "#         \"id\": str(uuid4()), \n",
    "#         \"class\": class_name, \n",
    "#         \"data\": {\n",
    "#             k: jmespath.search(v, item) for k, v in map.items() \n",
    "#         }\n",
    "#     } for item in data[key] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Dataset': [{'id': '7a106eef-10d9-44ec-840c-8475951ac6a6',\n",
       "   'class': 'Dataset',\n",
       "   'data': {'name': 'Open City Model (OCM)',\n",
       "    'description': 'Open City Model is an initiative to provide cityGML data for all the buildings in the United States.\\nBy using other open datasets in conjunction with our own code and algorithms it is our goal to provide 3D geometries for every US building.\\n',\n",
       "    'documentation': 'https://github.com/opencitymodel/opencitymodel',\n",
       "    'updateFrequency': 'Quarterly',\n",
       "    'license': 'https://github.com/opencitymodel/opencitymodel#license'}}],\n",
       " 'Publisher': [{'id': '1a66b2d5-dd6b-4044-8cef-873be68588af',\n",
       "   'class': 'Publisher',\n",
       "   'data': {'name': 'Open City Model (OCM)',\n",
       "    'contact': 'https://github.com/opencitymodel/opencitymodel#contact'}}],\n",
       " 'Resource': [{'id': 'b46b9134-06ba-48c6-b8ec-d4ec6b572da4',\n",
       "   'class': 'Resource',\n",
       "   'data': {'arn': 'arn:aws:s3:::opencitymodel',\n",
       "    'type': 'S3 Bucket',\n",
       "    'region': 'us-east-1',\n",
       "    'description': 'Project data files'}}],\n",
       " 'Tag': [{'id': 'd67c4e95-0e1a-47f1-aa87-c3edf0f58560',\n",
       "   'class': 'Tag',\n",
       "   'data': {'name': 'aws-pds'}},\n",
       "  {'id': '7bc9a4a0-89ff-4ea2-9195-fc906986ed1c',\n",
       "   'class': 'Tag',\n",
       "   'data': {'name': 'events'}},\n",
       "  {'id': '1a1c7781-8a80-4865-9944-827ded233dfd',\n",
       "   'class': 'Tag',\n",
       "   'data': {'name': 'cities'}},\n",
       "  {'id': 'e9553047-550c-4e69-a4fe-e05f9358dbc8',\n",
       "   'class': 'Tag',\n",
       "   'data': {'name': 'geospatial'}}]}"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': '7a106eef-10d9-44ec-840c-8475951ac6a6',\n",
       "  'class': 'Dataset',\n",
       "  'data': {'name': 'Open City Model (OCM)',\n",
       "   'description': 'Open City Model is an initiative to provide cityGML data for all the buildings in the United States.\\nBy using other open datasets in conjunction with our own code and algorithms it is our goal to provide 3D geometries for every US building.\\n',\n",
       "   'documentation': 'https://github.com/opencitymodel/opencitymodel',\n",
       "   'updateFrequency': 'Quarterly',\n",
       "   'license': 'https://github.com/opencitymodel/opencitymodel#license'}},\n",
       " {'id': '1a66b2d5-dd6b-4044-8cef-873be68588af',\n",
       "  'class': 'Publisher',\n",
       "  'data': {'name': 'Open City Model (OCM)',\n",
       "   'contact': 'https://github.com/opencitymodel/opencitymodel#contact'}},\n",
       " {'id': 'b46b9134-06ba-48c6-b8ec-d4ec6b572da4',\n",
       "  'class': 'Resource',\n",
       "  'data': {'arn': 'arn:aws:s3:::opencitymodel',\n",
       "   'type': 'S3 Bucket',\n",
       "   'region': 'us-east-1',\n",
       "   'description': 'Project data files'}},\n",
       " {'id': 'd67c4e95-0e1a-47f1-aa87-c3edf0f58560',\n",
       "  'class': 'Tag',\n",
       "  'data': {'name': 'aws-pds'}},\n",
       " {'id': '7bc9a4a0-89ff-4ea2-9195-fc906986ed1c',\n",
       "  'class': 'Tag',\n",
       "  'data': {'name': 'events'}},\n",
       " {'id': '1a1c7781-8a80-4865-9944-827ded233dfd',\n",
       "  'class': 'Tag',\n",
       "  'data': {'name': 'cities'}},\n",
       " {'id': 'e9553047-550c-4e69-a4fe-e05f9358dbc8',\n",
       "  'class': 'Tag',\n",
       "  'data': {'name': 'geospatial'}}]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[item for key in payload for item in payload[key]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['D',\n",
       " 'a',\n",
       " 't',\n",
       " 'a',\n",
       " 's',\n",
       " 'e',\n",
       " 't',\n",
       " 'P',\n",
       " 'u',\n",
       " 'b',\n",
       " 'l',\n",
       " 'i',\n",
       " 's',\n",
       " 'h',\n",
       " 'e',\n",
       " 'r',\n",
       " 'R',\n",
       " 'e',\n",
       " 's',\n",
       " 'o',\n",
       " 'u',\n",
       " 'r',\n",
       " 'c',\n",
       " 'e',\n",
       " 'T',\n",
       " 'a',\n",
       " 'g']"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "payload_ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "resources_array_map = {\n",
    "    \"arn\": \"Arn\",\n",
    "    \"type\": \"Type\",\n",
    "    \"region\": \"Region\",\n",
    "}\n",
    "\n",
    "dataatwork_tutorials = {\n",
    "    \"title\": \"Title\",\n",
    "    \"url\": \"Url\",\n",
    "    \"authorName\": \"AuthorName\",\n",
    "    \"authorUrl\": \"AuthorUrl\",\n",
    "    \"services\": \"Services\",\n",
    "}\n",
    "\n",
    "dataatwork_tools_apps = {\n",
    "    \"title\": \"Title\",\n",
    "    \"url\": \"Url\",\n",
    "    \"authorName\": \"AuthorName\",\n",
    "    \"authorUrl\": \"AuthorUrl\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'Open City Model (OCM)',\n",
       " 'description': 'Open City Model is an initiative to provide cityGML data for all the buildings in the United States.\\nBy using other open datasets in conjunction with our own code and algorithms it is our goal to provide 3D geometries for every US building.\\n',\n",
       " 'documentation': 'https://github.com/opencitymodel/opencitymodel',\n",
       " 'updateFrequency': 'Quarterly',\n",
       " 'license': 'https://github.com/opencitymodel/opencitymodel#license'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weaviate_object_from_json(data, dataset_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'Open City Model (OCM)',\n",
       " 'contact': 'https://github.com/opencitymodel/opencitymodel#contact'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weaviate_object_from_json(data, publisher_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "for resource in data['Resources']:\n",
    "    weaviate_object_from_json(resource, resources_array_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'Using Open City Model with the 3dCityDB', 'url': None, 'authorName': 'Allen Gilliland', 'authorUrl': None}\n",
      "{'title': 'Running queries on Open City Model using AWS Athena', 'url': None, 'authorName': 'Allen Gilliland', 'authorUrl': None}\n",
      "{'title': 'Investigating environmental characteristics of US cities using publicly available ASDI datasets', 'url': None, 'authorName': 'Darren Ko', 'authorUrl': None}\n"
     ]
    }
   ],
   "source": [
    "for tutorial in data['DataAtWork']['Tutorials']:\n",
    "    print(weaviate_object_from_json(tutorial, dataatwork_tutorials))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<weaviate.batch.crud_batch.Batch at 0x10e837a90>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# configure the batch settings\n",
    "client.batch.configure(\n",
    "  batch_size=100,\n",
    "  dynamic=False,\n",
    "  timeout_retries=3,\n",
    "  callback=weaviate.util.check_batch_result,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data for that class\n",
    "with open('../___data/classes/dataset-class.json') as f:\n",
    "    datasets = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Open City Model (OCM)', 'description': 'Open City Model is an initiative to provide cityGML data for all the buildings in the United States.\\nBy using other open datasets in conjunction with our own code and algorithms it is our goal to provide 3D geometries for every US building.\\n'}\n",
      "{'name': 'NOAA Continuously Operating Reference Stations (CORS) Network (NCN)', 'description': \"The [NOAA Continuously Operating Reference Stations (CORS) Network (NCN)](https://geodesy.noaa.gov/CORS/), managed by NOAA/National Geodetic Survey ([NGS](https://geodesy.noaa.gov/)), provide Global Navigation Satellite System (GNSS) data, supporting three dimensional positioning, meteorology, space weather, and geophysical applications throughout the United States. The NCN is a multi-purpose, multi-agency cooperative endeavor, combining the efforts of hundreds of government, academic, and private organizations. The stations are independently owned and operated. Each agency shares their GNSS/GPS carrier phase and code range measurements and station metadata with NGS, which are analyzed and distributed free of charge.\\nNGS provides access to all NCN data collected since 9 February (040) 1994.\\n- #### Access to NCN Data and Products\\n    - [NOAA-NCN on AWS](https://noaa-cors-pds.s3.amazonaws.com/index.html)\\n    - [NGS server: https://geodesy.noaa.gov/corsdata/](https://geodesy.noaa.gov/corsdata/)\\n    - [NGS's customized data request service (UFCORS)](https://geodesy.noaa.gov/UFCORS/)\\n    - [NGS Anonymous ftp://geodesy.noaa.gov/cors/ - This service is going away on August 02, 2021!](ftp://geodesy.noaa.gov/cors/)\\n- #### NCN Data and Products\\n    - **RINEX**: The GPS/GNSS data collected at NCN stations are made available to the public by NGS in Receiver INdependent EXchange (RINEX) format. Most data are available within 1 hour (60 minutes) from when they were recorded at the remote site, and a few sites have a delay of 24 hours (1440 minutes).<br/>RINEX data can be found at: *rinex/`YYYY`/`DDD`/`ssss`/*\\n    - **Station logs**: \\n        - Station log files contain all the historical equipment (receiver/antenna) used at that station, approximate location, owner and operating agency, etc..<br/>Station log files can be found at: *station_log/`ssss`.log.txt*\\n        - Historical and current equipment information of all NCN stations, except those that are considered IGS stations.<br/>These data can be found at: *station_log/cumulative.station.info.cors*\\n    - **Published Coordinates and Velocities**: NAD83 and ITRF coordinates and velocities of each NCN station. All published coordinates and velocities are given for the Antenna Reference Point (ARP).<br/>Published coordinate and velocity files can be found at: *coord/coord_`YY`/*<br/>In July 2019, NGS published [MYCS2](https://geodesy.noaa.gov/CORS/news/mycs2/mycs2.shtml)!\\n    - **Time-series Plots**:\\n        - *Short-term* plots show the repeatability of a site for the last 90-days with respect to the current published position, corrected for the effect of the published velocity. These plots are updated daily.<br/>Short-term plots can be found at: */Plots/`ssss`_`YY`.short.png*\\n        - *Long-term* plots show the show weekly residual positions with respect to the current published coordinates from our stacked solution. Newer sites may not have a long-term plot if they were added after our Multi-year Solution Processing campaign.<br/>Long-term plots can be found at: */Plots/Longterm/`ssss`_`YY`.long.png*\\n    - **Daily Broadcast Ephemeris**:\\n        - Daily GPS Broadcast ephemeris can be found at: *rinex/`YYYY`/`DDD`/brdc`DDD`0.`YY`n.gz*\\n        - Daily GLONASS-only Broadcast ephemeris can be found at: *rinex/`YYYY`/`DDD`/brdc`DDD`0.`YY`g.gz*\\n    - **Daily final, rapid, and hourly ultra-rapid GNSS Orbit** can be found at: \\n        - Daily final and rapid GNSS Orbit can be found at: *rinex/`YYYY`/`DDD`/`AAAWWWWD`.sp3.gz*\\n        - Hourly ultra-rapid GNSS Orbit can be found at: *rinex/`YYYY`/`DDD`/`AAAWWWWD`_`HH`.sp3.gz*\\n    - In which:\\n      - `YYYY`: 4-digit year\\n      - `YY`: The last 2-digit of year\\n      - `DDD`: 3-digit day of year [001,002,..366]\\n      - `D`: day of week [Sun=0, Mon=1,..,Fri=6]\\n      - `ssss`: 4-char station ID\\n      - `h`: 1-char hour of day (a=00, b=01, c=02,..,x=23)\\n      - `HH`: 2-digit hour of day (00,01,02,..,23)\\n      - `WWWW`: 4-digit GPS week number\\n      - `AAA`: 3-char analysis center name/type of solution, such as:\\n          - igs: IGS final solution combination\\n          - igl: IGS final solution combination (GLONASS-only)\\n          - igr: IGS rapid solution combination\\n          - igu: IGS ultra-rapid solution combination\\n    \\n\"}\n",
      "{'name': '1940 Census Population Schedules, Enumeration District Maps, and Enumeration District Descriptions', 'description': 'The 1940 Census population schedules were created by the Bureau of the Census in an attempt to enumerate every person living in the United States on April 1, 1940, although some persons were missed. The 1940 census population schedules were digitized by the National Archives and Records Administration (NARA) and released publicly on April 2, 2012.\\nThe 1940 Census enumeration district maps contain maps of counties, cities, and other minor civil divisions that show enumeration districts, census tracts, and related boundaries and numbers used for each census. The coverage is nation wide and includes territorial areas.\\nThe 1940 Census enumeration district descriptions contain written descriptions of census districts, subdivisions, and enumeration districts.\\n'}\n",
      "{'name': 'NOAA Global Hydro Estimator (GHE)', 'description': 'Global Hydro-Estimator provides a global\\nmosaic imagery of rainfall estimates from\\nmulti-geostationary satellites, which\\ncurrently includes GOES-16, GOES-15,\\nMeteosat-8, Meteosat-11 and Himawari-8.\\nThe GHE products include: Instantaneous\\nrain rate, 1 hour, 3 hour, 6 hour, 24 hour\\nand also multi-day rainfall accumulation.\\n'}\n",
      "{'name': 'USGS 3DEP LiDAR Point Clouds', 'description': 'The goal of the [USGS 3D Elevation Program ](https://www.usgs.gov/core-science-systems/ngp/3dep) (3DEP) is to collect elevation data in the form of light detection and ranging (LiDAR) data over the conterminous United States, Hawaii, and the U.S. territories, with data acquired over an 8-year period. This dataset provides two realizations of the 3DEP point cloud data. The first resource is a public access organization provided in [Entwine Point Tiles](https://entwine.io/entwine-point-tile.html) format, which a lossless, full-density, streamable octree based on [LASzip](https://laszip.org) (LAZ) encoding. The second resource is a [Requester Pays](https://docs.aws.amazon.com/AmazonS3/latest/dev/RequesterPaysBuckets.html) of the original, Raw LAZ (Compressed LAS) 1.4 3DEP format, and more complete in coverage, as sources with incomplete or missing CRS, will not have an ETP tile generated.  Resource names in both buckets correspond to the USGS project names.'}\n",
      "{'name': 'CIViC (Clinical Interpretation of Variants in Cancer)', 'description': 'Precision medicine refers to the use of prevention and treatment strategies that are tailored to the unique features of each individual and their disease. In the context of cancer this might involve the identification of specific mutations shown to predict response to a targeted therapy. The biomedical literature describing these associations is large and growing rapidly. Currently these interpretations exist largely in private or encumbered databases resulting in extensive repetition of effort. Realizing precision medicine will require this information to be centralized, debated and interpreted for application in the clinic. CIViC is an open access, open source, community-driven web resource for Clinical Interpretation of Variants in Cancer. Our goal is to enable precision medicine by providing an educational forum for dissemination of knowledge and active discussion of the clinical significance of cancer genome alterations.'}\n",
      "{'name': 'The Multilingual Amazon Reviews Corpus', 'description': \"We present a collection of Amazon reviews specifically designed to aid research in multilingual text classification. The dataset contains reviews in English, Japanese, German, French, Chinese and Spanish, collected between November 1, 2015 and November 1, 2019. Each record in the dataset contains the review text, the review title, the star rating, an anonymized reviewer ID, an anonymized product ID and the coarse-grained product category (e.g. 'books', 'appliances', etc.)\"}\n",
      "{'name': 'Software Heritage Graph Dataset', 'description': '[Software Heritage](https://www.softwareheritage.org/) is the largest\\nexisting public archive of software source code and accompanying\\ndevelopment history. The Software Heritage Graph Dataset is a fully\\ndeduplicated Merkle DAG representation of the Software Heritage archive.\\n\\nThe dataset links together file content identifiers, source code\\ndirectories, Version Control System (VCS) commits tracking evolution over\\ntime, up to the full states of VCS repositories as observed by Software\\nHeritage during periodic crawls. The dataset’s contents come from major\\ndevelopment forges (including GitHub and GitLab), FOSS distributions (e.g.,\\nDebian), and language-specific package managers (e.g., PyPI). Crawling\\ninformation is also included, providing timestamps about when and where all\\narchived source code artifacts have been observed in the wild.\\n'}\n",
      "{'name': 'Landsat 8', 'description': 'An ongoing collection of satellite imagery of all land on Earth produced by\\nthe Landsat 8 satellite.\\n'}\n",
      "{'name': 'IDEAM - Colombian Radar Network', 'description': 'Historical and one-day delay data from the IDEAM radar network.'}\n",
      "{'name': 'TIGER Training', 'description': '\"This dataset contains the training data for the [Tumor InfiltratinG lymphocytes in breast cancER or TIGER](https://tiger.grand-challenge.org) challenge. TIGER is the first challenge on fully automated assessment of tumor-infiltrating lymphocytes (TILs) in breast cancer histopathology slides. TILs are proving to be an important biomarker in cancer patients as they can play a part in killing tumor cells, particularly in some types of breast cancer. Identifying and measuring TILs can help to better target treatments, particularly immunotherapy, and may result in lower levels of other more aggressive treatments, including chemotherapy.\"\\n'}\n",
      "{'name': 'PersonPath22', 'description': 'PersonPath22 is a large-scale multi-person tracking dataset containing 236 videos captured mostly from static-mounted cameras,  collected from sources where we were given the rights to redistribute the content and participants have given explicit consent. Each video has ground-truth annotations including both bounding boxes and tracklet-ids for all the persons in each frame.'}\n",
      "{'name': 'Terra Fusion Data Sampler', 'description': 'The Terra Basic Fusion dataset is a fused dataset of the original Level 1 radiances\\nfrom the five Terra instruments. They have been fully validate to contain the original\\nTerra instrument Level 1 data. Each Level 1 Terra Basic Fusion file contains one full\\nTerra orbit of data and is typically 15 – 40 GB in size, depending on how much data was\\ncollected for that orbit. It contains instrument radiance in physical units; radiance\\nquality indicator; geolocation for each IFOV at its native resolution; sun-view geometry;\\nbservation time; and other attributes/metadata. It is stored in HDF5, conformed to CF\\nconventions, and accessible by netCDF-4 enhanced models. It’s naming convention\\nfollows: TERRA_BF_L1B_OXXXX_YYYYMMDDHHMMSS_F000_V000.h5. A concise description of the\\ndataset, along with links to complete documentation and available software tools, can\\nbe found on the Terra Fusion project page: https://terrafusion.web.illinois.edu.</br></br>\\n\\nTerra is the flagship satellite of NASA’s Earth Observing System (EOS). It was launched\\ninto orbit on December 18, 1999 and carries five instruments. These are the\\nModerate-resolution Imaging Spectroradiometer (MODIS), the Multi-angle Imaging\\nSpectroRadiometer (MISR), the Advanced Spaceborne Thermal Emission and Reflection\\nRadiometer (ASTER), the Clouds and Earth’s Radiant Energy System (CERES), and the\\nMeasurements of Pollution in the Troposphere (MOPITT).</br></br>\\n\\nThe Terra Basic Fusion dataset is an easy-to-access record of the Level 1 radiances\\nfor instruments on the Terra mission for selected WRS-2 paths covering the years\\n2000-2015. These paths are Paths 20-26 (e.g., US corn belt), 108 (e.g., Japan),\\n125 (e.g., China), 143 (e.g., India), 150 (e.g., Showa Station, Antarctica),\\n169 (e.g., Europe and Africa), 188 (e.g., Nigeria calibration site), and 233\\n(e.g., Greenland).\\n'}\n",
      "{'name': 'IChangeMyCity Complaints Data from Janaagraha', 'description': 'The [IChangeMyCity](https://www.ichangemycity.com) project provides insight into the complaints raised by citizens from diffent cities of India related to the issues in their neighbourhoods and the resolution of the same by the civic bodies.'}\n",
      "{'name': 'New York City Taxi and Limousine Commission (TLC) Trip Record Data', 'description': \"Data of trips taken by taxis and for-hire vehicles in New York City. Note: access to this dataset is free, however direct S3 access does require an AWS account. Anonymous downloads are accessible from the dataset's documentation webpage listed below.\"}\n",
      "{'name': 'Digital Earth Africa Sentinel-2 Level-2A', 'description': 'The Sentinel-2 mission is part of the European Union Copernicus programme for Earth observations. Sentinel-2 consists of twin satellites, Sentinel-2A (launched 23 June 2015) and Sentinel-2B (launched 7 March 2017). The two satellites have the same orbit, but 180° apart for optimal coverage and data delivery. Their combined data is used in the Digital Earth Africa Sentinel-2 product.\\nTogether, they cover all Earth’s land surfaces, large islands, inland and coastal waters every 3-5 days.\\nSentinel-2 data is tiered by level of pre-processing. Level-0, Level-1A and Level-1B data contain raw data from the satellites, with little to no pre-processing. Level-1C data is surface reflectance measured at the top of the atmosphere. This is processed using the Sen2Cor algorithm to give Level-2A, the bottom-of-atmosphere reflectance (Obregón et al, 2019). Level-2A data is the most ideal for research activities as it allows further analysis without applying additional atmospheric corrections.\\nThe Digital Earth Africa Sentinel-2 dataset contains Level-2A data of the African continent. Digital Earth Africa does not host any lower-level Sentinel-2 data.\\nNote that this data is a subset of the Sentinel-2 COGs dataset.\\n'}\n",
      "{'name': 'Therapeutically Applicable Research to Generate Effective Treatments (TARGET)', 'description': 'Therapeutically Applicable Research to Generate Effective Treatments (TARGET) is the collaborative effort of a large, diverse consortium of extramural and NCI investigators. The goal of the effort is to accelerate molecular discoveries that drive the initiation and progression of hard-to-treat childhood cancers and facilitate rapid translation of those findings into the clinic.\\nTARGET projects provide comprehensive molecular characterization to determine the genetic changes that drive the initiation and progression of childhood cancers.The dataset contains open Clinical Supplement, Biospecimen Supplement, RNA-Seq Gene Expression Quantification, miRNA-Seq Isoform Expression Quantification, miRNA-Seq miRNA Expression Quantification data from Genomic Data Commons (GDC), and open data from GDC Legacy Archive.\\n'}\n",
      "{'name': 'Genome Ark', 'description': 'The Genome Ark hosts genomic information for the Vertebrate Genomes Project (VGP) and other related projects. The VGP is an international collaboration that aims to generate complete and near error-free reference genomes for all extant vertebrate species. These genomes will be used to address fundamental questions in biology and disease, to identify species most genetically at risk for extinction, and to preserve genetic information of life.'}\n",
      "{'name': 'NOAA Integrated Surface Database (ISD)', 'description': 'The Integrated Surface Database (ISD) consists\\nof global hourly and synoptic observations\\ncompiled from numerous sources into a gzipped\\nfixed width format. ISD was developed as a joint\\nactivity within Asheville\\'s Federal Climate\\nComplex. The database includes over 35,000 stations\\nworldwide, with some having data as far back\\nas 1901, though the data show a substantial\\nincrease in volume in the 1940s and again in\\nthe early 1970s. Currently, there are over\\n14,000 \"active\" stations updated daily in the\\ndatabase. The total uncompressed data volume is\\naround 600 gigabytes; however, it continues to\\ngrow as more data are added. ISD includes\\nnumerous parameters such as wind speed and\\ndirection, wind gust, temperature, dew point,\\ncloud data, sea level pressure, altimeter setting,\\nstation pressure, present weather, visibility,\\nprecipitation amounts for various time periods,\\nsnow depth, and various other elements as observed\\nby each station.\\n'}\n",
      "{'name': 'NEXRAD on AWS', 'description': 'Real-time and archival data from the Next Generation Weather Radar (NEXRAD) network.'}\n",
      "{'name': 'Longitudinal Nutrient Deficiency', 'description': 'Dataset associated with the 2021 AAAI Paper- Detection and Prediction of Nutrient Deficiency Stress using Longitudinal Aerial Imagery.  The dataset contains 3 image sequences of aerial imagery from 386 farm parcels which have been annotated for nutrient deficiency stress.'}\n",
      "{'name': 'SondeHub Radiosonde Telemetry', 'description': 'SondeHub Radiosonde telemetry contains global radiosonde (weather balloon) data captured by SondeHub from our participating radiosonde_auto_rx receiving stations. radiosonde_auto_rx is a open source project aimed at receiving and decoding telemetry from airborne radiosondes using software-defined-radio techniques, enabling study of the telemetry and sometimes recovery of the radiosonde itself.\\nCurrently 313 receiver stations are providing data for an average of 384 radiosondes a day.  The data within this repository contains received telemetry frames, including radiosonde type, gps position, and for some radiosondes atmospheric sensor data (temperature, humidity, pressure). As the downlinked telemetry does not always contain calibration information, any atmospheric sensor data should be considered to be uncalibrated. Note that radiosonde_auto_rx does not have sensor data support for all radiosonde types.\\n'}\n",
      "{'name': 'NOAA Rapid Refresh (RAP)', 'description': 'The Rapid Refresh (RAP) is a NOAA/NCEP operational weather prediction system comprised primarily of a numerical forecast model and analysis/assimilation system to initialize that model. It covers North America and is run with a horizontal resolution of 13 km and 50 vertical layers. The RAP was developed to serve users needing frequently updated short-range weather forecasts, including those in the US aviation community and US severe weather forecasting community. The model is run for every hour of the day; it is integrated to 51 hours for the 03/09/15/21 UTC cycles and to 21 hours for every other cycle. The RAP uses the ARW core of the WRF model and the Gridpoint Statistical Interpolation (GSI) analysis - the analysis is aided with the assimilation of cloud and hydrometeor data to provide more skill in short-range cloud and precipitation forecasts.'}\n",
      "{'name': 'Sentinel-3', 'description': 'This data set consists of observations from the Sentinel-3 satellite of the European Commission’s Copernicus Earth Observation Programme. Sentinel-3 is a polar orbiting satellite that completes 14 orbits of the Earth a day. It carries the Ocean and Land Colour Instrument (OLCI) for medium resolution marine and terrestrial optical measurements, the Sea and Land Surface Temperature Radiometer (SLSTR), the SAR Radar Altimeter (SRAL), the MicroWave Radiometer (MWR) and the Precise Orbit Determination (POD) instruments. The satellite was launched in 2016 and entered routine operational phase in 2017. Data is available from July 2017 onwards.'}\n",
      "{'name': 'CBERS on AWS', 'description': \"Imagery acquired\\nby the China-Brazil Earth Resources Satellite (CBERS), 4 and 4A.\\nThe\\nimage files are recorded and processed by Instituto Nacional de Pesquisas\\nEspaciais (INPE) and are converted to Cloud Optimized Geotiff\\nformat in order to optimize its use for cloud based applications.\\nContains all CBERS-4 MUX, AWFI, PAN5M and\\nPAN10M scenes acquired since\\nthe start of the satellite mission and is daily updated with\\nnew scenes.\\nCBERS-4 PAN5M and PAN10M starting from 05-2022 are temporarily not ingested\\ndue to an error in the bands identification on INPE's catalog.\\nCBERS-4A MUX Level 4 (Orthorectified) scenes are being\\ningested starting from 04-13-2021. CBERS-4A WFI Level 4 (Orthorectified)\\nscenes are being ingested starting from 10-12-2022.\\n\"}\n",
      "{'name': 'Digital Earth Africa CHIRPS Rainfall', 'description': 'Digital Earth Africa (DE Africa) provides free and open access to a copy of the Climate Hazards Group InfraRed Precipitation with Station data (CHIRPS) monthly and daily products over Africa. The CHIRPS rainfall maps are produced and provided by the Climate Hazards Center in collaboration with the US Geological Survey, and use both rain gauge and satellite observations.\\nThe CHIRPS-2.0 Africa Monthly dataset is regularly indexed to DE Africa from the CHIRPS monthly data. The CHIRPS-2.0 Africa Daily dataset is likewise indexed from the CHIRPS daily data. Both products have been converted to cloud-opitmized GeoTIFFs, and can be accessed through DE Africa’s Open Data Cube. This means the full archive of CHIRPS daily and monthly rainfall can be easily used for inspection or analysis across DE Africa platforms, including the user-interactive DE Africa Map.\\nFor more information on the dataset, see the [CHIRPS website](https://www.chc.ucsb.edu/data/chirps).\\n'}\n",
      "{'name': 'Phrase Clustering Dataset (PCD)', 'description': 'This dataset is part of the paper \"McPhraSy: Multi-Context Phrase Similarity and Clustering\" by DN Cohen et al (2022). The purpose of PCD is to evaluate the quality of semantic-based clustering of noun phrases. The phrases were collected from the [Amazon Review Dataset] (https://nijianmo.github.io/amazon/).\\n'}\n",
      "{'name': 'Yale-CMU-Berkeley (YCB) Object and Model Set', 'description': \"This project primarily aims to facilitate performance benchmarking in robotics research. The dataset provides mesh models, RGB, RGB-D and point cloud images of over 80 objects. The physical objects are also available via the [YCB benchmarking project](http://www.ycbbenchmarks.com/). The data are collected by two state of the art systems: UC Berkley's scanning rig and the Google scanner. The UC Berkley's scanning rig data provide meshes generated with Poisson reconstruction, meshes generated with volumetric range image integration, textured versions of both meshes, Kinbody files for using the meshes with OpenRAVE, 600 High-resolution RGB images, 600 RGB-D images, and 600 point cloud images for each object. The Google scanner data provides 3 meshes with different resolutions (16k, 64k, and 512k polygons), textured versions of each mesh, Kinbody files for using the meshes with OpenRAVE.\"}\n",
      "{'name': 'Scottish Public Sector LiDAR Dataset', 'description': 'This dataset is Lidar data that has been collected by the Scottish public sector and made available under the Open Government Licence. The data are available as point cloud (LAS format or in LAZ compressed format), along with the derived Digital Terrain Model (DTM) and Digital Surface Model (DSM) products as Cloud optimized GeoTIFFs (COG) or standard GeoTIFF. The dataset contains multiple subsets of data which were each commissioned and flown in response to different organisational requirements. The details of each can be found at https://remotesensingdata.gov.scot/data#/list'}\n",
      "{'name': 'Multilingual Name Entity Recognition (NER) Datasets with Gazetteer', 'description': 'Name Entity Recognition datasets containing short sentences and queries with low-context,\\nincluding LOWNER, MSQ-NER, ORCAS-NER and Gazetteers (1.67 million entities).\\nThis release contains the multilingual versions of the datasets in [Low Context Name Entity Recognition (NER) Datasets with Gazetteer](https://registry.opendata.aws/lowcontext-ner-gaz/).\\n'}\n",
      "{'name': 'The Human Microbiome Project', 'description': 'The NIH-funded Human Microbiome Project (HMP) is a collaborative effort of over 300 scientists from more than 80 organizations to comprehensively characterize the microbial communities inhabiting the human body and elucidate their role in human health and disease. To accomplish this task, microbial community samples were isolated from a cohort of 300 healthy adult human subjects at 18 specific sites within five regions of the body (oral cavity, airways, urogenital track, skin, and gut). Targeted sequencing of the 16S bacterial marker gene and/or whole metagenome shotgun sequencing was performed for thousands of these samples. In addition, whole genome sequences were generated for isolate strains collected from human body sites to act as reference organisms for analysis. Finally, 16S marker and whole metagenome sequencing was also done on additional samples from people suffering from several disease conditions.'}\n",
      "{'name': 'ClinVar - Data Lakehouse Ready', 'description': 'ClinVar is a freely accessible, public archive of reports of the relationships among human variations and phenotypes, with supporting evidence. ClinVar thus facilitates access to and communication about the relationships asserted between human variation and observed health status, and the history of that interpretation. ClinVar processes submissions reporting variants found in patient samples, assertions made regarding their clinical significance, information about the submitter, and other supporting data. The alleles described in submissions are mapped to reference sequences, and reported according to the HGVS standard. ClinVar then presents the data for interactive users as well as those wishing to use ClinVar in daily workflows and other local applications. ClinVar works in collaboration with interested organizations to meet the needs of the medical genetics community as efficiently and effectively as possible. This representation of ClinVar is stored in Parquet format and most easily utilized through Amazon Athena. Follow the documentation link for install instructions (< 2 minute install).'}\n",
      "{'name': 'NOAA Oceanic Climate Data Records', 'description': \"NOAA's Climate Data Records (CDRs) are robust, sustainable, and scientifically sound climate records that provide trustworthy information on how, where, and to what extent the land, oceans, atmosphere and ice sheets are changing. These datasets are thoroughly vetted time series measurements with the longevity, consistency, and continuity to assess and measure climate variability and change. NOAA CDRs are vetted using standards established by the National Research Council (NRC).<br/><br/>\\nClimate Data Records are created by merging data from surface, atmosphere, and space-based systems across decades. NOAA’s Climate Data Records provides authoritative and traceable long-term climate records. NOAA developed CDRs by applying modern data analysis methods to historical global satellite data. This process can clarify the underlying climate trends within the data and allows researchers and other users to identify economic and scientific value in these records. NCEI maintains and extends CDRs by applying the same methods to present-day and future satellite measurements.<br/><br/>\\nOceanic Climate Data Records are measurements of oceans and seas both surface and subsurface as well as frozen state variables.\\n\"}\n",
      "{'name': 'Finnish Meteorological Institute Weather Radar Data', 'description': \"The up-to-date weather radar from the FMI radar network is available as Open Data. The data contain both single radar data along with composites over Finland in GeoTIFF and HDF5-formats. Available composite parameters consist of radar reflectivity (DBZ), rainfall intensity (RR), and precipitation accumulation of 1, 12, and 24 hours. Single radar parameters consist of radar reflectivity (DBZ), radial velocity (VRAD), rain classification (HCLASS), and Cloud top height (ETOP 20). Raw volume data from singe radars are also provided in HDF5 format with ODIM 2.3 conventions. Radar data becomes available as soon as it's received from the radar and pre-processed into deliverable formats. Typically the most recent radar data was collected less than 5 minutes ago.\"}\n",
      "{'name': 'NOAA Terrestrial Climate Data Records', 'description': \"NOAA's Climate Data Records (CDRs) are robust, sustainable, and scientifically sound climate records that provide trustworthy information on how, where, and to what extent the land, oceans, atmosphere and ice sheets are changing. These datasets are thoroughly vetted time series measurements with the longevity, consistency, and continuity to assess and measure climate variability and change. NOAA CDRs are vetted using standards established by the National Research Council (NRC).<br/><br/>\\nClimate Data Records are created by merging data from surface, atmosphere, and space-based systems across decades. NOAA’s Climate Data Records provides authoritative and traceable long-term climate records. NOAA developed CDRs by applying modern data analysis methods to historical global satellite data. This process can clarify the underlying climate trends within the data and allows researchers and other users to identify economic and scientific value in these records. NCEI maintains and extends CDRs by applying the same methods to present-day and future satellite measurements.<br/><br/>\\nTerrestrial CDRs are composed of sensor data that have been improved and quality controlled over time, together with ancillary calibration data.\\n\"}\n",
      "{'name': 'Earth Radio Occultation', 'description': 'This is an updating archive of radio occultation data using the transmitters of the Global Navigation Satellite Systems (GNSS) as generated and processed at the COSMIC DAAC, the Jet Propulsion Laboratory of Caltech, and the Radio Occultation Meteorology Satellite Application Facility (ROM SAF). <p> This dataset is funded by the NASA Earth Science Data Systems and the Advancing Collaborative Connections for Earth System Science (ACCESS) 2019 program.'}\n",
      "{'name': 'PASS: Perturb-and-Select Summarizer for Product Reviews', 'description': 'A collection of product reviews summaries automatically generated by PASS for 32 Amazon products from the FewSum dataset'}\n",
      "{'name': 'QIIME 2 User Tutorial Datasets', 'description': 'QIIME 2 is a powerful, extensible, and decentralized microbiome analysis package with a focus on data and analysis transparency. QIIME 2 enables researchers to start an analysis with raw DNA sequence data and finish with publication-quality figures and statistical results. This dataset contains the user docs (and related datasets) for QIIME 2.'}\n",
      "{'name': 'Global Seasonal Sentinel-1 Interferometric Coherence and Backscatter Data Set', 'description': \"This data set is the first-of-its-kind spatial representation of multi-seasonal, global SAR repeat-pass interferometric coherence and backscatter signatures. Global coverage comprises all land masses and ice sheets from 82 degrees northern to 79 degrees southern latitude. The data set is derived from high-resolution multi-temporal repeat-pass interferometric processing of about 205,000 Sentinel-1 Single-Look-Complex data acquired in Interferometric Wide-Swath mode (Sentinel-1 IW mode) from 1-Dec-2019 to 30-Nov-2020. The data set was developed by [Earth Big Data LLC](https://earthbigdata.com) and [Gamma Remote Sensing AG](https://www.gamma-rs.ch), under contract for [NASA's Jet Propulsion Laboratory](https://jpl.nasa.gov). The data set covers four sets of seasonal (DJF/MAM/JJA/SON) metrics: 1) Median 6-, 12-, 18-, 24-, 36-, and 48-day repeat coherence estimates for C-band VV and HH polarized data, 2) Mean backscatter (gamma naught) for VV, VH, HH, and HV polarizations, 3) Seasonal coherence decay model parameters rho, tau, and rmse, 4) Local incidence and layover/shadow regions for all relative orbits (175 orbits). Note that in the data set filenames the seasons were referred to as northern hemisphere winter (DJF), spring (MAM), summer (JJA), and fall (SON). The data set is available in two main components: 1) 1x1 degree tiles. Each tile contains GeoTiffs at 3 arcsec pixel spacing of all metrics available in the tile. (s3://sentinel-1-global-coherence-earthbigdata/data/tiles/), 2) Global mosaicked tiles as cloud optimized GeoTIFFs (COG) at 0.01 degree pixel spacing (s3://sentinel-1-global-coherence-earthbigdata/data/mosaics/) for each of the computed metrics.\\n\"}\n",
      "{'name': 'NIH NCBI Sequence Read Archive (SRA) on AWS', 'description': \"The Sequence Read Archive (SRA), produced by the [National Center for Biotechnology Information (NCBI)](https://www.ncbi.nlm.nih.gov/) at the [National Library of Medicine (NLM)](http://nlm.nih.gov/) at the [National Institutes of Health (NIH)](http://www.nih.gov/), stores raw DNA sequencing data and alignment information from high-throughput sequencing platforms. The SRA provides open access to these biological sequence data to support the research community's efforts to enhance reproducibility and make new discoveries by comparing data sets. Buckets in this registry contain public SRA data in the original (user submitted) format from select high value and newly-released studies as well as all public-access SRA formatted ETL+BQS data. Also included is all SRA metadata that can be leveraged for attribute-based data discovery.\"}\n",
      "{'name': 'Cancer Genome Characterization Initiatives - Burkitt Lymphoma, HIV+ Cervical Cancer', 'description': 'The Cancer Genome Characterization Initiatives (CGCI) program supports cutting-edge genomics research of adult and pediatric cancers. CGCI investigators develop and apply advanced sequencing methods that examine genomes, exomes, and transcriptomes within various types of tumors. The program includes Burkitt Lymphoma Genome Sequencing Project (BLGSP) project and HIV+ Tumor Molecular Characterization Project - Cervical Cancer (HTMCP-CC) project.\\nThe dataset contains open Clinical Supplement, Biospecimen Supplement, RNA-Seq Gene Expression Quantification, miRNA-Seq Isoform Expression Quantification, and miRNA Expression Quantification data.\\nThis dataset also contains controlled WGS/Targeted Sequencing/RNA-Seq/miRNA-Seq Aligned Reads, and RNA-Seq Splice Junction Quantification\\n'}\n",
      "{'name': 'UniProt', 'description': 'The Universal Protein Resource (UniProt) is a comprehensive resource for protein sequence and annotation data. The UniProt databases are the UniProt Knowledgebase (UniProtKB), the UniProt Reference Clusters (UniRef), and the UniProt Archive (UniParc). The UniProt consortium and host institutions [EMBL-EBI](https://www.ebi.ac.uk), [SIB Swiss Institute of Bioinformatics](https://www.sib.swiss) and [PIR](https://proteininformationresource.org/) are committed to the long-term preservation of the [UniProt](https://www.uniprot.org) databases.'}\n",
      "{'name': 'Community Earth System Model v2 ARISE (CESM2 ARISE)', 'description': 'Data from ARISE-SAI Experiments with CESM2'}\n",
      "{'name': 'Global Biodiversity Information Facility (GBIF) Species Occurrences', 'description': \"The Global Biodiversity Information Facility (GBIF) is an international network and data infrastructure funded by the world's governments providing global data that document the occurrence of species. GBIF currently integrates datasets documenting over 1.6 billion species occurrences, growing daily. The GBIF occurrence dataset combines data from a wide array of sources including specimen-related data from natural history museums, observations from citizen science networks and environment recording schemes. While these data are constantly changing at GBIF.org, periodic snapshots are taken and made available on AWS.\"}\n",
      "{'name': 'Google Books Ngrams', 'description': 'N-grams are fixed size tuples of items. In this case the items are words extracted from the Google Books corpus. The n specifies the number of elements in the tuple, so a 5-gram contains five words or characters. The n-grams in this dataset were produced by passing a sliding window of the text of books and outputting a record for each new token.'}\n",
      "{'name': 'NOAA National Bathymetric Source Data', 'description': 'The National Bathymetric Source (NBS) project creates and maintains \\nhigh-resolution bathymetry composed of the best available data. \\nThis project enables the creation of next-generation nautical charts \\nwhile also providing support for modeling, industry, science, \\nregulation, and public curiosity. Primary sources of bathymetry include \\nNOAA and U.S. Army Corps of Engineers hydrographic surveys and \\ntopographic bathymetric (topo-bathy) lidar (light detection and ranging) \\ndata.  Data submitted through the NOAA Office of Coast Survey’s external \\nsource data process are also included, with gaps in deep water filled \\nthrough Global Multi-Resolution Topography, a merged model of \\nbathymetry. Different vertical datums and file formats are made \\navailable to meet various uses. The BlueTopo folder includes multilayer \\nfloating point GeoTIFFs with associated Raster Attribute Tables (RAT) \\ncontaining elevation, vertical uncertainty, with other quality metrics \\nand source information. These files are arranged in a tiling, naming, \\nand resolution scheme corresponding to the Electronic Navigational \\nChart (ENC) but are not for navigation due to the inclusion of additional \\nnon-navigation data and non-navigation vertical datums. For \\nnavigational datasets please see the S-102 distribution portal. In the \\nfuture \"[nowCOAST](https://nowcoast.noaa.gov/)\" will provide to the public \\nweb mapping services for the BlueTopo products.\\n'}\n",
      "{'name': 'Open Bioinformatics Reference Data for Galaxy', 'description': \"This dataset provides genomic reference data and software packages for use with [Galaxy](https://galaxyproject.org/) and [Bioconductor](https://bioconductor.org/) applications. The reference data is available for hundreds of reference genomes and has been formatted for use with a variety of tools. The available configuration files make this data easily incorporable with a local Galaxy server without additional data preparation. Additionally, Bioconductor's AnnotationHub and ExperimentHub data are provided for use via R packages through which the data can be downloaded and installed into any R environment.\\n\"}\n",
      "{'name': 'ARPA-E PERFORM Forecast data', 'description': 'The ARPA-E PERFORM Program is an ARPA-E funded program that aim to use\\ntime-coincident power and load seeks to develop innovative management systems\\nthat represent the relative delivery risk of each asset and balance the\\ncollective risk of all assets across the grid. A risk-driven paradigm allows\\noperators to: (i) fully understand the true likelihood of maintaining a\\nsupply-demand balance and system reliability, (ii) optimally manage the system,\\nand (iii) assess the true value of essential reliability services. This\\nparadigm shift is critical for all power systems and is essential for grids\\nwith high levels of stochastic resources. Projects will propose methods to\\nquantify and manage risk at the asset level and at the system level.\\n\\nIn support of the ARPA-E PERFORM project, NREL has produced a set of\\ntime-coincident load, wind, and solar generation profiles, including actual and\\nforecasting time series. Both actuals and forecasts are provided in form of\\ntime-series with high temporal and spatial fidelity. Both deterministic and\\nprobabilistic forecasts are contained in the dataset.\\n'}\n",
      "{'name': 'AI2 Meaningful Citations Data Set', 'description': '630 paper annotations'}\n",
      "{'name': 'Clinical Proteomic Tumor Analysis Consortium 2 (CPTAC-2)', 'description': 'The Clinical Proteomic Tumor Analysis Consortium (CPTAC) is a national effort to accelerate the\\nunderstanding of the molecular basis of cancer through the application of large-scale proteome and\\ngenome analysis, or proteogenomics. CPTAC-2 is the Phase II of the CPTAC Initiative (2011-2016).\\nDatasets contain open RNA-Seq Gene Expression Quantification, miRNA-Seq Isoform Expression\\nQuantification, and miRNA Expression Quantification data.\\n'}\n",
      "{'name': 'National Archives Catalog', 'description': 'The National Archives Catalog dataset contains all of the descriptions; authority records; digitized and electronic records; and tags, transcriptions and comments for NARA’s archival holdings available in the Catalog.'}\n",
      "{'name': 'Orcasound - bioacoustic data for marine conservation', 'description': 'Live-streamed and archived audio data (~2018-present) from underwater microphones (hydrophones) containing marine biological signals as well as ambient ocean noise. Hydrophone placement and passive acoustic monitoring effort prioritizes detection of orca sounds (calls, clicks, whistles) and potentially harmful noise. Geographic focus is on the US/Canada critical habitat of Southern Resident killer whales (northern CA to central BC) with initial focus on inland waters of WA. In addition to the raw lossy or lossless compressed data, we provide a growing archive of annotated bioacoustic bouts.'}\n",
      "{'name': 'Natural Earth', 'description': 'Natural Earth is a public domain map dataset available at 1:10m, 1:50m, and 1:110 million scales. Featuring tightly integrated vector and raster data, with Natural Earth you can make a variety of visually pleasing, well-crafted maps with cartography or GIS software.'}\n",
      "{'name': 'Clinical Proteomic Tumor Analysis Consortium 3 (CPTAC-3)', 'description': 'The Clinical Proteomic Tumor Analysis Consortium (CPTAC) is a national effort to accelerate the\\nunderstanding of the molecular basis of cancer through the application of large-scale proteome and\\ngenome analysis, or proteogenomics. CPTAC-3 is the Phase III of the CPTAC Initiative. The dataset\\ncontains open RNA-Seq Gene Expression Quantification data.\\n'}\n",
      "{'name': 'AgricultureVision', 'description': 'Agriculture-Vision aims to be a publicly available large-scale aerial agricultural image dataset that is high-resolution, multi-band, and with multiple types of patterns annotated by agronomy experts.  The original dataset affiliated with the 2020 CVPR paper includes 94,986 512x512images sampled from 3,432 farmlands with nine types of annotations: double plant, drydown, endrow, nutrient deficiency, planter skip, storm damage, water, waterway and weed cluster.  All of these patterns have substantial impacts on field conditions and the final yield. These farmland images were captured between 2017 and 2019 across multiple growing seasons in numerous farming locations in the US.  Each field image contains four color channels: Near-infrared (NIR), Red, Green and Blue.  We first randomly split the 3,432 farmland images with a 6/2/2 train/val/test ratio. We then assign each sampled image to the split of the farmland image they are cropped from. This guarantees that no cropped images from the same farmland will appear in multiple splits in the final dataset.  The generated (supervised) Agriculture-Vision dataset thus contains 56,944/18,334/19,708 train/val/test images.\\nAdditionally, we continue to grow this dataset.  In 2021 as a part of the [Prize Challenge at CVPR](https://www.agriculture-vision.com/agriculture-vision-2021/prize-challenge-2021), we have added sequences of full-field imagery across 52 fields to promote the use of weakly supervised methods.\\n'}\n",
      "{'name': '1950 Census Population Schedules, Enumeration District Maps, and Enumeration District Descriptions', 'description': 'The 1950 Census population schedules were created by the Bureau of the Census in an attempt to enumerate every person living in the United States on April 1, 1950, although some persons were missed. The 1950 census population schedules were digitized by the National Archives and Records Administration (NARA) and released publicly on April 1, 2022.\\nThe 1950 Census enumeration district maps contain maps of counties, cities, and other minor civil divisions that show enumeration districts, census tracts, and related boundaries and numbers used for each census. The coverage is nation wide and includes territorial areas.\\nThe 1950 Census enumeration district descriptions contain written descriptions of census districts, subdivisions, and enumeration districts.\\n'}\n",
      "{'name': 'NOAA World Ocean Database (WOD)', 'description': \"The World Ocean Database (WOD) is the largest uniformly formatted, quality-controlled, publicly available historical subsurface ocean profile database. From Captain Cook's second voyage in 1772 to today's automated Argo floats, global aggregation of ocean variable information including temperature, salinity, oxygen, nutrients, and others vs. depth allow for study and understanding of the changing physical, chemical, and to some extent biological state of the World's Oceans. Browse the bucket via the AWS S3 explorer: https://noaa-wod-pds.s3.amazonaws.com/index.html\\n\"}\n",
      "{'name': 'GeoNet Aotearoa New Zealand Data', 'description': 'GeoNet provides geological hazard information for Aotearoa New Zealand. This dataset contains data and products recorded by the GeoNet sensor network. The dataset currently include GNSS data and additional datasets will be added in the near future. GNSS (Global Navigation Satellite System) data include raw data in proprietary and Receiver Independent Exchange Format (RINEX) and local tie-in survey conducted during equipment changes, more details can be found on [the GeoNet geodetic page](https://www.geonet.org.nz/data/types/geodetic) website. Coastal gauge data include relative measurement of sea level measured by tsunami monitoring gauges. Raw and quality control data are provided in CREX format (Character Form for the Representtion and eXchange of metereological data), more details can be found on [the GeoNet coastal tsunami monitoring gauges page](https://www.geonet.org.nz/data/types/tidal_gauges). Camera images data include webcam images from the GeoNet Volcano monitoring network and Built Environment Instrumentation Programme, more details can be found on [the GeoNet camera page](https://www.geonet.org.nz/data/types/camera). Waveform data include raw data from weak and strong motion instruments of the GeoNet seismic networks, more details can be found on [the GeoNet seismic waveform page](https://www.geonet.org.nz/data/types/seismic_waveforms). Seismic data products include strong motion derived data, more details can be found on [the GeoNet Strong Motion products page](https://strongmotion.geonet.org.nz).'}\n",
      "{'name': 'Digital Earth Africa Coastlines', 'description': \"Africa's long and dynamic coastline is subject to a wide range of pressures, including extreme weather and climate, sea level rise and human development. Understanding how the coastline responds to these pressures is crucial to managing this region, from social, environmental and economic perspectives.\\nThe Digital Earth Africa Coastlines (provisional) is a continental dataset that includes annual shorelines and rates of coastal change along the entire African coastline from 2000 to the present.\\nThe product combines satellite data from the Digital Earth Africa program with tidal modelling to map the typical location of the coastline at mean sea level for each year. The product enables trends of coastal erosion and growth to be examined annually at both a local and continental scale, and for patterns of coastal change to be mapped historically and updated regularly as data continues to be acquired. This allows current rates of coastal change to be compared with that observed in previous years or decades.\\nThe ability to map shoreline positions for each year provides valuable insights into whether changes to the coastline are the result of particular events or actions, or a process of more gradual change over time. This information can enable scientists, managers and policy makers to assess impacts from the range of drivers impacting the coastlines and potentially assist planning and forecasting for future scenarios.\\n\"}\n",
      "{'name': 'OpenAQ', 'description': 'Global, aggregated physical air quality data from public data sources provided by government, research-grade and other sources. These awesome groups do the hard work of measuring these data and publicly sharing them, and our community makes them more universally-accessible to both humans and machines.'}\n",
      "{'name': 'Enriched Topical-Chat Dataset for Knowledge-Grounded Dialogue Systems', 'description': 'This dataset provides extra annotations on top of the publicly released\\nTopical-Chat dataset(https://github.com/alexa/Topical-Chat) which will help in reproducing the results in our paper\\n\"Policy-Driven Neural Response Generation for Knowledge-Grounded Dialogue Systems\" (https://arxiv.org/abs/2005.12529?context=cs.CL). \\nThe dataset contains 5 files: train.json, valid_freq.json, valid_rare.json, test_freq.json and test_rare.json. \\nEach of these files will have additional annotations on top of the original Topical-Chat dataset.\\nThese specific annotations are: dialogue act annotations and knowledge sentence annotations.\\nThe annotations were computed automatically using off the shelf models which are mentioned in the README.txt\\n'}\n",
      "{'name': 'Fly Brain Anatomy: FlyLight Gen1 and Split-GAL4 Imagery', 'description': \"This data set, made available by Janelia's FlyLight project, consists of fluorescence images \\nof Drosophila melanogaster driver lines, aligned to standard templates, and stored in formats \\nsuitable for rapid searching in the cloud. Additional data will be added as it is published. \\n\"}\n",
      "{'name': 'Transiting Exoplanet Survey Satellite (TESS)', 'description': 'The Transiting Exoplanet Survey Satellite (TESS) is a multi-year survey that will discover exoplanets in orbit around bright stars across the entire sky using high-precision photometry.  The survey will also enable a wide variety of stellar astrophysics, solar system science, and extragalactic variability studies. More information about TESS is available at [MAST](https://archive.stsci.edu/tess/) and the [TESS Science Support Center](https://heasarc.gsfc.nasa.gov/docs/tess/).\\n'}\n",
      "{'name': 'NOAA National Water Model CONUS Retrospective Dataset', 'description': \"The NOAA National Water Model Retrospective dataset contains input and output from multi-decade CONUS retrospective simulations. These simulations used meteorological input fields from meteorological retrospective datasets. The output frequency and fields available in this historical NWM dataset differ from those contained in the real-time operational NWM forecast model.\\n<br/>\\n<br/>\\nOne application of this dataset is to provide historical context to current near real-time streamflow, soil moisture and snowpack conditions. The retrospective data can be used to infer flow frequencies and perform temporal analyses with hourly streamflow output and 3-hourly land surface output. This dataset can also be used in the development of end user applications which require a long baseline of data for system training or verification purposes. <br />\\n<br/>\\n<br/>\\nCurrently there are three versions of the NWM retrospective dataset\\n<br/>\\n<br/>\\nA 42-year (February 1979 through December 2020) retrospective simulation using version 2.1 of the National Water Model.\\nA 26-year (January 1993 through December 2018) retrospective simulation using version 2.0 of the National Water Model.\\nA 25-year (January 1993 through December 2017) retrospective simulation using version 1.2 of the National Water Model.\\n<br/>\\n<br/>\\nVersion 2.1 uses forcings from the Office of Water Prediction Analysis of Record for Calibration (AORC) dataset while Version 2.0 and version 1.2 use input meteorological forcing from the North American Land Data Assimilation (NLDAS) data set. Note that no streamflow or other data assimilation is performed within any of the NWM retrospective simulations.\\n<br/>\\n<br/>\\nNWM Retrospective data is available in two formats, NetCDF and Zarr.  The NetCDF files contain the full set of NWM output data, while the Zarr files contain a subset of NWM output fields that vary with model version. \\n<br/>\\n<br/>\\nNWM V2.1:  All model output and forcing input fields are available in the NetCDF format.  All model output fields along with the precipitation forcing field are available in the Zarr format.\\nNWM V2.0:  All model output fields are available in NetCDF format. Model channel output including streamflow and related fields are available in Zarr format.\\nNWM V1.2:  All model output fields are available in NetCDF format.\\n<br/>\\n<br/>\\nA table listing the data available within each NetCDF and Zarr file is located in the '[documentation page](https://github.com/NOAA-Big-Data-Program/bdp-data-docs/blob/main/nwm/README.md)'.  This data includes meteorological NWM forcing inputs along with NWM hydrologic and land surface outputs, and varies by version number. \\n\"}\n",
      "{'name': 'Visual Anomaly (VisA)', 'description': 'Largest Visual Anomaly detection dataset containing objects from 12 classes in 3 domains across 10,821(9,621 normal and 1,200 anomaly) images. Both image and pixel level annotations are provided.'}\n",
      "{'name': 'ESA WorldCover', 'description': 'The European Space Agency (ESA) WorldCover is a global land cover map with 11 different land cover classes produced at 10m resolution based on combination of both Sentinel-1 and Sentinel-2 data. In areas where Sentinel-2 images are covered by clouds for an extended period of time, Sentinel-1 data then provides complimentary information on the structural characteristics of the observed land cover. Therefore, the combination of Sentinel-1 and Sentinel-2 data makes it possible to update the land cover map almost in real time. WorldCover Map has been produced for 2020 (01 January to 31 December) with a global coverage as part of the 5th Earth Observation Envelope Programme (EOEP-5). It provides valuable information for applications such as biodiversity, food security, carbon assessment and climate modelling. More information can be found on the [WorldCover website](https://esa-worldcover.org/en) and the product [User Manual](https://esa-worldcover.s3.amazonaws.com/v100/2020/docs/WorldCover_PUM_V1.0.pdf).'}\n",
      "{'name': 'RarePlanes', 'description': 'RarePlanes is a unique open-source machine learning dataset from CosmiQ Works and AI.Reverie that incorporates both real and synthetically generated satellite imagery. The RarePlanes dataset specifically focuses on the value of AI.Reverie synthetic data to aid computer vision algorithms in their ability to automatically detect aircraft and their attributes in satellite imagery. Although other synthetic/real combination datasets exist, RarePlanes is the largest openly-available very high resolution dataset built to test the value of synthetic data from an overhead perspective. The real portion of the dataset consists of 253 Maxar WorldView-3 satellite scenes spanning 112 locations and 2,142 km^2 with 14,700 hand-annotated aircraft. The accompanying synthetic dataset is generated via AI.Reverie’s novel simulation platform and features 50,000 synthetic satellite images with ~630,000 aircraft annotations.'}\n",
      "{'name': 'Sentinel-2', 'description': \"The [Sentinel-2 mission](https://sentinel.esa.int/web/sentinel/missions/sentinel-2) is\\na land monitoring constellation of two satellites that provide high resolution\\noptical imagery and provide continuity for the current SPOT and Landsat missions.\\nThe mission provides a global coverage of the Earth's land surface every 5 days,\\nmaking the data of great use in on-going studies. L1C data are available from\\nJune 2015 globally. L2A data are available from November 2016 over Europe\\nregion and globally since January 2017.\\n\"}\n",
      "{'name': 'Atmospheric Models from Météo-France', 'description': 'Global and high-resolution regional atmospheric models from Météo-France.\\n\\n- ARPEGE World covers the entire world at a base horizontal resolution of 0.5° (~55km) between grid points, it predicts weather out up to 114 hours in the future.\\n\\n- ARPEGE Europe covers Europe and North-Africa at a base horizontal resolution of 0.1° (~11km) between grid points, it predicts weather out up to 114 hours in the future.\\n\\n- AROME France covers France at a base horizontal resolution of 0.025° (~2.5km) between grid points, it predicts weather out up to 42 hours in the future.\\n\\n- AROME France HD covers France and neighborhood at a base horizontal resolution of 0.01° (~1.5km) between grid points, it predicts weather out up to 42 hours in the future.\\n\\nDozens of atmospheric variables are available through this datase: temperatures, winds, precipitation...\\n\\nOur work is based on open-data from Météo-France, but we are not affiliated or endorsed by Météo-France.\\n'}\n",
      "{'name': 'Analysis Ready Sentinel-1 Backscatter Imagery', 'description': 'The [Sentinel-1 mission](https://sentinel.esa.int/web/sentinel/missions/sentinel-1) is a constellation of\\nC-band Synthetic Aperature Radar (SAR) satellites from the European Space Agency launched since 2014.\\nThese satellites collect observations of radar backscatter intensity day or night, regardless of the\\nweather conditions, making them enormously valuable for environmental monitoring.\\nThese radar data have been processed from original Ground Range Detected (GRD) scenes into a Radiometrically\\nTerrain Corrected, tiled product suitable for analysis. This product is available over the Contiguous United States (CONUS)\\nsince 2017 when Sentinel-1 data became globally available.\\n'}\n",
      "{'name': 'SILO climate data on AWS', 'description': \"[SILO](https://www.longpaddock.qld.gov.au/silo) is a database of Australian [climate data](https://www.longpaddock.qld.gov.au/silo/about/climate-variables) from 1889 to the present. It provides continuous, daily time-step [data products](https://www.longpaddock.qld.gov.au/silo/about/data-products) in ready-to-use [formats](https://www.longpaddock.qld.gov.au/silo/about/file-formats-and-samples) for research and operational applications.\\nSILO's gridded datasets (in NetCDF and GeoTiff formats) are hosted on AWS Public Data. Point data (at both station and grid cell locations) are available from the [SILO website](https://www.longpaddock.qld.gov.au/silo/). Incremental update files for mirroring point datasets at station locations are also available on AWS Public Data.\\n\"}\n",
      "{'name': 'Beat Acute Myeloid Leukemia (AML) 1.0', 'description': 'Beat AML 1.0 is a collaborative research program involving 11 academic medical centers who worked\\ncollectively to better understand  drugs and drug combinations that should be prioritized for\\nfurther development within clinical and/or molecular subsets of acute myeloid leukemia (AML)\\npatients. Beat AML 1.0 provides the largest-to-date dataset on primary acute myeloid leukemia\\nsamples offering genomic, clinical, and drug response.\\n\\nThis dataset contains open Clinical Supplement and RNA-Seq Gene Expression Quantification data.\\n\\nThis dataset also contains controlled Whole Exome Sequencing (WXS) and RNA-Seq Aligned Reads, WXS Annotated Somatic Mutation,\\nWXS Raw Somatic Mutation, and RNA-Seq Splice Junction Quantification  \\n'}\n",
      "{'name': 'Provision of Web-Scale Parallel Corpora for Official European Languages (ParaCrawl)', 'description': 'ParaCrawl is a set of large parallel corpora to/from English for all official EU languages by a broad web crawling effort. State-of-the-art methods are applied for the entire processing chain from identifying web sites with translated text all the way to collecting, cleaning and delivering parallel corpora that are ready as training data for CEF.AT and translation memories for DG Translation.\\n'}\n",
      "{'name': 'Broad Genome References', 'description': 'Broad maintained human genome reference builds hg19/hg38 and decoy references.'}\n",
      "{'name': 'Aristo Mini Corpus', 'description': '1,197,377 science-relevant sentences'}\n",
      "{'name': 'NREL Wind Integration National Dataset', 'description': \"Released to the public as part of the Department of Energy's Open Energy Data Initiative,\\nthe [Wind Integration National Dataset (WIND)](https://www.nrel.gov/grid/wind-toolkit.html)\\nis an update and expansion of the Eastern Wind Integration Data Set and\\nWestern Wind Integration Data Set. It supports the next generation of wind\\nintegration studies.\\n\"}\n",
      "{'name': 'First Street Foundation (FSF) Flood Risk Summary Statistics', 'description': 'CSV files of flood statistics for the 48 contiguous states at the congressional district, county, and zip code level. The CSV for each of these geographical extents includes statistics on the amount of properties at risk according to FEMA, the number of properties at risk according to First Street Foundation, and the difference between the two.'}\n",
      "{'name': 'NASA NEX', 'description': \"A collection of Earth science datasets maintained by NASA, including climate change projections and satellite images of the Earth's surface.\"}\n",
      "{'name': 'Sentinel-1 SLC dataset for Germany', 'description': 'The Sentinel1 Single Look Complex (SLC) unzipped dataset contains Synthetic Aperture Radar (SAR) data from the European Space Agency’s Sentinel-1 mission. Different from the zipped data provided by ESA, this dataset allows direct access to individual swaths required for a given study area, thus drastically minimizing the storage and downloading time requirements of a project. Since the data is stored on S3, users can utilize the boto3 library and s3 get_object method to read the entire content of the object into the memory for processing, without actually having to download it. The Sentinel-1 constellation consists of two satellites equipped with SAR sensors and a combined revisit time of six days. SAR imagery gets recorded regardless of weather conditions and daylight, which makes it ideally suited for monitoring land-use changes, surface deformations, land applications, oil spills, sea-ice, natural hazards, and for emergency response. In its current first stage, the dataset covers the entirety of Germany and is being updated continuously. As a next stage, the dataset will provide up-to-date coverage of the sentinel-1 SLC data over Europe.\\nThis dataset is retrieved from Alaska Satellite Facility (ASF) and consists of all Sentinel1-SLC imagery from the beginning (2014) to present.\\n'}\n",
      "{'name': 'NASA / USGS Europa Controlled Observations', 'description': \"The Solid State Imager (SSI) on NASA's Galileo spacecraft acquired more than 500 images of Jupiter's moon, Europa. These images vary from relatively low-resolution hemispherical imaging, to high-resolution targeted images that cover a small portion of the surface. Here we provide a set of 481 minimally processed, projected Galileo images with photogrammetrically improved locations on Europa's surface. These individual images were subsequently used as input into a set of 92 observation mosaics. <br/><br/>\\nThese images provide users with nearly the entire Galileo Europa imaging dataset at its native resolution and with improved relative image locations. The Solid State Imager on NASA's Galileo spacecraft provided the only moderate- to high-resolution images of Jupiter's moon, Europa. Unfortunately, uncertainty in the position and pointing of the spacecraft, as well as the position and orientation of Europa, when the images were acquired resulted in significant errors in image locations on the surface. The result of these errors is that images acquired during different Galileo orbits, or even at different times during the same orbit, are significantly misaligned (errors of up to 100 km on the surface). <br/><br/>\\nThe dataset provides a set of individual images that can be used for scientific analysis and mission planning activities.\\n\"}\n",
      "{'name': 'Open Observatory of Network Interference (OONI)', 'description': 'A free software, global observation network for detecting censorship, surveillance and traffic manipulation on the internet.'}\n",
      "{'name': 'Downscaled Climate Data for Alaska', 'description': 'This dataset contains historical and projected dynamically downscaled climate data for the State of Alaska and surrounding regions at 20km spatial resolution and hourly temporal resolution. Select variables are also summarized into daily resolutions. This data was produced using the Weather Research and Forecasting (WRF) model (Version 3.5). We downscaled both ERA-Interim historical reanalysis data (1979-2015) and both historical and projected runs from 2 GCM’s from the Coupled Model Inter-comparison Project 5 (CMIP5): GFDL-CM3 and NCAR-CCSM4 (historical run: 1970-2005 and RCP 8.5: 2006-2100).'}\n",
      "{'name': 'COVID-19 Molecular Structure and Therapeutics Hub', 'description': 'Aggregating critical information to accelerate drug discovery for the molecular modeling and simulation community.\\nA community-driven data repository and curation service for molecular structures, models, therapeutics, and\\nsimulations related to computational research related to therapeutic opportunities for COVID-19\\n(caused by the SARS-CoV-2 coronavirus).\\n'}\n",
      "{'name': 'Tabula Muris', 'description': 'Tabula Muris is a compendium of single cell transcriptomic data from the model organism *Mus musculus* comprising more than 100,000 cells from 20 organs and tissues. These data represent a new resource for cell biology, reveal gene expression in poorly characterized cell populations, and allow for direct and controlled comparison of gene expression in cell types shared between tissues, such as T-lymphocytes and endothelial cells from different anatomical locations. Two distinct technical approaches were used for most organs: one approach, microfluidic droplet-based 3’-end counting, enabled the survey of thousands of cells at relatively low coverage, while the other, FACS-based full length transcript analysis, enabled characterization of cell types with high sensitivity and coverage. The cumulative data provide the foundation for an atlas of transcriptomic cell biology. See: https://www.nature.com/articles/s41586-018-0590-4\\n'}\n",
      "{'name': 'JMA Himawari-8/9', 'description': 'Himawari-8, stationed at 140E, owned and operated by the Japan Meteorological Agency (JMA), is a geostationary meteorological satellite, with Himawari-9  as on-orbit back-up, that provides constant and uniform coverage of east Asia, and the west and central Pacific regions from around 35,800 km above the equator with an orbit corresponding to the period of the earth’s rotation. This allows JMA weather offices to perform uninterrupted observation of environmental phenomena such as typhoons, volcanoes, and general weather systems. Archive data back to July 2015 is available for Full Disk (AHI-L1b-FLDK) products in the bucket. For questions regarding Himawari-8 imagery specifications, visit the JMA site at https://www.data.jma.go.jp/mscweb/en/himawari89/space_segment/spsg_ahi.html.  For examples of Full Disk Himawari-8 imagery coverage, visit the NOAA Himawari-8 data page at https://www.goes.noaa.gov/f_himawari-8.html.'}\n",
      "{'name': 'Quoref', 'description': '24K Question/Answer (QA) pairs over 4.7K paragraphs, split between train (19K QAs), development (2.4K QAs) and a hidden test partition (2.5K QAs).'}\n",
      "{'name': \"Department of Energy's Open Energy Data Initiative (OEDI)\", 'description': \"Data released under the Department of Energy's Open Energy Data Initiative\\n(DOE). The Open Energy Data Initiative (OEDI) aims to improve and automate\\naccess of high-value energy data sets across the U.S. Department of Energy’s\\n(DOE’s) programs, offices, and national laboratories. OEDI aims to make data\\nactionable and discoverable by researchers and industry to accelerate\\nanalysis and advance innovation.\\n\"}\n",
      "{'name': 'Reasoning Over Paragraph Effects in Situations (ROPES)', 'description': '14k QA pairs over 1.7K paragraphs, split between train (10k QAs), development (1.6k QAs) and a hidden test partition (1.7k QAs).'}\n",
      "{'name': 'Reference Elevation Model of Antarctica (REMA)', 'description': 'The Reference Elevation Model of Antarctica - 2m GSD Digital Elevation Models (DEMs) and mosaics from 2009 to the present. The REMA project seeks to fill the need for high-resolution time-series elevation data in the Antarctic. The time-dependent nature of the strip DEM files allows users to perform change detection analysis and to compare observations of topography data acquired in different seasons or years. The mosaic DEM tiles are assembled from multiple strip DEMs with the intention of providing a more consistent and comprehensive product over large areas. REMA data is constructed from in-track and cross-track high-resolution (~0.5 meter) imagery acquired by the Maxar constellation of optical imaging satellites.'}\n",
      "{'name': 'New Jersey Statewide Digital Aerial Imagery Catalog', 'description': 'The New Jersey Office of GIS, NJ Office of Information Technology manages a series of 11 digital orthophotography and scanned aerial photo maps collected at various years ranging from 1930 to 2017. Each year’s worth of imagery are available as Cloud Optimized GeoTIFF (COG) files and some years are available as compressed MrSID and/or JP2 files.  Additionally, each year of imagery is organized into a tile grid scheme covering the entire geography of New Jersey.  Many years share the same tiling grid while others have unique grids as defined by the project at the time.'}\n",
      "{'name': 'Basic Local Alignment Sequences Tool (BLAST) Databases', 'description': 'A centralized repository of pre-formatted BLAST databases created by the National Center for Biotechnology Information (NCBI).'}\n",
      "{'name': 'Community Earth System Model v2 Large Ensemble (CESM2 LENS)', 'description': 'The US National Center for Atmospheric Research partnered with the IBS Center for Climate Physics in South Korea to generate the CESM2 Large Ensemble which consists of 100 ensemble members at 1 degree spatial resolution covering the period 1850-2100 under CMIP6 historical and SSP370 future radiative forcing scenarios. Data sets from this ensemble were made downloadable via the Climate Data Gateway on June 14th, 2021.\\nNCAR has copied a subset (currently ~500 TB) of CESM2 LENS data to Amazon S3 as part of the AWS Public Datasets Program. To optimize for large-scale analytics we have represented the data as ~275 Zarr stores format accessible through the Python Xarray library. Each Zarr store contains a single physical variable for a given model run type and temporal frequency (monthly, daily).\\n'}\n",
      "{'name': 'Safecast', 'description': 'An ongoing collection of radiation and air quality measurements taken by devices involved in the Safecast project.'}\n",
      "{'name': 'The Massively Multilingual Image Dataset (MMID)', 'description': \"MMID is a large-scale, massively multilingual dataset of images paired with the words they represent collected at the [University of Pennsylvania](https://upenn.edu).\\nThe dataset is doubly parallel: for each language, words are stored parallel to images that represent the word, _and_ parallel to the word's translation into English (and corresponding images.)\\n\"}\n",
      "{'name': 'AWS Public Blockchain Data', 'description': 'The AWS Public Blockchain Data provide datasets from the Bitcoin and Ethereum blockchains. The blockchain data is transformed into multiple tables as compressed Parquet files partitioned by date to allow efficient access for most common analytics queries.'}\n",
      "{'name': 'Wizard of Tasks', 'description': 'Wizard of Tasks (WoT) is a dataset containing conversations for Conversational Task Assistants (CTAs). A CTA is a conversational agent whose goal is to help humans to perform real-world tasks. A CTA can help in exploring available tasks, answering task-specific questions and guiding users through step-by-step instructions. WoT contains about 550 conversations with ~18,000 utterances in two domains, i.e., Cooking and Home Improvement.'}\n",
      "{'name': 'Hubble Space Telescope Public Data', 'description': 'The Hubble Space Telescope (HST) is one of the most productive scientific instruments ever created. This dataset contains calibrated and raw data for all of the currently active instruments on HST: ACS, COS, STIS and WFC3.\\n'}\n",
      "{'name': 'OpenEEW', 'description': 'Grillo has developed an IoT-based earthquake early-warning system,\\nwith sensors currently deployed in Mexico, Chile, Puerto Rico and Costa Rica,\\nand is now opening its entire archive of unprocessed accelerometer\\ndata to the world to encourage the development of new algorithms\\ncapable of rapidly detecting and characterizing earthquakes in\\nreal time.\\n'}\n",
      "{'name': 'World Bank - Light Every Night', 'description': 'Light Every Night - World Bank Nighttime Light Data – provides open access to all nightly imagery and data from the Visible Infrared Imaging Radiometer Suite Day-Night Band (VIIRS DNB) from 2012-2020 and the Defense Meteorological Satellite Program Operational Linescan System (DMSP-OLS) from 1992-2013. The underlying data are sourced from the NOAA National Centers for Environmental Information (NCEI) archive. Additional processing by the University of Michigan enables access in Cloud Optimized GeoTIFF format (COG) and search using the Spatial Temporal Asset Catalog (STAC) standard. The data is published and openly available under the terms of the World Bank’s open data license.'}\n",
      "{'name': 'SUCHO Ukrainian Cultural Heritage Web Archives', 'description': 'The dataset contains web archives of Open Access collections of digitised cultural heritage from more than 3,000+ websites of Ukrainian cultural institutions, such as museums, libraries or archives. The web archives have been produced by SUCHO, which is a volunteer group of more than 1,300 international cultural heritage professionals – librarians, archivists, researchers, programmers - who have joined forces to save as much digitised cultural heritage during the 2022 invasion of Ukraine before the servers hosting them get destroyed, damaged or go offline for any other reason. The web archives were created using the tools of the Webrecorder Open Source project in the open WACZ format: https://webrecorder.github.io/wacz-spec/1.1.1/. WACZ files are zipped containers of WARC (Web Archive Format) files enriched with metadata, which can contain several crawls in a single file. The file sizes can range from a few MBs to several TBs.\\n'}\n",
      "{'name': 'NOAA Unified Forecast System Short-Range Weather (UFS SRW) Application', 'description': 'The \"[Unified Forecast System (UFS)](https://ufscommunity.org/)\" is a community-based, coupled, comprehensive Earth Modeling System. It supports \" [multiple applications](https://ufscommunity.org/science/aboutapps/)\" with different forecast durations and spatial domains. The UFS Short-Range Weather (SRW) Application figures among these applications. It targets predictions of atmospheric behavior on a limited spatial domain and on time scales from minutes to several days. The SRW Application includes a prognostic atmospheric model, pre-processor, post-processor, and community workflow for running the system end-to-end. The \"[SRW Application Users\\'s Guide](https://ufs-srweather-app.readthedocs.io/en/develop/)\" includes information on these components and provides detailed instructions on how to build and run the SRW Application. Users can access additional technical support via the \"[UFS Community Forum](https://forums.ufscommunity.org/)\" <br/> <br/> This data registry contains the data required to run the “out-of-the-box” SRW Application case. The SRW App requires numerous input files to run, including static datasets (fix files containing climatological information, terrain and land use data), initial condition data files, lateral boundary condition data files, and model configuration files (such as namelists). The SRW App experiment generation system also contains a set of workflow end-to-end (WE2E) tests that exercise various configurations of the system (e.g., different grids, physics suites). Data for running a subset of these WE2E tests are also included within this registry. <br/> <br/> Users can generate forecasts for dates not included in this data registry by downloading and manually adding raw model files for the desired dates. Many of these model files are publicly available and can be accessed via links on the \"[Developmental Testbed Center](https://dtcenter.org/nwp-containers-online-tutorial/publicly-available-data-sets)\" website.'}\n",
      "{'name': 'Sophos/ReversingLabs 20 Million malware detection dataset', 'description': 'A dataset intended to support research on machine learning\\ntechniques for detecting malware.  It includes metadata and EMBER-v2\\nfeatures for approximately 10 million benign and 10 million malicious\\nPortable Executable files, with disarmed but otherwise complete\\nfiles for all malware samples.  All samples are labeled using Sophos\\nin-house labeling methods, have features extracted using the\\nEMBER-v2 feature set, well as metadata obtained via the pefile\\npython library, detection counts obtained via ReversingLabs\\ntelemetry, and additional behavioral tags that indicate the rough\\nbehavior of the samples.\\n'}\n",
      "{'name': 'Central Weather Bureau OpenData', 'description': 'Various kinds of weather raw data and charts from Central Weather Bureau.'}\n",
      "{'name': 'CAFE60 reanalysis', 'description': 'The CSIRO Climate retrospective Analysis and Forecast Ensemble system: version 1 (CAFE60v1) provides a large ensemble retrospective analysis of the global climate system from 1960 to present with sufficiently many realizations and at spatio-temporal resolutions suitable to enable probabilistic climate studies. Using a variant of the ensemble Kalman filter, 96 climate state estimates are generated over the most recent six decades. These state estimates are constrained by monthly mean ocean, atmosphere and sea ice observations such that their trajectories track the observed state while enabling estimation of the uncertainties in the approximations to the retrospective mean climate over recent decades. Strongly coupled data assimilation (SCDA) is implemented via an ensemble transform Kalman filter in order to constrain a general circulation climate model to observations. Satellite (altimetry, sea surface temperature, sea ice concentration) and in situ ocean temperature and salinity profiles are directly assimilated each month, whereas atmospheric observations are sub-sampled from the JRA55 atmospheric reanalysis. Strong coupling is implemented via explicit cross domain covariances between ocean, atmosphere, sea ice and ocean biogeochemistry. Atmospheric and surface ocean fields are available at daily resolution and monthly resolution for the land, subsurface ocean and sea ice. The system also produces a complete data archive of initial conditions potentially enabling individual forecasts for all members each month over the 60 year period. The size of the ensemble and application of strongly coupled data assimilation lead to new insights for future reanalyses. CAFE60v1 has been validated in comparison to empirical indices of the major climate teleconnections and blocking from various reanalysis products (ERA5, JRA55, NCEP NR1). Estimates of the large scale ocean structure and transports have been compared to those derived from gridded observational products (WOA18, HadISST, ERSSTv5) and climate model projections (CMIP). Sea ice (extent, concentration and variability) and land surface (precipitation and surface air temperatures) are also compared to a variety of model (ERA5, CMIP) and observational (GPCP, AWAP, HadCRU4, GIOMAS, NSIDC, HadISST) products. This analysis shows that CAFE60v1 is a useful, comprehensive and unique data resource for studying internal climate variability and predictability, including the recent climate response to anthropogenic forcing on multi-year to decadal time scales.'}\n",
      "{'name': 'NIH NCBI PubMed Central (PMC) Article Datasets - Full-Text Biomedical and Life Sciences Journal Articles on AWS', 'description': \"PubMed Central® (PMC) is a free full-text archive of biomedical and life sciences journal article at the U.S. National Institutes of Health's National Library of Medicine (NIH/NLM). The PubMed Central (PMC) Article Datasets include full-text articles archived in PMC and made available under license terms that allow for text mining and other types of secondary analysis and reuse. The articles are organized on AWS based on general license type:<br/><br/>\\nThe PMC Open Access (OA) Subset, which includes all articles in PMC with a machine-readable Creative Commons license<br/><br/> The Author Manuscript Dataset, which includes all articles collected under a funder policy in PMC and made available in machine-readable formats for text mining<br/><br/>\\nThese datasets collectively span more than half of PMC’s total collection of full-text articles. PMC enables access to these datasets to expand the impact of open access and publicly-funded research; enable greater machine learning across the spectrum of scientific research; reach new audiences; and open new doors for discovery. The bucket in this registry contains individual articles in NISO Z39.96-2015 JATS XML format as well as in plain text as extracted from the XML. The bucket is updated daily with new and updated articles. Also included are file lists that include metadata for articles in each dataset.\"}\n",
      "{'name': 'Ford Multi-AV Seasonal Dataset', 'description': 'This research presents a challenging multi-agent seasonal dataset collected by a fleet of Ford autonomous vehicles at different days and times during 2017-18. The vehicles The vehicles were manually driven on an average route of 66 km in Michigan that included a mix of driving scenarios like the Detroit Airport, freeways, city-centres, university campus and suburban neighbourhood, etc. Each vehicle used in this data collection  is a Ford Fusion outfitted with an Applanix POS-LV inertial measurement unit (IMU), four HDL-32E Velodyne 3D-lidar scanners, 6 Point Grey 1.3 MP Cameras arranged on the rooftop for 360 degree coverage and 1 Pointgrey 5 MP camera mounted behind the windsheild for forward field of view. We present the seasonal variation in weather, lighting, construction and traffic conditions experienced in dynamic urban environments. This dataset can help design robust algorithms for autonomous vehicles and multi-agent systems. Each log in the dataset is time-stamped and contains raw data from all the sensors, calibration values, pose trajectory, ground truth pose, and 3D maps. All data is available in Rosbag format that can be visualized, modified and applied using the open-source Robot Operating System (ROS).'}\n",
      "{'name': 'ChEMBL - Data Lakehouse Ready', 'description': 'ChEMBL is a manually curated database of bioactive molecules with drug-like properties. It brings together chemical, bioactivity and genomic data to aid the translation of genomic information into effective new drugs. This representation of ChEMBL is stored in Parquet format and most easily utilized through Amazon Athena. Follow the documentation for install instructions (< 2 minute install). New ChEMBL releases occur sporadically; the most up to date information on ChEMBL releases can be found [here](https://chembl.gitbook.io/chembl-interface-documentation/downloads).'}\n",
      "{'name': 'NOAA S-111 Surface Water Currents Data', 'description': \"S-111 is a data and metadata encoding specification that is part of the [S-100 Universal Hydrographic Data Model](https://iho.int/en/s100-project), an international standard for hydrographic data. This collection of data contains surface water currents forecast guidance from [NOAA/NOS Operational Forecast Systems](https://tidesandcurrents.noaa.gov/models.html), a set of operational hydrodynamic nowcast and forecast modeling systems, for various U.S. coastal waters and the great lakes. The collection also contains surface current forecast guidance output from the [NCEP Global Real-Time Ocean Forecast System (GRTOFS)](https://polar.ncep.noaa.gov/global/) for some offshore areas. These datasets are encoded as HDF-5 files conforming to the S-111 specification, and are geospatially subset into individual tiles conforming to the NOAA/OCS Nautical Product Tiling Scheme, with filenames indicating the corresponding NOAA Electronic Navigational Chart (ENC) Cell Identifier.\\nA full set of S-111 tiles is created for each new model run cycle, which occurs four times per day for all models except for RTOFS, which updates only once per day. Files are organized using a path naming convention that includes the OFS identifier (e.g. 'cbofs' corresponding with output from the Chesapeake Bay Operational Forecast System) as well as the year, month, day, and hour corresponding with each model run initialization time. Each individual S-111 (HDF-5) file contains all forecast projections from a single model run for that geographic area. In other words, a single S-111 file will contain multiple gridded arrays each containing a forecast valid at a distinct time in the future, out to the forecast horizon of the underlying modeling system. All surface currents forecasts in this collection are computed at a depth of 4.5 meters below water surface, or half the water column depth, whichever is shallower.\\n\"}\n",
      "{'name': 'NOAA Fundamental Climate Data Records (FCDR)', 'description': \"NOAA's Climate Data Records (CDRs) are robust, sustainable, and scientifically sound climate records that provide trustworthy information on how, where, and to what extent the land, oceans, atmosphere and ice sheets are changing. These datasets are thoroughly vetted time series measurements with the longevity, consistency, and continuity to assess and measure climate variability and change. NOAA CDRs are vetted using standards established by the National Research Council (NRC).<br/><br/>\\nClimate Data Records are created by merging data from surface, atmosphere, and space-based systems across decades. NOAA’s Climate Data Records provides authoritative and traceable long-term climate records. NOAA developed CDRs by applying modern data analysis methods to historical global satellite data. This process can clarify the underlying climate trends within the data and allows researchers and other users to identify economic and scientific value in these records. NCEI maintains and extends CDRs by applying the same methods to present-day and future satellite measurements.<br/><br/>\\nFundamental CDRs are composed of sensor data (e.g. calibrated radiances, brightness temperatures) that have been improved and quality controlled over time, together with ancillary calibration data.\\n\"}\n",
      "{'name': 'Cornell EAS Data Lake', 'description': 'Earth & Atmospheric Sciences at Cornell University has created a public data lake of climate data. The data is stored in columnar storage formats (ORC) to make it straightforward to query using standard tools like Amazon Athena or Apache Spark. The data itself is originally intended to be used for building decision support tools for farmers and digital agriculture. The first dataset is the historical NDFD / NDGD data distributed by NCEP / NOAA / NWS. The NDFD (National Digital Forecast Database) and NDGD (National Digital Guidance Database) contain gridded forecasts and observations at 2.5km resolution for the Contiguous United States (CONUS). There are also 5km grids for several smaller US regions and non-continguous territories, such as Hawaii, Guam, Puerto Rico and Alaska. NOAA distributes archives of the NDFD/NDGD via its NOAA Operational Model Archive and Distribution System (NOMADS) in Grib2 format. The data has been converted to ORC to optimize storage space and to, more importantly, simplify data access via standard data analytics tools.\\n'}\n",
      "{'name': 'Genome Aggregation Database (gnomAD) - Data Lakehouse Ready', 'description': 'The [Genome Aggregation Database (gnomAD)](https://gnomad.broadinstitute.org/) is a resource developed by an international coalition of investigators that aggregates and harmonizes both exome and genome data from a wide range of large-scale human sequencing projects\\nSign up for the gnomAD mailing list [here](http://broad.io/gnomad_list). This dataset was derived from summary data from gnomAD release 3.1, available on the [Registry of Open Data on AWS](https://registry.opendata.aws/broad-gnomad/) for ready enrollment into the [Data Lake as Code](https://github.com/aws-samples/data-lake-as-code/tree/blog).\\n'}\n",
      "{'name': 'NOAA Joint Polar Satellite System (JPSS)', 'description': \"Satellites in the JPSS constellation gather global measurements of atmospheric, terrestrial and oceanic conditions, including sea and land surface temperatures, vegetation, clouds, rainfall, snow and ice cover, fire locations and smoke plumes, atmospheric temperature, water vapor and ozone. JPSS delivers key observations for the Nation's essential products and services, including forecasting severe weather like hurricanes, tornadoes and blizzards days in advance, and assessing environmental hazards such as droughts, forest fires, poor air quality and harmful coastal waters. Further, JPSS will provide continuity of critical, global observations of Earth’s atmosphere, oceans and land through 2038. The data will be available from 2012-01-19 to present.\"}\n",
      "{'name': 'Digital Earth Africa Cropland Extent Map (2019)', 'description': 'Digital Earth Africa\\'s cropland extent map (2019) shows the estimated location of croplands in Africa for the period January to December 2019. Cropland is defined as: \"a piece of land of minimum 0.01 ha (a single 10m x 10m pixel) that is sowed/planted and harvest-able at least once within the 12 months after the sowing/planting date.\" This definition will exclude non-planted grazing lands and perennial crops which can be difficult for satellite imagery to differentiate from natural vegetation. \\nThis provisional cropland extent map has a resolution of 10m, and was built using Copernicus Sentinel-2 satellite images from 2019. The cropland extent map was produced using extensive training data from regions across Africa, coupled with a Random Forest machine learning model. The continental service contains maps built separately for eight Agro-Ecological Zones (AEZs). For a detailed exploration of the methods used to produce the cropland extent map, read the Jupyter Notebooks in DE Africa’s crop-mask GitHub repository.\\n'}\n",
      "{'name': 'iHART Whole Genome Sequencing Data Set', 'description': 'iHART is the [Hartwell Foundation](http://www.thehartwellfoundation.org/)’s Autism Research and Technology Initiative. This release contains whole genome data from over 1000 families with 2 or more children with autism, of which biomaterials were provided by the Autism Genetic Resource Exchange ([AGRE](http://research.agre.org/)).'}\n",
      "{'name': 'National Herbarium of NSW', 'description': \"The National Herbarium of New South Wales is one of the most significant scientific, cultural and historical botanical resources in the Southern hemisphere. The 1.43 million preserved plant specimens have been captured as high-resolution images and the biodiversity metadata associated with each of the images captured in digital form. Botanical specimens date from year 1770 to today, and form voucher collections that document the distribution and diversity of the world's flora through time, particularly that of NSW, Austalia and the Pacific.\\n\\nThe data is used in biodiversity assessment, systematic botanical research, ecosystem conservation and policy development. The data is used by scientists, students and the public.\\n\"}\n",
      "{'name': 'Discrete Reasoning Over the content of Paragraphs (DROP)', 'description': 'The DROP dataset contains 96k Question and Answer pairs (QAs) over 6.7K paragraphs, split between train (77k QAs), development (9.5k QAs) and a hidden test partition (9.5k QAs).'}\n",
      "{'name': 'ECMWF ERA5 Reanalysis', 'description': \"ERA5 is the fifth generation of ECMWF atmospheric reanalyses of the global climate, and the first reanalysis produced as an operational service. It utilizes the best available observation data from satellites and in-situ stations, which are assimilated and processed using ECMWF's Integrated Forecast System (IFS) Cycle 41r2.\\nThe dataset provides all essential atmospheric meteorological parameters like, but not limited to, air temperature, pressure and wind at different altitudes, along with surface parameters like rainfall, soil moisture content and sea parameters like sea-surface temperature and wave height.\\nERA5 provides data at a considerably higher spatial and temporal resolution than its legacy counterpart ERA-Interim. ERA5 consists of high resolution version with 31 km horizontal resolution, and a reduced resolution ensemble version with 10 members. It is currently available since 2008, but will be continuously extended backwards, first until 1979 and then to 1950.\\nLearn more about ERA5 in Jon Olauson's paper [ERA5: The new champion of wind power modelling?](https://www.researchgate.net/publication/320084119_ERA5_The_new_champion_of_wind_power_modelling).\\n\"}\n",
      "{'name': 'Allen Mouse Brain Atlas', 'description': 'The Allen Mouse Brain Atlas is a genome-scale collection of cellular resolution gene expression profiles using in situ hybridization (ISH). Highly methodical data production methods and comprehensive anatomical coverage via dense, uniformly spaced sampling facilitate data consistency and comparability across >20,000 genes. The use of an inbred mouse strain with minimal animal-to-animal variance allows one to treat the brain essentially as a complex but highly reproducible three-dimensional tissue array. The entire Allen Mouse Brain Atlas dataset and associated tools are available through an unrestricted web-based viewing application (http://mouse.brain-map.org). The collection of > 650,000 images have been made available in this Open Data bucket to enable efficient access and analysis of the this dataset.\\n'}\n",
      "{'name': 'Humor patterns used for querying Alexa traffic', 'description': 'Humor patterns used for querying Alexa traffic when creating the taxonomy described in the paper \"“Alexa, Do You Want to Build a Snowman?” Characterizing Playful Requests to Conversational Agents\"  by Shani C., Libov A., Tolmach S., Lewin-Eytan L., Maarek Y., and Shahaf D. (CHI LBW 2022). These patterns corrospond to the researchers\\' hypotheses regarding what humor types are likely to appear in Alexa traffic. These patterns were used for querying Alexa traffic to evaluate these hypotheses.'}\n",
      "{'name': 'AI2 Reasoning Challenge (ARC) 2018', 'description': '7,787 multiple choice science questions and associated corpora'}\n",
      "{'name': 'Mouse Brain Anatomy: MouseLight Imagery', 'description': \"This data set, made available by Janelia's MouseLight project, consists of \\nimages and neuron annotations of the Mus musculus brain, stored in formats suitable\\nfor viewing and annotation using the HortaCloud cloud-based annotation system.\\n\"}\n",
      "{'name': 'NASA Earth Exchange Global Daily Downscaled Projections (NEX-GDDP-CMIP6)', 'description': 'The NEX-GDDP-CMIP6 dataset is comprised of global downscaled climate\\nscenarios derived from the General Circulation Model (GCM) runs\\nconducted under the Coupled Model Intercomparison Project Phase 6\\n(CMIP6) and across two of the four \"Tier 1\" greenhouse gas emissions\\nscenarios known as Shared Socioeconomic Pathways (SSPs). The CMIP6\\nGCM runs were developed in support of the Sixth Assessment Report\\nof the Intergovernmental Panel on Climate Change (IPCC AR6). This\\ndataset includes downscaled projections from ScenarioMIP model\\nruns for which daily scenarios were produced and distributed\\nthrough the Earth System Grid Federation. The purpose of this dataset\\nis to provide a set of global, high resolution, bias-corrected\\nclimate change projections that can be used to evaluate climate\\nchange impacts on processes that are sensitive to finer-scale climate\\ngradients and the effects of local topography on climate conditions.\\n'}\n",
      "{'name': 'Serratus: Ultra-deep Search for Novel Viruses - Versioned Data Release', 'description': 'Serratus is a collaborative open science project for ultra-rapid discovery of known and unknown coronaviruses in response to the COVID-19 pandemic through re-analysis of publicly available genomic data. Our resulting vertebrate viral alignment data is explorable via the [Serratus Explorer](https://serratus.io/explorer) and directly accessible on Amazon S3.'}\n",
      "{'name': 'Nanopore Reference Human Genome', 'description': 'This dataset includes the sequencing and assembly of a reference standard human genome (GM12878) using the MinION nanopore sequencing instrument with the R9.4 1D chemistry.'}\n",
      "{'name': 'NOAA Rapid Refresh Forecast System (RRFS) [Prototype]', 'description': 'The Rapid Refresh Forecast System (RRFS) is the National Oceanic and Atmospheric Administration’s (NOAA) next generation convection-allowing, rapidly-updated ensemble prediction system, currently scheduled for operational implementation in 2024. The operational configuration will feature a 3 km grid covering North America and include deterministic forecasts every hour out to 18 hours, with deterministic and ensemble forecasts to 60 hours four times per day at 00, 06, 12, and 18 UTC.The RRFS will provide guidance to support forecast interests including, but not limited to, aviation, severe convective weather, renewable energy, heavy precipitation, and winter weather on timescales where rapidly-updated guidance is particularly useful.<br/><br/>\\n\\nThe RRFS is underpinned by the [Unified Forecast System (UFS)](https://ufscommunity.org/), a community-based Earth modeling initiative, and benefits from collaborative development efforts across NOAA, academia, and research institutions.<br/><br/>\\n\\nThis bucket provides access to real time, experimental RRFS prototype output as of October 2022. This bucket also holds output from past experimental RRFS prototypes that were evaluated as a part of NOAA testbed projects. The immediate section describes the data for the real time system. The section that follows thereafter describes outputs from three past NOAA Testbed experiments. <br/><br/>\\n<hr/>\\nReal time, experimental RRFS Prototype output  <br/><br/>\\n\\nThe real-time RRFS prototype is experimental and evolving. It is not under 24x7 monitoring and is not operational. Output may be delayed or missing. Outputs will change. When significant changes to output take place, this description will be updated.<br/><br/>\\n\\nWe currently provide hourly deterministic forecasts at 3 km grid spacing over the CONUS out to 60 hours at 00 and 12 UTC, and out to 18 hours at other times. Future enhancements will include an ensemble forecast component and expansion to the planned North American domain. All forecasts are initialized from a hybrid 3DEnVar data assimilation system with hourly updates.\\n\\nOutput is available on the S3 bucket for every third cycle, and is organized by cycle day and time of day.  For example, `rrfs_a/rrfs_a.20221012/00/` contains the forecast initialized at 00 UTC on 12 October 2022.  Users will find two types of output in GRIB2 format.  The first is:<br/><br/>\\nrrfs.t00z.natlev.f018.conus_3km.grib2 <br/><br/>\\nMeaning that this is the RRFS_A initialized at 00 UTC, covers the CONUS domain, and is the native level post-processed gridded data at hour 18.  This output is on a Lambert Conic Conformal domain at 3 km grid spacing.<br/><br/>\\nThe second output file in grib2 format is:<br/><br/>\\nrrfs.t00z.prslev.f018.conus_3km.grib2 <br/><br/>\\nMeaning that this is the pressure level post-processed gridded data.\\n\\n<br/><br/>\\n<hr/>  \\nPast output from NOAA Testbed Experiments  \\n\\n<br/><br/>\\n\\nThis bucket also provides datasets from three of the 2021 NOAA Testbed Experiments. During each of these experiments, a prototype version of RRFS under development was run. The following is a high-level overview dates and RRFS configurations for each of the Testbed Experiments.<br/><br/>\\n\\n2021 Hazardous Weather Testbed (HWT) Spring Forecast Experiment (May 3 through June 4 2021) and 2021 Hydrometeorological Testbed Annual Flash Flood and Intense Rainfall Experiment (FFaIR) (June 21 through July 23 2021, excluding the week of July 4). A 9-member multi-physics ensemble with stochastic perturbations run once per day at 3 km grid spacing covering North America out to 60 hours. Initial conditions and lateral boundary conditions are taken from the GFS and GEFS.<br/><br/>\\n2021-2022 Hydrometeorological Testbed Winter Weather Experiment (WWE) (mid November through mid-March). Select cases only. Deterministic forecasts were run once per day at 00 UTC at 3 km grid spacing covering the CONUS out to 60 hours.  A 36-member, 3 km ensemble Kalman filter data assimilation approach is implemented through hourly cycling starting at 18 UTC on the previous day.<br/><br/>\\n\\nFor each cycle of the HWT and FFaIR experiments, the dataset is organized by cycle day, time of day, and member. For example, `rrfs.20210504/00/mem01/` contains the forecast from ensemble member 1 initialized at 00 UTC on 04 May 2021. Users will find two types of output in GRIB2 format. The first is:<br/><br/>\\nrrfs.t00z.mem01.naf024.grib2<br/><br/>\\nMeaning that this is RRFS ensemble member 1 initialized at 00 UTC, covers the North American domain, and is the post-processed gridded data at hour 24. This output is on a rotated latitude-longitude domain at 3 km grid spacing. These are large files and users may wish to subset or re-project the grid after downloading. We recommend using the [WGRIB2 application](https://www.cpc.ncep.noaa.gov/products/wesley/wgrib2/) for such purposes.<br/><br/>\\nThe second output file in grib2 format is as follows:<br/><br/>\\nrrfs.t00z.mem01.testbed.conusf020.grib2<br/><br/>\\nThese grids have been subset from the much larger North American domain to a CONUS domain on a Lambert Conic Conformal projection and also contain significantly fewer fields, resulting in smaller files. <br/><br/>\\nGraphics for select runs are also included in a plots/ directory under each experiment day for quick, yet simple visualization.<br/><br/>\\n\\nFor each cycle of the WWE, the dataset is organized by cycle day and time of day.  For example, `rrfs.20220306/00/` contains data for the forecast initialized at 00 UTC on 06 March 2022.  The initial conditions for the 36 ensemble members are located in the `ens_ics/mem???` subdirectories.  Users will find two types of output in GRIB2 format in the post subdirectories.  The first is:<br/><br/>\\nBGDAWP.GrbF12<br/><br/>\\nMeaning that this is the forecast initialized at 00 UTC, covers the CONUS domain, and is the pressure level post-processed gridded data at forecast hour 18.  This output is on a Lambert Conic Conformal grid at 3 km grid spacing.<br/><br/>\\nThe second output file in grib2 format is as follows:<br/><br/>\\ntestbed.conusf030.grib2<br/><br/>\\nThese grids contain significantly fewer fields, resulting in smaller files.<br/><br/>\\n\\nThis work is supported by the Unified Forecast System Research to Operation (UFS R2O) Project which is jointly funded by NOAA’s Office of Science and Technology Integration (OSTI) of National Weather Service (NWS) and Weather Program Office (WPO), [Joint Technology Transfer Initiative (JTTI)] of the Office of Oceanic and Atmospheric Research (OAR).<br/><br/>\\n*DISCLAIMER* The output provided here is experimental and is subject to change, outages, and gaps. Please contact the data managers for additional information or questions.\\n'}\n",
      "{'name': 'Protein Data Bank 3D Structural Biology Data', 'description': 'The \"Protein Data Bank (PDB) archive\" was established in 1971 as the first open-access digital data archive in biology. It is a collection of three-dimensional (3D) atomic-level structures of biological macromolecules (i.e., proteins, DNA, and RNA) and their complexes with one another and various small-molecule ligands (e.g., US FDA approved drugs, enzyme co-factors). For each PDB entry (unique identifier: 1abc or PDB_0000001abc) multiple data files contain information about the 3D atomic coordinates, sequences of biological macromolecules, information about any small molecules/ligands present in the entry, details about the structure-determination experiment, authors and publication information, experimental data, and the wwPDB validation report. Additional content stored in the archive includes documentation, summary reports, and software (among others).\\nThe PDB is a jointly-managed core archive of the Worldwide Protein Data Bank partnership [RCSB Protein Data Bank (RCSB PDB, rcsb.org); Protein Data Bank in Europe (PDBe, pdbe.org); Protein Data Bank Japan (PDBj, pdbj.org); Electron Microscopy Data Bank (EMDB, emdb-empiar.org); and Biological Magnetic Resonance Bank (BMRB, bmrb.io)].\\nRCSB PDB serves as the wwPDB-designated Archive Keeper for the Protein Data Bank.\\nAdditional wwPDB Core Archives are as follows:\\nElectron Microscopy Data Bank (wwPDB-designated Archive Keeper: EMDB)\\nBiological Magnetic Resonance Bank (wwPDB-designated Archive Keeper: BMRB)\\n'}\n",
      "{'name': 'NapierOne Mixed File Dataset', 'description': 'NapierOne is a modern cybersecurity mixed file data set, primarily aimed at, but not limited to, ransomware detection and forensic analysis. The dataset contains over 500,000 distinct files, representing 44 distinct popular file types. It was designed to address the known deficiency in research reproducibility and improve consistency by facilitating research replication and repeatability. The data set was inspired by the Govdocs1 data set and it is intended that ‘NapierOne’ be used as a complement to this original data set. An investigation was performed with the goal of determining the common files types currently in use. No specific research was found that explicitly provided this information, so an alternative consensus approach was employed. This involved combining the findings from multiple sources of file type usage into an overall ranked list. After which 5,000 real-world example files were gathered, and a specific data subset was created, for each of the common file types identified. In some circumstances, multiple data subsets were created for a specific file type, each subset representing a specific characteristic for that file type. For example, there are multiple data subsets for the ZIP file type with each subset containing examples of a specific compression method. Ransomware execution tends to produce files that have high entropy, so examples of file types that naturally have this attribute are also present. The resulting entire data set comprises of more than 90 separate data subsets divided between 44 distinct file types, resulting in over 500,000 unique files in total. Currently, the data set contains examples of the following file types APK, BIN, BMP, CSS, CSV, DOC, DOCX, DWG, ELF, EPS,EPUB, EXE, GIF, GZIP, HTML, ICS, JS, JPG, JSON, MKV, MP3, MP4, ODS, OXPS, PDF, PNG, PPT, PPTX, PS1, RAR, SVG, TAR, TIF, TXT, WEBP, XLS, XLSX, XML, ZIP, ZLIB, 7Zip'}\n",
      "{'name': 'ICGC on AWS', 'description': 'The International Cancer Genome Consortium (ICGC) coordinates projects with the common aim of accelerating research into the causes and control of cancer. The PanCancer Analysis of Whole Genomes (PCAWG) study is an international collaboration to identify common patterns of mutation in whole genomes from ICGC. More than 2,400 consistently analyzed genomes corresponding to over 1,100 unique ICGC donors are now freely available on Amazon S3 to credentialed researchers subject to ICGC data sharing policies.'}\n",
      "{'name': 'A Realistic Cyber Defense Dataset (CSE-CIC-IDS2018)', 'description': 'This dataset is the result of a collaborative project between the Communications Security Establishment (CSE) and The Canadian Institute for Cybersecurity (CIC) that use the notion of profiles to generate cybersecurity dataset in a systematic manner. It incluides a detailed description of intrusions along with abstract distribution models for applications, protocols, or lower level network entities. The dataset includes seven different attack scenarios, namely Brute-force, Heartbleed, Botnet, DoS, DDoS, Web attacks, and infiltration of the network from inside. The attacking infrastructure includes 50 machines and the victim organization has 5 departments includes 420 PCs and 30 servers. This dataset includes the network traffic and log files of each machine from the victim side, along with 80 network traffic features extracted from captured traffic using CICFlowMeter-V3.\\nFor more information on the creation of this dataset, see this paper by researchers at the Canadian Institute for Cybersecurity (CIC) and the University of New Brunswick (UNB): [Toward Generating a New Intrusion Detection Dataset and Intrusion Traffic Characterization](http://www.scitepress.org/Papers/2018/66398/66398.pdf).\\n'}\n",
      "{'name': 'Google Brain Genomics Sequencing Dataset for Benchmarking and Development', 'description': 'To facilitate benchmarking and development, the Google Brain group has sequenced 9 human samples covering the Genome in a Bottle truth sets on different sequencing instruments, sequencing modalities (Illumina short read and Pacific BioSciences long read), sample preparation protocols, and for whole genome and whole exome capture. The original source of these data are [gs://google-brain-genomics-public](https://console.cloud.google.com/storage/browser/brain-genomics-public/research/sequencing;tab=objects?prefix=&forceOnObjectsSortingFiltering=false).'}\n",
      "{'name': 'Conformational Space of Short Peptides', 'description': 'Co-managed by [Toyoko](toyoko.io) and the [Structural Biology Group at the Universidad Nacional de Quilmes](http://ufq.unq.edu.ar/sbg/), this dataset allows us to explore the conformational space of all possible peptides using the 20 common amino acids. It consists of a collection of exhaustive molecular dynamics simulations of tripeptides and pentapeptides.'}\n",
      "{'name': 'Aristo Tuple KB', 'description': '294,000 science-relevant tuples'}\n",
      "{'name': 'Airborne Object Tracking Dataset', 'description': 'Airborne Object Tracking (AOT) is a collection of 4,943 flight sequences of around 120 seconds each, collected at 10 Hz in diverse conditions. There are 5.9M+ images and 3.3M+ 2D annotations of airborne objects in the sequences. There are 3,306,350 frames without labels as they contain no airborne objects. For images with labels, there are on average 1.3 labels per image. All airborne objects in the dataset are labelled.'}\n",
      "{'name': 'Seattle Alzheimer’s Disease Brain Cell Atlas (SEA-AD)', 'description': \"The Seattle Alzheimer’s Disease Brain Cell Atlas (SEA-AD) consortium strives to gain a deep molecular and cellular understanding of the early pathogenesis of Alzheimer’s disease and is funded by the National Institutes on Aging (NIA U19AG060909). The SEA-AD datasets available here comprise single cell profiling (transcriptomics and epigenomics) and quantitative neuropathology. To explore gene expression and chromatin accessibility information, the single-cell profiling data includes: snRNAseq and snATAC-seq data from the SEA-AD donor cohort (aged brains which span the spectrum of Alzheimer's Disease pathology) and neurotypical reference brains. To explore key pathological proteins and cell types of interest to Alzheimer’s disease, the neuropathology data includes: full resolution brightfield images, images processed and segmented in HALO® image analysis software, image annotations, and quantification summary files for the relevant stains including Abeta (6E10), IBA1, a-Synuclein, GFAP, H&E-LFB, NeuN, pTau(AT8), and pTDP43.\\n\"}\n",
      "{'name': 'Cell Painting Gallery', 'description': 'The Cell Painting Gallery is a collection of image datasets created using the [Cell Painting](https://pubmed.ncbi.nlm.nih.gov/27560178/) assay. \\nThe images of cells are captured by microscopy imaging, and reveal the response of various labeled cell components to whatever treatments are tested, which can include genetic perturbations, chemicals or drugs, or different cell types. \\nThe datasets can be used for diverse applications in basic biology and pharmaceutical research, such as identifying disease-associated phenotypes, understanding disease mechanisms, and predicting a drug’s activity, toxicity, or mechanism of action ([Chandrasekaran et al 2020](https://carpenter-singh-lab.broadinstitute.org/files/anne/files/141_Chandrasekaran_NatRevDrugDiscov_2020.pdf)). \\nThis collection is maintained by the [Carpenter–Singh lab](https://carpenter-singh-lab.broadinstitute.org/) and the [Cimini lab](https://cimini-lab.broadinstitute.org/) at the [Broad Institute](https://www.broadinstitute.org/).\\nA human-friendly listing of datasets, instructions for accessing them, and other documentation is at the [corresponding GitHub page](https://github.com/broadinstitute/cellpainting-gallery) about the Gallery.\\n'}\n",
      "{'name': 'The Singapore Nanopore Expression Data Set', 'description': 'The Singapore Nanopore Expression (SG-NEx) project is an international collaboration to generate reference transcriptomes and a comprehensive benchmark data set for long read Nanopore RNA-Seq. Transcriptome profiling is done using PCR-cDNA sequencing (PCR-cDNA), amplification-free cDNA sequencing (direct cDNA), direct sequencing of native RNA (direct RNA), and short read RNA-Seq. The SG-NEx core data includes 5 of the most commonly used cell lines and it is extended with additional cell lines and samples that cover a broad range of human tissues. All core samples are sequenced with at least 3 high quality replicates. For a subset of samples spike-in RNAs are used and matched m6A profiling data is available.'}\n",
      "{'name': 'Africa Soil Information Service (AfSIS) Soil Chemistry', 'description': 'This dataset contains soil infrared spectral data and paired soil property\\nreference measurements for georeferenced soil samples that were collected\\nthrough the Africa Soil Information Service (AfSIS) project, which lasted\\nfrom 2009 through 2018. In this release, we include data collected during\\nPhase I (2009-2013.) Georeferenced samples were collected from 19 countries\\nin Sub-Saharan African using a statistically sound sampling scheme,\\nand their soil properties were analyzed using *both* conventional soil\\ntesting methods and spectral methods (infrared diffuse reflectance\\nspectroscopy). The two types of data can be paired to form a training\\ndataset for machine learning, such that certain soil properties can be\\nwell-predicted through less expensive spectral techniques.\\n'}\n",
      "{'name': 'Earth Observation Data Cubes for Brazil', 'description': 'Earth observation (EO) data cubes produced from analysis-ready data (ARD) of CBERS-4, Sentinel-2 A/B and Landsat-8 satellite images for Brazil. The datacubes are regular in time and use a hierarchical tiling system. Further details are described in [Ferreira et al. (2020)](https://www.mdpi.com/2072-4292/12/24/4033).'}\n",
      "{'name': '2021 Amazon Last Mile Routing Research Challenge Dataset', 'description': 'The 2021 Amazon Last Mile Routing Research Challenge was an innovative research initiative led by Amazon.com and supported by the Massachusetts Institute of Technology’s Center for Transportation and Logistics. Over a period of 4 months, participants were challenged to develop innovative machine learning-based methods to enhance classic optimization-based approaches to solve the travelling salesperson problem, by learning from historical routes executed by Amazon delivery drivers. The primary goal of the Amazon Last Mile Routing Research Challenge was to foster innovative applied research in route planning, building on recent advances in predictive modeling, and using a real-world problem and data. The dataset released for the research challenge includes route-, stop-, and package-level features for 9,184 historical routes performed by Amazon drivers in 2018 in five metropolitan areas in the United States. This real-world dataset excludes any personally identifiable information (all route and package identifiers have been randomly regenerated and related location data have been obfuscated to ensure anonymity). Although multiple synthetic benchmark datasets are available in the literature, the dataset of the 2021 Amazon Last Mile Routing Research Challenge is the first large and publicly available dataset to include instances based on real-world operational routing data. The dataset is fully described and formally introduced in the following Transportation Science article: https://pubsonline.informs.org/doi/10.1287/trsc.2022.1173'}\n",
      "{'name': 'Humor Detection from Product Question Answering Systems', 'description': 'This dataset provides labeled humor detection from product question answering systems.\\nThe dataset contains 3 csv files: [Humorous.csv](https://humor-detection-pds.s3-us-west-2.amazonaws.com/Humorous.csv) \\ncontaining the humorous product questions, \\n[Non-humorous-unbiased.csv](https://humor-detection-pds.s3-us-west-2.amazonaws.com/Non-humorous-unbiased.csv) containing \\nthe non-humorous prodcut questions from the same products as the humorous one, and, \\n[Non-humorous-biased.csv](https://humor-detection-pds.s3-us-west-2.amazonaws.com/Non-humours-biased.csv) containing \\nthe non-humorous prodcut questions from randomly selected products. \\nEach file has the question, product description, image url to the product, \\nand a label if the question is humorous or not.\\n'}\n",
      "{'name': 'Amazonia EO satellite on AWS', 'description': 'Imagery acquired\\nby Amazonia-1 satellite.\\nThe\\nimage files are recorded and processed by Instituto Nacional de Pesquisas\\nEspaciais (INPE) and are converted to Cloud Optimized Geotiff\\nformat in order to optimize its use for cloud based applications.\\nWFI Level 4 (Orthorectified) scenes are being\\ningested daily starting from 08-29-2022, the complete\\nLevel 4 archive will be ingested by the end of October 2022.\\n'}\n",
      "{'name': 'Pancreatic Cancer Organoid Profiling', 'description': 'This study generated a collection of patient-derived pancreatic normal and cancer organoids and it was sequenced using Whole Genome Sequencing (WGS), Whole Exome Sequencing (WXS) and RNA-Seq as well as matched tumor and normal tissue if available. The study provides a valuable resource for pancreatic cancer researchers.\\nThe dataset contains open RNA-Seq Gene Expression Quantification data and controlled WGS/WXS/RNA-Seq Aligned Reads, WXS Annotated Somatic Mutation, WXS Raw Somatic Mutation, and RNA-Seq Splice Junction Quantification.\\n'}\n",
      "{'name': 'STOIC2021 Training', 'description': 'The STOIC project collected Computed Tomography (CT) images of 10,735 individuals suspected of being infected with SARS-COV-2 during the first wave of the pandemic in France, from March to April 2020. For each patient in the training set, the dataset contains binary labels for COVID-19 presence, based on RT-PCR test results, and COVID-19 severity, defined as intubation or death within one month from the acquisition of the CT scan. This S3 bucket contains the training sample of the STOIC dataset as used in the STOIC2021 challenge on grand-challenge.org.'}\n",
      "{'name': 'MIMIC-III (‘Medical Information Mart for Intensive Care’)', 'description': 'MIMIC-III (‘Medical Information Mart for Intensive Care’) is a large, \\nsingle-center database comprising information relating to patients \\nadmitted to critical care units at a large tertiary care hospital. \\nData includes vital signs, medications, laboratory measurements, \\nobservations and notes charted by care providers, fluid balance, \\nprocedure codes, diagnostic codes, imaging reports, hospital length \\nof stay, survival data, and more. The database supports applications \\nincluding academic and industrial research, quality improvement initiatives, \\nand higher education coursework. The MIMIC-III dataset is freely-available. \\nResearchers seeking to use the database must formally request access. For details, see \\n[the getting started page](https://mimic.physionet.org/gettingstarted/access/).  Once you \\nhave a PhysioNet account, you must enable access to the MIMIC-III dataset from your \\nAWS account.  To do this, please [input your AWS account number](https://physionet.org/settings/cloud/), and \\n[request access to the MIMIC-III Clinical Database on AWS](https://physionet.org/projects/mimiciii/1.4/request_access/2).\\n'}\n",
      "{'name': 'NOAA Wave Ensemble Reforecast', 'description': 'This is a 20-year global wave reforecast generated by WAVEWATCH III model (https://github.com/NOAA-EMC/WW3) forced by GEFSv12 winds (https://noaa-gefs-retrospective.s3.amazonaws.com/index.html). The wave ensemble was run with one cycle per day (at 03Z), spatial resolution of 0.25°X0.25° and temporal resolution of 3 hours. There are five ensemble members (control plus four perturbed members) and, once a week (Wednesdays), the ensemble is expanded to eleven members. The forecast range is 16 days and, once a week (Wednesdays), it extends to 35 days. More information about the wave modeling, wave grids and calibration can be found in the WAVEWATCH III regtest ww3_ufs1.3 (https://github.com/NOAA-EMC/WW3/tree/develop/regtests/ww3_ufs1.3).\\n<br/><br/>\\nThe 20 years of reforecast results were analyzed and quality-controlled. Three output types are available\\n<br/><br/>\\n1) Global wave fields, in grib2 format, with several variables including significant wave height, period, direction, and partitions;\\n<br/>\\n2) Point output tables, in netcdf format, containing time-series of significant wave height, period and direction, for 658 points (latitude/longitude informed) at the positions of wave buoys;  and,\\n<br/>\\n3) For the same positions, spectral outputs are available, in netcdf format, containing the full spectra (2D directional spectrum).\\n<br/>\\nEach file refers to one forecast cycle with date (year, month, day) written in the file name.\\n<br/>\\n<br/>\\nThis effort has been funded by a NOAA OAR/NWS Service Level Agreement (SLA) whereby NOAA/OAR supports R&D and transition needed for mission delivery of climate\\nservices within NWS. The SLA project is led by NWS/NCEP/Ocean Prediction Center in collaboration with NWS/NCEP/Environmental Modeling Center and NWS/NCEP/Climate \\nPrediction Center. This is a project also in cooperation with NOAA’s Atlantic Oceanographic and Meteorological Laboratory (AOML) and the University of Miami’s \\nCooperative Institute for Marine and Atmospheric Studies (CIMAS).\\n<br/>\\n<br/>\\nThe following github repository has examples and scripts to support users to download, visualize, and process the WAVEWATCH III output files\\n<br/>\\n<br/>\\nhttps://github.com/ricampos/gefswaves_reforecast\\n<br/>\\n<br/>\\nThe reforecast simulations were generated on the supercomputer Orion, which is funded via a grant from NOAA to support research activities in environmental  \\nmodeling, including weather modeling and simulation\\n<br/>\\n<br/>\\nhttps://www.noaa.gov/organization/information-technology/orion\\n'}\n",
      "{'name': 'AI2 Tablestore (November 2015 Snapshot)', 'description': '68 tables of curated facts'}\n",
      "{'name': 'NAIP on AWS', 'description': 'The National Agriculture Imagery Program (NAIP) acquires aerial imagery during the agricultural growing seasons in the continental U.S. This \"leaf-on\" imagery andtypically ranges from 60 centimeters to 100 centimeters in resolution and is available from the naip-analytic Amazon S3 bucket as 4-band (RGB + NIR) imagery in MRF format, on naip-source Amazon S3 bucket as 4-band (RGB + NIR) in uncompressed Raw GeoTiff format and naip-visualization as 3-band (RGB) Cloud Optimized GeoTiff format. NAIP data is delivered at the state level; every year, a number of states receive updates, with an overall update cycle of two or three years. More details on [NAIP](https://www.fsa.usda.gov/programs-and-services/aerial-photography/imagery-programs/naip-imagery/)'}\n",
      "{'name': 'NA-CORDEX - North American component of the Coordinated Regional Downscaling Experiment', 'description': 'The NA-CORDEX dataset contains regional climate change scenario data and guidance for North America, for use in impacts, decision-making, and climate science. The NA-CORDEX data archive contains output from regional climate models (RCMs) run over a domain covering most of North America using boundary conditions from global climate model (GCM) simulations in the CMIP5 archive. These simulations run from 1950–2100 with a spatial resolution of 0.22°/25km or 0.44°/50km.  This AWS S3 version of the data includes selected variables converted to Zarr format from the original NetCDF. Only daily data are currently available; all daily data were mapped to the Gregorian calendar. Sub-daily data may be added later. Both raw and bias-corrected data are available. Further details about this version of the dataset are available at the documentation link below.\\n'}\n",
      "{'name': 'iSDAsoil', 'description': 'iSDAsoil is a resource containing soil property predictions for the entire African continent, generated using machine learning. Maps for over 20 different soil properties have been created at 2 different depths (0-20 and 20-50cm).  Soil property predictions were made using machine learning coupled with remote sensing data and a training set of over 100,000 analyzed soil samples. Included in this dataset are images of predicted soil properties, model error and satellite covariates used in the mapping process.'}\n",
      "{'name': 'Foldingathome COVID-19 Datasets', 'description': \"[Folding@home](http://foldingathome.org) is a massively distributed computing project that uses biomolecular simulations to investigate the [molecular origins of disease](https://foldingathome.org/diseases/) and accelerate the discovery of new therapies. Run by the [Folding@home Consortium](https://foldingathome.org/about/the-foldinghome-consortium/), a worldwide network of research laboratories focusing on a variety of different diseases, Folding@home seeks to address problems in human health on a scale that is infeasible by another other means, sharing the results of these large-scale studies with the research community through [peer-reviewed publications](https://foldingathome.org/papers-results/) and publicly shared datasets. During the [COVID-19 epidemic](https://en.wikipedia.org/wiki/Coronavirus_disease_2019), Folding@home focused its resources on understanding the vulnerabilities in [SARS-CoV-2](https://en.wikipedia.org/wiki/Severe_acute_respiratory_syndrome_coronavirus_2), the virus that causes COVID-19 disease, and working closely with a number of experimental collaborators to accelerate progress toward effective therapies for treating COVID-19 and ending the pandemic. In the process, it created the world's first [exascale distributed computing resource](https://doi.org/10.1101/2020.06.27.175430), enabling it to generate valuable scientific datasets of unprecedented size. More information about Folding@home's COVID-19 research activities at the [Folding@home COVID-19 page](https://foldingathome.org/diseases/infectious-diseases/covid-19/). In addition to working directly with experimental collaborators and rapidly sharing new research findings through preprint servers, Folding@home has joined other researchers in committing to [rapidly share all COVID-19 research data](https://doi.org/10.1021/acs.jcim.0c00319), and has joined forces with [AWS](https://aws.amazon.com/) and the [Molecular Sciences Software Institute (MolSSI)](http://molssi.org) to share datasets of unprecedented side through the [AWS Open Data Registry](https://registry.opendata.aws/), indexing these massive datasets via the [MolSSI COVID-19 Molecular Structure and Therapeutics Hub](https://covid.molssi.org/). The complete index of all Folding@home datasets can be found [here](https://covid.molssi.org//org-contributions/#folding--home). This repository contains several major datasets from this effort and comprises the single largest collection of molecular simulation data ever released.\\n\"}\n",
      "{'name': 'Xiph.Org Test Media', 'description': 'Uncompressed video used for video compression and video processing research.'}\n",
      "{'name': 'Foundation Medicine Adult Cancer Clinical Dataset (FM-AD)', 'description': \"The Foundation Medicine Adult Cancer Clinical Dataset (FM-AD) is a study conducted by Foundation\\nMedicine Inc (FMI). Genomic profiling data for approximately 18,000 adult patients with a diverse\\narray of cancers was generated using FoundationeOne, FMI's commercially available, comprehensive\\ngenomic profiling assay. This dataset contains open Clinical and Biospecimen data.\\n\"}\n",
      "{'name': 'OpenNeuro', 'description': 'OpenNeuro is a database of openly-available brain imaging data. The data are shared according to a Creative Commons CC0 license, providing a broad range of brain imaging data to researchers and citizen scientists alike. The database primarily focuses on functional magnetic resonance imaging (fMRI) data, but also includes other imaging modalities including structural and diffusion MRI, electroencephalography (EEG), and magnetoencephalograpy (MEG). OpenfMRI is a project of the [Center for Reproducible Neuroscience at Stanford University](http://reproducibility.stanford.edu). Development of the OpenNeuro resource has been funded by the National Science Foundation, National Institute of Mental Health, National Institute on Drug Abuse, and the Laura and John Arnold Foundation.'}\n",
      "{'name': 'Kepler Mission Data', 'description': 'The Kepler mission observed the brightness of more than 180,000 stars near the Cygnus constellation at a 30 minute cadence for 4 years in order to find transiting exoplanets, study variable stars, and find eclipsing binaries. More information about the Kepler mission is available at [MAST](https://archive.stsci.edu/kepler/).\\n'}\n",
      "{'name': 'COVID-19 Harmonized Data', 'description': 'A harmonized collection of the core data pertaining to COVID-19 reported cases by geography, in a format prepared for analysis'}\n",
      "{'name': 'Textbook Question Answering (TQA)', 'description': '1,076 textbook lessons, 26,260 questions, 6229 images'}\n",
      "{'name': 'Consented Activities of People', 'description': 'The Consented Activities of People (CAP) dataset is a fine grained activity dataset for visual AI research curated using the [Visym Collector](https://www.visym.com/collector/) platform.'}\n",
      "{'name': 'iNaturalist Licensed Observation Images', 'description': 'iNaturalist is a community science effort in which participants share observations of living organisms that they encounter and document with photographic evidence, location, and date. The community works together reviewing these images to identify these observations to species. This collection represents the licensed images accompanying iNaturalist observations.'}\n",
      "{'name': 'Speedtest by Ookla Global Fixed and Mobile Network Performance Maps', 'description': 'Global fixed broadband and mobile (cellular) network performance, allocated to zoom level 16 web mercator tiles (approximately 610.8 meters by 610.8 meters at the equator). Data is provided in both Shapefile format as well as Apache Parquet with geometries represented in Well Known Text (WKT) projected in EPSG:4326. Download speed, upload speed, and latency are collected via the Speedtest by Ookla applications for Android and iOS and averaged for each tile. Measurements are filtered to results containing GPS-quality location accuracy.\\n'}\n",
      "{'name': 'Open NeuroData', 'description': 'This bucket contains multiple neuroimaging datasets (as Neuroglancer Precomputed Volumes) across multiple modalities and scales, ranging from nanoscale (electron microscopy), to microscale (cleared lightsheet microscopy and array tomography), and mesoscale (structural and functional magnetic resonance imaging). Additionally, many of the datasets include segmentations and meshes.'}\n",
      "{'name': 'Genome Aggregation Database (gnomAD)', 'description': 'The Genome Aggregation Database (gnomAD) is a resource developed by an international coalition of investigators that aggregates and harmonizes both exome and genome data from a wide range of large-scale human sequencing projects. The summary data provided here are released for the benefit of the wider scientific community without restriction on use.\\nThe v2 data set (GRCh37) spans 125,748 exome sequences and 15,708 whole-genome sequences from unrelated individuals. The v3 data set (GRCh38) spans 71,702 genomes, selected as in v2.\\nSign up for the gnomAD mailing list [here](http://broad.io/gnomad_list).\\n'}\n",
      "{'name': 'Refgenie reference genome assets', 'description': 'Pre-built refgenie reference genome data assets used for aligning and analyzing DNA sequence data.'}\n",
      "{'name': 'University of British Columbia Sunflower Genome Dataset', 'description': \"This dataset captures Sunflower's genetic diversity originating\\nfrom thousands of wild, cultivated, and landrace sunflower\\nindividuals distributed across North America.\\n\\nThe data consists of raw sequences and associated botanical metadata,\\naligned sequences (to three different reference genomes), and sets of\\nSNPs computed across several cohorts.\\n\"}\n",
      "{'name': 'AI2 Diagram Dataset (AI2D)', 'description': '4,817 illustrative diagrams for research on diagram understanding and associated question answering.'}\n",
      "{'name': 'Australasian Genomes', 'description': 'Australasian Genomes is the genomic data repository for the Threatened Species Initiative (TSI) and the ARC Centre for Innovations in Peptide and Protein Science (CIPPS). This repository contains reference genomes, transcriptomes, resequenced genomes and reduced representation sequencing data from Australasian species. Australasian Genomes is managed by the Australasian Wildlife Genomics Group (AWGG) at the University of Sydney on behalf of our collaborators within TSI and CIPPS.'}\n",
      "{'name': 'CAM6 Data Assimilation Research Testbed (DART) Reanalysis: Cloud-Optimized Dataset', 'description': 'This is a cloud-hosted subset of the CAM6+DART (Community Atmosphere Model version 6 Data Assimilation Research Testbed) Reanalysis dataset. These data products are designed to facilitate a broad variety of research using the NCAR CESM 2.1 (National Center for Atmospheric Research\\'s Community Earth System Model version 2.1), including model evaluation, ensemble hindcasting, data assimilation experiments, and sensitivity studies. They come from an 80 member ensemble reanalysis of the global troposphere and stratosphere using DART and CAM6. The data products represent states of the atmosphere consistent with observations from 2011 through 2019 at 1 degree horizontal resolution and weekly frequency. Each ensemble member is an equally likely description of the atmosphere, and is also consistent with dynamics and physics of CAM6. The dataset also contains corresponding land surface values at 6-hourly frequency.  This dataset is a reformatting, with no change to numerical values, of data from the \"CAM6 Data Assimilation Research Testbed (DART) Reanalysis\", [DOI:10.5065/JG1E-8525](https://doi.org/10.5065/JG1E-8525).\\n'}\n",
      "{'name': 'Medical Segmentation Decathlon', 'description': 'With recent advances in machine learning, semantic segmentation algorithms are becoming increasingly general purpose and translatable to unseen tasks. Many key algorithmic advances in the field of medical imaging are commonly validated on a small number of tasks, limiting our understanding of the generalisability of the proposed contributions. A model which works out-of-the-box on many tasks, in the spirit of AutoML, would have a tremendous impact on healthcare. The field of medical imaging is also missing a fully open source and comprehensive benchmark for general purpose algorithmic validation and testing covering a large span of challenges, such as: small data, unbalanced labels, large-ranging object scales, multi-class labels, and multimodal imaging, etc. This challenge and dataset aims to provide such resource through the open sourcing of large medical imaging datasets on several highly different tasks, and by standardising the analysis and validation process.'}\n",
      "{'name': 'New Jersey Statewide LiDAR', 'description': 'Elevation datasets in New Jersey have been collected over several years as several\\ndiscrete projects.  Each project covers a geographic area, which is a subsection of\\nthe entire state, and has differing specifications based on the available technology\\nat the time and project budget.  The geographic extent of one project may overlap that\\nof a neighboring project. Each of the 18 projects contains deliverable products such\\nas LAS (Lidar point cloud) files, unclassified/classified, tiled to cover project area;\\nrelevant metadata records or documents, most adhering to the Federal Geographic Data\\nCommittee’s (FGDC) Content Standard for Digital Geospatial Metadata (CSDGM); tiling\\nindex feature class or shapefile; flights lines feature class or shapefile; Digital\\nElevation Model in image format or Esri grid format; other derivative data products\\nsuch as contour lines feature class or shapefile.\\n'}\n",
      "{'name': 'NOAA Unified Forecast System Subseasonal to Seasonal Prototypes', 'description': 'The Unified Forecast System Subseasonal to Seasonal prototypes consist of reforecast data from the UFS atmosphere-ocean coupled model experimental prototype version 5, 6, 7, and 8 produced by the Medium Range and Subseasonal to Seasonal Application team of the UFS-R2O project. The UFS prototypes are the first dataset released to the broader weather community for analysis and feedback as part of the development of the next generation operational numerical weather prediction system from NWS. The datasets includes all the major weather variables for atmosphere, land, ocean, sea ice, and ocean waves.\\n<br/>\\n<br/>\\nAcknowledgment - The Unified Forecast System (UFS) atmosphere-ocean coupled model experimental version # data used in this study are made available through the UFS Research to Operations (UFS-R2O) project sponsored by the National Weather Service (NWS) Office of Science and Technology Integration (OSTI) Modeling Program Division and the National Oceanic and Atmospheric Administration (NOAA) Oceanic and Atmospheric Research (OAR) Weather Program Office (WPO).\\n'}\n",
      "{'name': 'ISERV', 'description': \"ISS SERVIR Environmental Research and Visualization System (ISERV) was a fully-automated prototype camera aboard the International Space Station that was tasked to capture high-resolution Earth imagery of specific locations at 3-7 frames per second. In the course of its regular operations during 2013 and 2014, ISERV's camera acquired images that can be used primaliry in use is environmental and disaster management.\"}\n",
      "{'name': 'Boreas Autonomous Driving Dataset', 'description': 'This autonomous driving dataset includes data from a 128-beam Velodyne Alpha-Prime lidar, a 5MP Blackfly camera, a 360-degree Navtech radar, and post-processed Applanix POS LV GNSS data. This dataset was collect in various weather conditions (sun, rain, snow) over the course of a year. The intended purpose of this dataset is to enable benchmarking of long-term all-weather odometry and metric localization across various sensor types. In the future, we hope to also support an object detection benchmark.'}\n",
      "{'name': 'NOAA Unified Forecast System (UFS) Marine Reanalysis: 1979-2019', 'description': 'The NOAA UFS Marine Reanalysis is a global sea ice ocean coupled reanalysis product produced by the marine data assimilation team of the UFS Research-to-Operation (R2O) project. Underlying forecast and data assimilation systems are based on the UFS model prototype version-6 and the Next Generation Global Ocean Data Assimilation System (NG-GODAS) release of the Joint Effort for Data assimilation Integration (JEDI) Sea Ice Ocean Coupled Assimilation (SOCA). Covering the 40 year reanalysis time period from 1979 to 2019, the data atmosphere option of the UFS coupled global atmosphere ocean sea ice (DATM-MOM6-CICE6) model was applied with two atmospheric forcing data sets: CFSR from 1979 to 1999 and GEFS from 2000 to 2019. Assimilated observation data sets include extensive space-based marine observations and conventional direct measurements of in situ profile data sets.\\n<br/><br/>\\nThis first UFS-marine interim reanalysis product is released to the broader weather and earth system modeling and analysis communities to obtain scientific feedback and applications for the development of the next generation operational numerical weather prediction system at the National Weather Service(NWS). The released file sets include two parts 1.) 1979 - 2019 UFS-DATM-MOM6-CICE6 model free runs and 2) 1979-2019 reanalysis cycle outputs (see descriptions embedded in each file set). Analyzed sea ice and ocean variables are ocean temperature, salinity, sea surface height, and sea ice concentration.\\n<br/><br/>\\nAcknowledgment - The UFS data assimilation and reanalysis and reforecast products produced from the UFS-R2O project are sponsored by the NWS Office of Science and Technology Integration (OSTI) Modeling Program Division and the National Oceanic and Atmospheric Administration’s(NOAA) Oceanic and Atmospheric Research (OAR) Weather Program Office (WPO).\\n'}\n",
      "{'name': 'ZINC Database', 'description': '3D models for molecular docking screens.'}\n",
      "{'name': 'SMN Hi-Res Weather Forecast over Argentina', 'description': 'The Servicio Meteorológico Nacional de Argentina (SMN-Arg), the National Meteorological Service of Argentina, shares its deterministic forecasts generated with WRF 4.0 (Weather and Research Forecasting) initialized at 00 and 12 UTC every day.\\n<br/>\\n<br/>\\nThis forecast includes some key hourly surface variables –2 m temperature, 2 m relative humidity, 10 m wind magnitude and direction, and precipitation–, along with other daily variables, minimum and maximum temperature.\\n<br/>\\n<br/>\\nThe forecast covers Argentina, Chile, Uruguay, Paraguay and parts of Bolivia and Brazil  in a Lambert conformal projection, with 4 km horizontal resolution. The maximum lead time is 72 hours. \\n<br/>\\n<br/>\\nForecasts are initialized with the NCEP-NOAA Global Forecast Systems analysis and forecasts ([GFS](https://www.ncei.noaa.gov/products/weather-climate-models/global-forecast)).\\n'}\n",
      "{'name': 'Global Database of Events, Language and Tone (GDELT)', 'description': \"This project monitors the world's broadcast, print,\\nand web news from nearly every corner of every country in\\nover 100 languages and identifies the people, locations,\\norganizations, counts, themes, sources, emotions,\\nquotes, images and events driving our global society every\\nsecond of every day.\\n\"}\n",
      "{'name': 'Amazon-PQA', 'description': 'Amazon product questions and their answers, along with the public product information.'}\n",
      "{'name': 'IRS 990 Filings (Spreadsheets)', 'description': 'Excerpts of electronic Form 990 and 990-EZ filings, converted to spreadsheet form. Additional fields being added regularly.'}\n",
      "{'name': 'Southern California Earthquake Data', 'description': 'This dataset contains ground motion velocity and acceleration seismic waveforms recorded by the Southern California Seismic Network (SCSN) and archived at the Southern California Earthquake Data Center (SCEDC). A Distributed Acousting Sensing (DAS) dataset is included.'}\n",
      "{'name': 'PoroTomo', 'description': \"Released to the public as part of the Department of Energy's Open Energy Data\\nInitiative, these data represent vertical and horizontal distributed acoustic\\nsensing (DAS) data collected as part of the Poroelastic Tomography (PoroTomo)\\nproject funded in part by the Office of Energy Efficiency and Renewable\\nEnergy (EERE), U.S. Department of Energy.\\n\"}\n",
      "{'name': 'NOAA Emergency Response Imagery', 'description': \"In order to support NOAA's homeland security and emergency response requirements, the National Geodetic Survey Remote Sensing Division (NGS/RSD) has the capability to acquire and rapidly disseminate a variety of spatially-referenced datasets to federal, state, and local government agencies, as well as the general public. Remote sensing technologies used for these projects have included lidar, high-resolution digital cameras, a film-based RC-30 aerial camera system, and hyperspectral imagers. Examples of rapid response initiatives include acquiring high resolution images with the Emerge/Applanix Digital Sensor System (DSS).\"}\n",
      "{'name': 'Pre- and post-purchase product questions', 'description': 'This dataset provides product related questions, including their textual content and gap, in hours, between purchase and posting time. \\nEach question is also associated with related product details, including its id and title.\\n'}\n",
      "{'name': 'TSBench', 'description': 'TSBench comprises thousands of benchmark evaluations for time series forecasting methods. It provides various metrics (i.e. measures of accuracy, latency, number of model parameters, ...) of 13 time series forecasting methods across 44 heterogeneous datasets. Time series forecasting methods include both classical and deep learning methods while several hyperparameters settings are evaluated for the deep learning methods.\\n\\nIn addition to the tabular data providing the metrics, TSBench includes the probabilistic forecasts of all evaluated methods for all 44 datasets. While the tabular data is small (about 10 MiB), the forecasts amount to almost 600 GiB of data.\\n'}\n",
      "{'name': 'The Genome Modeling System', 'description': 'The Genome Institute at Washington University has developed a high-throughput, fault-tolerant analysis information management system called the Genome Modeling System (GMS), capable of executing complex, interdependent, and automated genome analysis pipelines at a massive scale. The GMS framework provides detailed tracking of samples and data coupled with reliable and repeatable analysis pipelines. GMS includes a full system image with software and services, expandable from one workstation to a large compute cluster.'}\n",
      "{'name': 'NOAA Global Mosaic of Geostationary Satellite Imagery (GMGSI)', 'description': 'NOAA/NESDIS Global Mosaic of Geostationary Satellite Imagery (GMGSI) visible (VIS), shortwave infrared (SIR), longwave infrared (LIR) imagery, and water vaport imagery (WV) are composited from data from several geostationary satellites orbiting the globe, including the GOES-East and GOES-West Satellites operated by U.S. NOAA/NESDIS, the Meteosat-11 and Meteosat-8 satellites from theMeteosat Second Generation (MSG) series of satellites operated by European Organization for the Exploitation of Meteorological Satellites (EUMETSAT), and the Himawari-8 satellite operated by the Japan Meteorological Agency (JMA). GOES-East is positioned at 75 deg W longitude over the equator. GOES-West is located at 137.2 deg W longitude over the equator. Both satellites cover an area from the eastern Atlantic Ocean to the central Pacific Ocean region. The Meteosat-11 satellite is located at 0 deg E longitude to cover Europe and Africa regions. The Meteosat-8 satellite is located at 41.5 deg E longitude to cover the Indian Ocean region. The Himawari-8 satellite is located at 140.7 deg E longitude to cover the Asia-Oceania region. The visible imagery indicates cloud cover and ice and snow cover. The shortwave, or mid-infrared, indicates cloud cover and fog at night. The longwave, or thermal infrared, depicts cloud cover and land/sea temperature patterns. The water vapor imagery indicates the amount of water vapor contained in the mid to upper levels of the troposphere, with the darker grays indicating drier air and the brighter grays/whites indicating more saturated air. GMGSI composite images have an approximate 8 km (5 mile) horizontal resolution and are updated every hour.\\n'}\n",
      "{'name': 'Retired – UK Met Office Atmospheric Deterministic and Probabilistic Forecasts', 'description': '\\nAs of the 13th July, no further Met Office forecasts or data will be available via this service. <p>\\n\\nMeteorological data reusers now have an exciting opportunity to sample, experiment and evaluate\\nMet Office atmospheric model data, whilst also experiencing a transformative method of requesting\\ndata via Restful APIs on AWS.\\n\\nFor information about the data see the [Met Office website](https://www.metoffice.gov.uk/services/data/met-office-data-for-reuse/discovery).\\nFor examples of using the data check out the [examples repository](https://github.com/MetOffice/aws-earth-examples).\\nIf you need help and support using the data please raise an issue on the examples repository.\\n\\n **Please note:** Met Office continuously improves and updates its operational forecast models.\\nOur last update became effective 04/12/2019. Please find the details [here](https://www.metoffice.gov.uk/services/data/met-office-data-for-reuse/ps43_aws).\\n'}\n",
      "{'name': 'PROJ datum grids', 'description': 'Horizontal and vertical adjustment datasets for coordinate transformation to be used by PROJ 7 or later. PROJ is a generic coordinate transformation software that transforms geospatial coordinates from one coordinate reference system (CRS) to another. This includes cartographic projections as well as geodetic transformations.'}\n",
      "{'name': 'NOAA Climate Forecast System (CFS)', 'description': \"The Climate Forecast System (CFS) is a model representing the global interaction between Earth's oceans, land, and atmosphere. Produced by several dozen scientists under guidance from the National Centers for Environmental Prediction (NCEP), this model offers hourly data with a horizontal resolution down to one-half of a degree (approximately 56 km) around Earth for many variables. CFS uses the latest scientific approaches for taking in, or assimilating, observations from data sources including surface observations, upper air balloon observations, aircraft observations, and satellite observations. <br /> Please note that the data in this bucket are the CFSv2 Operational Forecasts. To obtain other CFSv2 products such as the Operational Analysis, please visit our [website](https://www.ncdc.noaa.gov/data-access/model-data/model-datasets/climate-forecast-system-version2-cfsv2).\"}\n",
      "{'name': 'Human PanGenomics Project', 'description': 'This dataset includes sequencing data, assemblies, and analyses for the offspring of ten parent-offspring trios.'}\n",
      "{'name': 'NOAA Operational Forecast System (OFS)', 'description': \"ANNOUNCEMENTS: [NOS OFS Version Updates and Implementation of Upgraded Oceanographic Forecast Modeling Systems for Lakes Superior and Ontario; Effective October 25, 2022}(https://www.weather.gov/media/notification/pdf2/scn22-91_nos_loofs_lsofs_v3.pdf)\\n<br/>\\n<br/>\\nFor decades, mariners in the United States have depended on NOAA's Tide Tables for the best estimate of expected water levels. These tables provide accurate predictions of the astronomical tide (i.e., the change in water level due to the gravitational effects of the moon and sun and the rotation of the Earth); however, they cannot predict water-level changes due to wind, atmospheric pressure, and river flow, which are often significant.\\n<br/>\\n<br/>\\nThe National Ocean Service (NOS) has the mission and mandate to provide guidance and information to support navigation and coastal needs. To support this mission, NOS has been developing and implementing hydrodynamic model-based [Operational Forecast Systems](https://tidesandcurrents.noaa.gov/forecast_info.html). \\n<br/>\\n<br/>\\nThis forecast guidance provides oceanographic information that helps mariners safely navigate their local waters. This national network of hydrodynamic models provides users with operational nowcast and forecast guidance (out to 48 – 120 hours) on parameters such as water levels, water temperature, salinity, and currents. These forecast systems are implemented in critical ports, harbors, estuaries, Great Lakes and coastal waters of the United States, and form a national backbone of real-time data, tidal predictions, data management and operational modeling.\\n<br/>\\n<br/>\\nNowcasts and forecasts are scientific predictions about the present and future states of water levels (and possibly currents and other relevant oceanographic variables, such as salinity and temperature) in a coastal area. These predictions rely on either observed data or forecasts from a numerical model. A nowcast incorporates recent (and often near real-time) observed meteorological, oceanographic, and/or river flow rate data. A nowcast covers the period from the recent past (e.g., the past few days) to the present, and it can make predictions for locations where observational data are not available. A forecast incorporates meteorological, oceanographic, and/or river flow rate forecasts and makes predictions for times where observational data will not be available. A forecast is usually initiated by the results of a nowcast.\\n<br/>\\n<br/>\\nOFS generally runs four times per day (every 6 hours) on NOAA's Weather and Climate Operational Supercomputing Systems (WCOSS) in a standard Coastal Ocean Modeling Framework (COMF) developed by the [Center for Operational Oceanographic Products and Services (CO-OPS)](https://tidesandcurrents.noaa.gov/). COMF is a set of standards and tools for developing and maintaining NOS’s hydrodynamic model–based operational forecast systems. The goal of COMF is to provide a standard and comprehensive software infrastructure to enhance ease of use, performance, portability, and interoperability of NOS’s operational forecast systems.\\n<br/>\\n<br/>\\n\"}\n",
      "{'name': 'Sounds of Central African landscapes', 'description': 'Archival soundscapes recorded in the rainforest landscapes of\\nCentral Africa, with a focus on the vocalizations of African forest\\nelephants (Loxodonta cyclotis).\\n'}\n",
      "{'name': 'Oregon Health & Science University Chronic Neutrophilic Leukemia Dataset', 'description': 'The OHSU-CNL study offers the whole exome and RNA-sequencing on a cohort of 100 cases with rare\\nhematologic malignancies such as Chronic neutrophilic leukemia (CNL), atypical chronic myeloid\\nleukemia (aCML), and unclassified myelodysplastic syndrome/myeloproliferative neoplasms\\n(MDS/MPN-U). This dataset contains open RNA-Seq Gene Expression Quantification data.\\n'}\n",
      "{'name': 'Ozone Monitoring Instrument (OMI) / Aura NO2 Tropospheric Column Density', 'description': 'NO2 tropospheric column density, screened for CloudFraction < 30% global daily\\n composite at 0.25 degree resolution for the temporal range of 2004 to May\\n2020. Original archive data in HDF5 has been processed into a [Cloud-Optimized\\nGeoTiff (COG)](https://www.cogeo.org/) format. Quality Assurance - This data\\nhas been validated by the NASA Science Team at Goddard Space Flight Center.\\n\\nCautionary Note: https://airquality.gsfc.nasa.gov/caution-interpretation.\\n'}\n",
      "{'name': 'IBL Neuropixels Brainwide Map on AWS', 'description': 'Electrophysiological recordings of mouse brain activity acquired using Neuropixels probes.'}\n",
      "{'name': 'ArcticDEM', 'description': 'ArcticDEM - 2m GSD Digital Elevation Models (DEMs) and mosaics from 2007 to the present. The ArticDEM project seeks to fill the need for high-resolution time-series elevation data in the Arctic. The time-dependent nature of the strip DEM files allows users to perform change detection analysis and to compare observations of topography data acquired in different seasons or years. The mosaic DEM tiles are assembled from multiple strip DEMs with the intention of providing a more consistent and comprehensive product over large areas. ArcticDEM data is constructed from in-track and cross-track high-resolution (~0.5 meter) imagery acquired by the Maxar constellation of optical imaging satellites.'}\n",
      "{'name': 'District of Columbia - Classified Point Cloud LiDAR', 'description': '\"LiDAR point cloud data for Washington, DC is available for anyone to use on Amazon S3.\\nThis dataset, managed by the Office of the Chief Technology Officer (OCTO), through the\\ndirection of the District of Columbia GIS program, contains tiled point cloud data for\\nthe entire District along with associated metadata.\"\\n'}\n",
      "{'name': 'The MIT Supercloud Dataset', 'description': 'Collection of parsed datacenter logs and time series data of hardware utilization from the MIT Supercloud system.'}\n",
      "{'name': 'Clinical Trial Sequencing Project - Diffuse Large B-Cell Lymphoma', 'description': 'The goal of the project is to identify recurrent genetic alterations (mutations, deletions,\\namplifications, rearrangements) and/or gene expression signatures. National Cancer Institute (NCI)\\nutilized whole genome sequencing and/or whole exome sequencing in conjunction with transcriptome\\nsequencing. The samples were processed and submitted for genomic characterization using pipelines\\nand procedures established within The Cancer Genome Analysis (TCGA) project.\\n'}\n",
      "{'name': 'ZEST: ZEroShot  learning  from Task descriptions', 'description': 'ZEST is a benchmark for zero-shot generalization to unseen NLP tasks, with 25K labeled instances across 1,251 different tasks.'}\n",
      "{'name': 'Coupled Model Intercomparison Project Phase 5 (CMIP5) University of Wisconsin-Madison Probabilistic Downscaling Dataset', 'description': 'The University of Wisconsin Probabilistic Downscaling (UWPD) is a statistically downscaled dataset based on the Coupled Model Intercomparison Project Phase 5 (CMIP5) climate models. UWPD consists of three variables, daily precipitation and maximum and minimum temperature. The spatial resolution is 0.1<span>&#176;</span>x0.1<span>&#176;</span> degree resolution for the United States and southern Canada east of the Rocky Mountains.\\n<br/>\\n<br/>\\nThe downscaling methodology is not deterministic. Instead, to properly capture unexplained variability and extreme events, the methodology predicts a spatially and temporally varying Probability Density Function (PDF) for each variable. Statistics such as the mean, mean PDF and annual maximum statistics can be calculated directly from the daily PDF and these statistics are included in the dataset. In addition, “standard”, “raw” data is created by randomly sampling from the PDFs to create a “realization” of the local scale given the large-scale from the climate model. There are 3 realizations for temperature and 14 realizations for precipitation.\\n<br/>\\n<br/>\\nThe directory structure of the data is as follows\\n<br/>\\n<code>[cmip_version]/[scenario]/[climate_model]/[ensemble_member]/</code>\\n<br/>\\nThe realizations are as follows\\n<br/>\\n<code>prcp_[realization_number]_[year].nc</code>\\n<code>temp_[realization_number]_[year].nc</code>\\n<br/>\\nThe time mean files averaged over certain year bounds are as follows\\n<br/>\\n<code>prcp_mean_[year_bound_1]_[year_bound_2].nc</code>\\n<code>temp_mean_[year_bound_1]_[year_bound_2].nc</code>\\n<br/>\\nThe time-mean Cumulative Distribution Function (CDF) files are as follows\\n<br/>\\n<code>prcp_cdf_[year_bound_1]_[year_bound_2].nc</code>\\n<code>temp_cdf_[year_bound_1]_[year_bound_2].nc</code>\\n<br/>\\nThe CDF of the annual maximum precipitation is given for each year in the record\\n<code>prcp_annual_max_cdf_[start_year_of_scenario]_[end_year_of_scenario].nc</code>\\n<br />\\n'}\n",
      "{'name': 'Crowdsourced Bathymetry', 'description': 'Community provided bathymetry data collected in collaboration with the International Hydrographic Organization.'}\n",
      "{'name': 'CAncer MEtastases in LYmph nOdes challeNge (CAMELYON) Dataset', 'description': '\"This dataset contains the all data for the [CAncer MEtastases in LYmph nOdes challeNge or CAMELYON](https://camelyon17.grand-challenge.org). CAMELYON was the first challenge using whole-slide images in computational pathology and aimed to help pathologists identify breast cancer metastases in sentinel lymph nodes. Lymph node metastases are extremely important to find, as they indicate that the cancer is no longer localized and systemic treatment might be warranted. Searching for these metastases in H&E-stained tissue is difficult and time-consuming and AI algorithms can play a role in helping make this faster and more accurate.\\n'}\n",
      "{'name': 'Legal Entity Identifier (LEI) and Legal Entity Reference Data (LE-RD)', 'description': \"The Legal Entity Identifier (LEI) is a 20-character, alpha-numeric code based on the ISO 17442 standard developed by the International Organization for Standardization (ISO). It connects to key reference information that enables clear and unique identification of legal entities participating in financial transactions. Each LEI contains information about an entity’s ownership structure and thus answers the questions of 'who is who’ and ‘who owns whom’. Simply put, the publicly available LEI data pool can be regarded as a global directory, which greatly enhances transparency in the global marketplace. The Financial Stability Board (FSB) has reiterated that global LEI adoption underpins “multiple financial stability objectives” such as improved risk management in firms as well as better assessment of micro and macro prudential risks. As a result, it promotes market integrity while containing market abuse and financial fraud. Last but not least, LEI rollout “supports higher quality and accuracy of financial data overall”. The publicly available LEI data pool is a unique key to standardized information on legal entities globally. The data is registered and regularly verified according to protocols and procedures established by the Regulatory Oversight Committee. In cooperation with its partners in the Global LEI System, the Global Legal Entity Identifier Foundation (GLEIF) continues to focus on further optimizing the quality, reliability and usability of LEI data, empowering market participants to benefit from the wealth of information available with the LEI population. The drivers of the LEI initiative, i.e. the Group of 20, the FSB and many regulators around the world, have emphasized the need to make the LEI a broad public good. The Global LEI Index, made available by GLEIF, greatly contributes to meeting this objective. It puts the complete LEI data at the disposal of any interested party, conveniently and free of charge. The benefits for the wider business community to be generated with the Global LEI Index grow in line with the rate of LEI adoption. To maximize the benefits of entity identification across financial markets and beyond, firms are therefore encouraged to engage in the process and get their own LEI. Obtaining an LEI is easy. Registrants simply contact their preferred business partner from the list of LEI issuing organizations available on the GLEIF website.\\n\"}\n",
      "{'name': \"DOE's Water Power Technology Office's (WPTO) US Wave dataset\", 'description': \"Released to the public as part of the Department of Energy's Open Energy Data Initiative,\\nthis is the highest resolution publicly available long-term wave hindcast\\ndataset that – when complete – will cover the entire U.S. Exclusive Economic\\nZone (EEZ).\\n\"}\n",
      "{'name': 'Sentinel-5P Level 2', 'description': 'This data set consists of observations from the Sentinel-5 Precursor (Sentinel-5P) satellite of the European Commission’s Copernicus Earth Observation Programme. Sentinel-5P is a polar orbiting satellite that completes 14 orbits of the Earth a day. It carries the TROPOspheric Monitoring Instrument (TROPOMI) which is a spectrometer that senses ultraviolet (UV), visible (VIS), near (NIR) and short wave infrared (SWIR) to monitor ozone, methane, formaldehyde, aerosol, carbon monoxide, nitrogen dioxide and sulphur dioxide in the atmosphere. The satellite was launched in October 2017 and entered routine operational phase in March 2019. Data is available from July 2018 onwards.'}\n",
      "{'name': 'Maxar Open Data Program', 'description': 'Pre and post event high-resolution satellite imagery in support of emergency planning, risk assessment, \\nmonitoring of staging areas and emergency response, damage assessment, and recovery. These images are generated\\nusing the [Maxar ARD](https://ard.maxar.com/docs) pipeline, tiled on an organized grid in analysis-ready\\ncloud-optimized formats.\\n'}\n",
      "{'name': \"Co-Produced Climate Data to Support California's Resilience Investments\", 'description': \"Downscaled future and historical climate projections for California and her environs in support of California's Fifth Climate Assessment\"}\n",
      "{'name': '3000 Rice Genomes Project', 'description': 'The 3000 Rice Genome Project is an international effort to sequence the genomes of 3,024 rice varieties from 89 countries.'}\n",
      "{'name': 'Open Targets - Data Lakehouse Ready', 'description': \"This a Parquet representation of the Open Targets Platform's [latest export](https://www.targetvalidation.org/downloads/data). The Open Targets Platform integrates evidence from genetics, genomics, transcriptomics, drugs, animal models and scientific literature to score and rank target-disease associations for drug target identification. The Open Targets Platform (https://www.targetvalidation.org) is a freely available resource for the integration of genetics, genomics, and chemical data to aid systematic drug target identification and prioritisation. \\nThis dataset is 'Lakehouse Ready'. Meaning, you can query this data in-place straight out of the Registry of Open Data S3 bucket. [Deploy this dataset's corresponding CloudFormation template](https://console.aws.amazon.com/cloudformation/home?region=us-east-1#/stacks/quickcreate?templateUrl=https%3A%2F%2Faws-roda-hcls-datalake.s3.amazonaws.com%2FOpenTargets.latest.RodaTemplate.json&stackName=OpenTargets-Latest-RODA) to create the  AWS Glue catalog entries into your account in about 30 seconds. That one step will enable you to write SQL with AWS Athena, build dashboards and charts with Amazon Quicksight, perform HPC with AWS EMR, or join into your AWS Redshift clusters. More detail in (the documentation)[https://github.com/aws-samples/data-lake-as-code/blob/roda/README.md.\\n\"}\n",
      "{'name': 'NOAA Severe Weather Data Inventory (SWDI)', 'description': \"The Storm Events Database is an integrated database of severe weather events across the United States from 1950 to this year, with information about a storm event's location, azimuth, distance, impact, and severity, including the cost of damages to property and crops. It contains data documenting: The occurrence of storms and other significant weather phenomena having sufficient intensity to cause loss of life, injuries, significant property damage, and/or disruption to commerce. Rare, unusual, weather phenomena that generate media attention, such as snow flurries in South Florida or the San Diego coastal area. Other significant meteorological events, such as record maximum or minimum temperatures or precipitation that occur in connection with another event. Data about a specific event is added to the dataset within 120 days to allow time for damage assessments and other analysis.\\n\"}\n",
      "{'name': 'MWIS VR Instances', 'description': 'Large-scale node-weighted conflict graphs for maximum weight independent set solvers'}\n",
      "{'name': 'Genome in a Bottle on AWS', 'description': 'Several reference genomes to enable translation of whole human genome sequencing to clinical practice. On 11/12/2020 these data were updated to reflect the most [up to date GIAB release](https://www.nist.gov/programs-projects/genome-bottle).'}\n",
      "{'name': 'Digital Earth Africa Fractional Cover', 'description': \"Fractional cover (FC) describes the landscape in terms of coverage by green vegetation, non-green vegetation (including deciduous trees during autumn, dry grass, etc.) and bare soil. It provides insight into how areas of dry vegetation and/or bare soil and green vegetation are changing over time. The product is derived from Landsat satellite data, using an algorithm developed by the [Joint Remote Sensing Research Program](https://www.jrsrp.org.au/). \\nDigital Earth Africa's FC service has two components. Fractional Cover is estimated from each Landsat scene, providing measurements from individual days. Fractional Cover Annual Summary (Percentiles) provides 10th, 50th, and 90th percentiles estimated independently for the green vegetation, non-green vegetation, and bare soil fractions observed in each calendar year (1st of January - 31st December).\\nWhile the scene based Fractional Cover can be used to study dynamic processes, the annual summaries make it easier to analyse year to year changes. The percentiles provide robust estimates of the low, median and high proportion values observed for each cover type in a year, which can be used to characterise the land cover.\\n\"}\n",
      "{'name': 'NASA SOHO/LASCO2 comet challenge on AWS', 'description': 'The SOHO/LASCO data set (prepared for the challenge hosted in Topcoder) provided here comes from the instrument’s C2 telescope and comprises approximately 36,000 images spread across 2,950 comet observations. The human eye is a very sensitive tool and it is the only tool currently used to reliably detect new comets in SOHO data - particularly comets that are very faint and embedded in the instrument background noise. Bright comets can be easily detected in the LASCO data by relatively simple automated algorithms, but the majority of comets observed by the instrument are extremely faint, noise-level observations. Comets in SOHO/LASCO data are dynamic and morphologically diverse objects, and thus computationally highly complex to detect and track.\\n'}\n",
      "{'name': 'Storm EVent ImageRy (SEVIR)', 'description': 'Collection of spatially and temporally aligned GOES-16 ABI satellite imagery, NEXRAD radar mosaics, and GOES-16 GLM lightning detections.'}\n",
      "{'name': 'COVID-19 Open Research Dataset (CORD-19)', 'description': 'Full-text and metadata dataset of COVID-19 and coronavirus-related research articles optimized for machine readability.'}\n",
      "{'name': 'Sentinel-2 L2A 120m Mosaic', 'description': 'Sentinel-2 L2A 120m mosaic is a derived product, which contains best pixel values for 10-daily periods, modelled by removing the cloudy pixels and then performing interpolation among remaining values. As there are some parts of the world, which have lengthy cloudy periods, clouds might be remaining in some parts. The actual modelling script is available [here](https://sentinel-hub.github.io/custom-scripts/sentinel-2/interpolated_time_series/).'}\n",
      "{'name': 'COCO - Common Objects in Context - fast.ai datasets', 'description': 'COCO is a large-scale object detection, segmentation, and captioning dataset.\\nThis is part of the fast.ai datasets collection hosted by AWS for convenience\\nof fast.ai students. If you use this dataset in your research please cite\\narXiv:1405.0312 [cs.CV].\\n'}\n",
      "{'name': 'Amazon Berkeley Objects Dataset', 'description': 'Amazon Berkeley Objects (ABO) is a collection of 147,702 product listings with multilingual metadata and 398,212 unique catalog images. 8,222 listings come with turntable photography (also referred as \"spin\" or \"360º-View\" images), as sequences of 24 or 72 images, for a total of 586,584 images in 8,209 unique sequences. For 7,953 products, the collection also provides high-quality 3d models, as glTF 2.0 files.'}\n",
      "{'name': 'OpenAlex dataset', 'description': 'An open, comprehensive index of scolarly papers, citations, authors, institutions, and journals.'}\n",
      "{'name': 'NOAA/PMEL Ocean Climate Stations Moorings', 'description': 'The mission of the Ocean Climate Stations (OCS) Project is to make meteorological and \\noceanic measurements from autonomous platforms.  Calibrated, quality-controlled, and well-documented \\nclimatological measurements are available on the OCS webpage and the OceanSITES Global Data\\nAssembly Centers (GDACs), with near-realtime data available prior to release of the complete, \\ndownloaded datasets.<br/><br/>\\n\\nOCS measurements served through the Big Data Program come from OCS high-latitude moored buoys located in the Kuroshio \\nExtension (32°N 145°E) and the Gulf of Alaska (50°N 145°W).  Initiated in 2004 and 2007, \\nthe respective moored buoys, KEO and Papa, measure a suite of surface and subsurface essential ocean variables.\\nThe surface suite includes air temperature, relative humidity, shortwave and longwave radiation, barometric pressure, winds, and rain, \\nwhile subsurface instrumentation includes temperature, salinity, and ocean currents.  Individual buoy deployments are stitched together into \\na continuous time-series, which is synced to the OceanSITES GDACs, and subsequently, to BDP.\\n'}\n",
      "{'name': 'Daylight Map Distribution of OpenStreetMap', 'description': 'Daylight is a complete distribution of global, open map data that’s freely available with support from community and professional mapmakers. Meta combines the work of global contributors to projects like OpenStreetMap with quality and consistency checks from Daylight mapping partners to create a free, stable, and easy-to-use street-scale global map. <br/><br/>\\nThe Daylight Map Distribution contains a validated subset of the OpenStreetMap database. In addition to the standard OpenStreetMap PBF format, Daylight is available in two parquet formats that are optimized for AWS Athena including geometries (Points, LineStrings, Polygons, or MultiPolygons). First, **Daylight OSM Features** contains the nearly 1B renderable OSM features. Second, **Daylight OSM Elements** contains all of OSM, including all 7B nodes without attributes, and relations that do not contain geometries, such as turn restrictions. <br/><br/> \\nDaylight Earth Table is a new data schema that classifies OpenStreetMap-style tags into a 3-level ontology (theme, class, subclass) and is the result of running the earth table classification over the latest release (v1.18) of the Daylight Map Distribution. The Daylight Earth Table is available as parquet files on Amazon S3. \\n'}\n",
      "{'name': 'NOAA Global Surface Summary of Day', 'description': \"Global Surface Summary of the Day is derived from The Integrated Surface Hourly (ISH) dataset. The ISH dataset includes global data obtained from the USAF Climatology Center, located in the Federal Climate Complex with NCDC. The latest daily summary data are normally available 1-2 days after the date-time of the observations used in the daily summaries. The online data files begin with 1929 and are at the time of this writing at the Version 8 software level. Over 9000 stations' data are typically available. The daily elements included in the dataset (as available from each station) are: <br/>\\nMean temperature (.1 Fahrenheit) <br/>\\nMean dew point (.1 Fahrenheit) <br/>\\nMean sea level pressure (.1 mb) <br/>\\nMean station pressure (.1 mb) <br/>\\nMean visibility (.1 miles) <br/>\\nMean wind speed (.1 knots) <br/>\\nMaximum sustained wind speed (.1 knots) <br/>\\nMaximum wind gust (.1 knots) <br/>\\nMaximum temperature (.1 Fahrenheit) <br/>\\nMinimum temperature (.1 Fahrenheit) <br/>\\nPrecipitation amount (.01 inches) <br/>\\nSnow depth (.1 inches) <br/>\\nIndicator for occurrence of: Fog, Rain or Drizzle, Snow or Ice Pellets, Hail, Thunder, Tornado/Funnel Cloud.<br/><br/>\\nGlobal summary of day data for 18 surface meteorological elements are derived from the synoptic/hourly observations contained in USAF DATSAV3 Surface data and Federal Climate Complex Integrated Surface Hourly (ISH). Historical data are generally available for 1929 to the present, with data from 1973 to the present being the most complete. For some periods, one or more countries' data may not be available due to data restrictions or communications problems. In deriving the summary of day data, a minimum of 4 observations for the day must be present (allows for stations which report 4 synoptic observations/day). Since the data are converted to constant units (e.g, knots), slight rounding error from the originally reported values may occur (e.g, 9.9 instead of 10.0). The mean daily values described below are based on the hours of operation for the station. For some stations/countries, the visibility will sometimes 'cluster' around a value (such as 10 miles) due to the practice of not reporting visibilities greater than certain distances. The daily extremes and totals--maximum wind gust, precipitation amount, and snow depth--will only appear if the station reports the data sufficiently to provide a valid value. Therefore, these three elements will appear less frequently than other values. Also, these elements are derived from the stations' reports during the day, and may comprise a 24-hour period which includes a portion of the previous day. The data are reported and summarized based on Greenwich Mean Time (GMT, 0000Z - 2359Z) since the original synoptic/hourly data are reported and based on GMT.\\n\"}\n",
      "{'name': '1000 Genomes Phase 3 Reanalysis with DRAGEN 3.5 and 3.7', 'description': 'This dataset contains alignment files and short nucleotide, copy number, repeat expansion (STR) and structural variant call files from the 1000 Genomes Project Phase 3 dataset (n=3202) using Illumina DRAGEN v3.5.7b and v3.7.6 software. The v3.7.6 dataset also includes results from joint small variant, de novo structural variant, de novo copy number variant and repeat expansion calls on 602 trio families comprised of members from the 1000 Genomes Project Phase 3 dataset, as well as DRAGEN gVCF Genotyper (v3.8.3) analysis on the entire dataset (n=3202).  Improvements and new features in the v3.7.6 individual samples analyses include CYP2D6 variant calling and joint detection (see ‘DRAGEN 3.7 User Guide’ for details on these features) and use of graph-based hg19 and hg38 reference hash tables (see ‘DRAGEN Wins at PrecisionFDA Truth Challenge V2 Showcase Accuracy Gains from Alt-aware Mapping and Graph Reference Genomes’ for details).'}\n",
      "{'name': 'LOFAR ELAIS-N1 cycle 2 observations on AWS', 'description': 'These data correspond to the [International LOFAR Telescope](http://www.lofar.org/) observations of the sky field ELAIS-N1 (16:10:01 +54:30:36) during the cycle 2 of observations. There are 11 runs of about 8 hours each plus the corresponding observation of the calibration targets before and after the target field. The data are measurement sets ([MS](https://casa.nrao.edu/Memos/229.html)) containing the cross-correlated data and metadata divided in 371 frequency sub-bands per target centred at ~150 MHz.'}\n",
      "{'name': 'GATK Structural Variation (SV) Data', 'description': 'This dataset holds the data needed to run a [structural variation discovery pipeline](https://github.com/broadinstitute/gatk-sv)\\nfor Illumina short-read whole-genome sequencing (WGS) data in AWS.\\n'}\n",
      "{'name': 'Low Altitude Disaster Imagery (LADI) Dataset', 'description': 'The Low Altitude Disaster Imagery (LADI) Dataset consists of human and machine annotated airborne images collected by the Civil Air Patrol in support of various disaster responses from 2015-2019. The initial release of LADI focuses on the Atlantic hurricane seasons and coastal states along the Atlantic Ocean and Gulf of Mexico. Annotations are included for major hurricanes of Harvey, Maria, and Florence. Two key distinctions are the low altitude, oblique perspective of the imagery and disaster-related features, which are rarely featured in computer vision benchmarks and datasets.'}\n",
      "{'name': 'Capella Space Synthetic Aperture Radar (SAR) Open Dataset', 'description': \"Open Synthetic Aperture Radar (SAR) data from Capella Space. Capella Space is an information services company\\n  that provides on-demand, industry-leading, high-resolution synthetic aperture radar (SAR) Earth observation\\n  imagery. Through a constellation of small satellites, Capella provides easy access to frequent, timely, and\\n  flexible information affecting dozens of industries worldwide. Capella's high-resolution SAR satellites are\\n  matched with unparalleled infrastructure to deliver reliable global insights that sharpen our understanding\\n  of the changing world – improving decisions about commerce, conservation, and security on Earth. Learn more\\n  at www.capellaspace.com.\\n\"}\n",
      "{'name': 'InRad COVID-19 X-Ray and CT Scans', 'description': 'This dataset is a collection of anonymized thoracic radiographs (X-Rays) and computed tomography (CT) scans of patients with suspected COVID-19. Images are acommpanied by a positive or negative diagnosis for SARS-CoV2 infection via RT-PCR. These images were provided by Hospital das Clínicas da Universidade de São Paulo, Hospital Sirio-Libanes, and by Laboratory Fleury.'}\n",
      "{'name': 'UK Biobank Pan-Ancestry Summary Statistics', 'description': 'A multi-ancestry analysis of 7,221 phenotypes using a generalized mixed model association testing framework, spanning 16,119 genome-wide association studies. We provide standard meta-analysis across all populations and with a leave-one-population-out approach for each trait. The data are provided in tsv format (per phenotype) and Hail MatrixTable (all phenotypes and variants). Metadata is provided in phenotype and variant manifests.'}\n",
      "{'name': 'Digital Earth Africa Landsat Collection 2 Level 2', 'description': 'Digital Earth Africa (DE Africa) provides free and open access to a copy of Landsat Collection 2 Level-2 products over Africa. These products are produced and provided by the United States Geological Survey (USGS).\\nThe Landsat series of Earth Observation satellites, jointly led by USGS and NASA, have been continuously acquiring images of the Earth’s land surface since 1972. DE Africa provides data from Landsat 5, 7 and 8 satellites, including historical observations dating back to late 1980s and regularly updated new acquisitions.\\nNew Level-2 Landsat 7 and Landsat 8 data are available after 15 to 27 days from acquisition. See Landsat Collection 2 Generation Timeline for details.\\nUSGS Landsat Collection 2 was released early 2021 and offers improved processing, geometric accuracy, and radiometric calibration compared to previous Collection 1 products. The Level-2 products are endorsed by the Committee on Earth Observation Satellites (CEOS) to be Analysis Ready Data for Land (CARD4L)-compliant. This internationally recognized certification ensures these products have been processed to a minimum set of requirements and organized into a form that allows immediate analysis with a minimum of additional user effort and interoperability both through time and with other datasets.\\n'}\n",
      "{'name': 'District of Columbia - Classified Point Cloud LiDAR', 'description': 'LiDAR point cloud data for Washington, DC is available for anyone to use on Amazon S3.\\nThis dataset, managed by the Office of the Chief Technology Officer (OCTO), through the\\ndirection of the District of Columbia GIS program, contains tiled point cloud data for\\nthe entire District along with associated metadata.\\n'}\n",
      "{'name': 'VoiSeR', 'description': 'Voice-based refinements of product search'}\n",
      "{'name': 'Sea Surface Temperature Daily Analysis: European Space Agency Climate Change Initiative product version 2.1', 'description': 'Global daily-mean sea surface temperatures, presented on a 0.05° latitude-longitude grid, with gaps between available daily observations filled by statistical means, spanning late 1981 to recent time.  Suitable for large-scale oceanographic meteorological and climatological applications, such as evaluating or constraining environmental models or case-studies of marine heat wave events. Includes temperature uncertainty information and auxiliary information about land-sea fraction and sea-ice coverage. For reference and citation see: www.nature.com/articles/s41597-019-0236-x.'}\n",
      "{'name': 'OpenStreetMap Linear Referencing', 'description': 'OSMLR a linear referencing system built on top of OpenStreetMap. OSM has great information about roads around the world and their interconnections, but it lacks the means to give a stable identifier to a stretch of roadway. OSMLR provides a stable set of numerical IDs for every 1 kilometer stretch of roadway around the world. In urban areas, OSMLR IDs are attached to each block of roadways between significant intersections.'}\n",
      "{'name': 'National Cancer Institute Center for Cancer Research - Diffuse Large B Cell Lymphoma (DLBCL) Genomics and Expression', 'description': 'The study describes integrative analysis of genetic lesions in 574 diffuse large B cell lymphomas\\n(DLBCL) involving exome and transcriptome sequencing, array-based DNA copy number analysis and\\ntargeted amplicon resequencing. The dataset contains open RNA-Seq Gene Expression Quantification\\ndata.\\n'}\n",
      "{'name': 'MODIS', 'description': 'The Moderate Resolution Imaging Spectroradiometer (MODIS) MCD43A4 Version 6 Nadir Bidirectional Reflectance Distribution Function (BRDF)-Adjusted Reflectance (NBAR) dataset is produced daily using 16 days of Terra and Aqua MODIS data at 500 meter (m) resolution. The view angle effects are removed from the directional reflectances, resulting in a stable and consistent NBAR product. Data are temporally weighted to the ninth day which is reflected in the Julian date in the file name.'}\n",
      "{'name': '4D Nucleome (4DN)', 'description': 'The goal of the National Institutes of Health (NIH) Common Fund’s 4D Nucleome (4DN) program\\nis to study the three-dimensional organization of the nucleus in space and time (the 4th dimension).\\nThe nucleus of a cell contains DNA, the genetic “blueprint” that encodes all of the genes a living\\norganism uses to produce proteins needed to carry out life-sustaining cellular functions. Understanding\\nthe conformation of the nuclear DNA and how it is maintained or changes in response to environmental\\nand cellular cues over time will provide insights into basic biology as well as aspects of human\\nhealth and disease. The 4DN is an international consortium of researchers who generate data that\\ninclude results from a variety of genomics and imaging assays with a focus on, but not exclusive to,\\nthose that demonstrate close contact between chromatin loci that are non-adjacent on the linear DNA\\nsequence of chromosomes. Additional assays probe the nuclear landscape in the context of interactions\\nof chromatin with specific proteins, RNAs and epigenetic changes.\\n'}\n",
      "{'name': 'High Resolution Population Density Maps + Demographic Estimates by CIESIN and Meta', 'description': 'Population data for a selection of countries, allocated to 1 arcsecond blocks and provided in a combination of CSV\\nand Cloud-optimized GeoTIFF files. This refines [CIESIN’s Gridded Population of the World](https://sedac.ciesin.columbia.edu/data/collection/gpw-v4)\\nusing machine learning models on high-resolution worldwide Maxar satellite imagery. CIESIN population counts aggregated from worldwide census\\ndata are allocated to blocks where imagery appears to contain buildings.\\n'}\n",
      "{'name': 'Registry of Open Data on AWS', 'description': 'The Registry of Open Data on AWS contains publicly available datasets that are available for access from AWS resources. Note that datasets in this registry are available via AWS resources, but they are not provided by AWS; these datasets are owned and maintained by a variety of government organizations, researchers, businesses, and individuals. This dataset contains derived forms of the data in [https://github.com/awslabs/open-data-registry](https://github.com/awslabs/open-data-registry) that have been transformed for ease of use with machine interfaces. Currently, only the [ndjson](http://ndjson.org/) form of the registry is populated here.'}\n",
      "{'name': 'MODIS MYD13A1, MOD13A1, MYD11A1, MOD11A1, MCD43A4', 'description': 'Data from the Moderate Resolution Imaging Spectroradiometer (MODIS), managed by\\nthe U.S. Geological Survey and NASA. Five products are included:\\nMCD43A4 (MODIS/Terra and Aqua Nadir BRDF-Adjusted Reflectance Daily L3 Global 500 m SIN Grid),\\nMOD11A1 (MODIS/Terra Land Surface Temperature/Emissivity Daily L3 Global 1 km SIN Grid),\\nMYD11A1 (MODIS/Aqua Land Surface Temperature/Emissivity Daily L3 Global 1 km SIN Grid),\\nMOD13A1 (MODIS/Terra Vegetation Indices 16-Day L3 Global 500 m SIN Grid),\\nand MYD13A1 (MODIS/Aqua Vegetation Indices 16-Day L3 Global 500 m SIN Grid).\\nMCD43A4 has global coverage, all time (~21 years).\\nThe other products have ~11 years of global coverage.  All data files are in single-band\\ncloud-optimized GeoTIFF (COG) format.\\n'}\n",
      "{'name': 'IRS 990 Filings', 'description': '**On December 16, 2021 the IRS announced that it would [discontinue updates to the IRS 990 Filings dataset on AWS, starting December 31, 2021](https://www.irs.gov/newsroom/irs-makes-tax-exempt-organization-search-primary-source-to-get-exempt-organization-data).**\\n\\nThe IRS has requested public inquiries be directed to +1-800-829-1040.\\n\\nMachine-readable data from certain electronic 990 forms filed with the IRS from 2013 to present.\\n'}\n",
      "{'name': 'DialoGLUE: A Natural Language Understanding Benchmark for Task-Oriented Dialogue', 'description': 'This bucket contains the checkpoints used to reproduce the baseline results reported in the DialoGLUE benchmark hosted\\non EvalAI (https://evalai.cloudcv.org/web/challenges/challenge-page/708/overview). The associated scripts for using the checkpoints are located here:\\nhttps://github.com/alexa/dialoglue. The associated paper describing the benchmark and checkpoints is here: https://arxiv.org/abs/2009.13570.\\nThe provided checkpoints include the CONVBERT model, a BERT-esque model trained on a large open-domain conversational\\ndataset. It also includes the CONVBERT-DG and BERT-DG checkpoints described in the linked\\npaper.\\n'}\n",
      "{'name': 'High Resolution Downscaled Climate Data for Southeast Alaska', 'description': 'This dataset contains historical and projected dynamically downscaled climate data for the Southeast region of the State of Alaska at 1 and 4km spatial resolution and hourly temporal resolution. Select variables are also summarized into daily resolutions. This data was produced using the Weather Research and Forecasting (WRF) model (Version 4.0). We downscaled both Climate Forecast System Reanalysis (CFSR) historical reanalysis data (1980-2019) and both historical and projected runs from two GCM’s from the Coupled Model Inter-comparison Project 5 (CMIP5): GFDL-CM3 and NCAR-CCSM4 (historical run: 1980-2010 and RCP 8.5: 2030-2060).'}\n",
      "{'name': 'Terrain Tiles', 'description': 'A global dataset providing bare-earth terrain heights, tiled for easy usage and provided on S3.'}\n",
      "{'name': 'NOAA U.S. Climate Gridded Dataset (NClimGrid)', 'description': 'The NOAA Monthly U.S. Climate Gridded Dataset (NClimGrid) consists of four climate variables derived from the GHCN-D dataset: maximum temperature, minimum temperature, average temperature and precipitation. Each file provides monthly values in a 5x5 lat/lon grid for the Continental United States. Data is available from 1895 to the present. On an annual basis, approximately one year of \"final\" nClimGrid will be submitted to replace the initially supplied \"preliminary\" data for the same time period. Users should be sure to ascertain which level of data is required for their research.\\n<br/>\\n<br/>\\nEpiNOAA is an analysis ready dataset that consists of a daily time-series of nClimGrid measures (maximum temperature, minimum temperature, average temperature, and precipitation) at the county scale.  Each file provides daily values for the Continental United States. Data are available from 1951 to the present. Daily data are updated every 3 days with a preliminary data file and replaced with the scaled (i.e., quality controlled) data file every three months. This derivative data product is an enhancement from the original daily nClimGrid dataset in that all four weather parameters are now packaged into one file and assembled in a daily time-series format. In addition to a direct download option, an R package and web interface has been developed to streamline access to the final data product. These options allow end users three separate access modes to arrive at a customized dataset unique to each end user’s application. Users should be sure to review the data documentation to inform which level of data is required for their research.\\n'}\n",
      "{'name': 'NLP - fast.ai datasets', 'description': 'Some of the most important datasets for NLP, with a focus on classification, including\\nIMDb, AG-News, Amazon Reviews (polarity and full), Yelp Reviews (polarity and\\nfull), Dbpedia, Sogou News (Pinyin), Yahoo Answers, Wikitext 2 and Wikitext\\n103, and ACL-2010 French-English 10^9 corpus.  This is part of the\\nfast.ai datasets collection hosted by AWS for convenience of fast.ai\\nstudents. See documentation link for citation and license details for each\\ndataset.\\n'}\n",
      "{'name': 'NOAA Global Historical Climatology Network Daily (GHCN-D)', 'description': '<br /> UPDATE TO GHCN PREFIXES - The NODD team is working on improving performance and access to the GHCNd data and will be implementing an updated prefix structure. For more information on the prefix changes, please see the \"[READ ME on the NODD Github](https://github.com/NOAA-Big-Data-Program/bdp-data-docs/tree/main/GHCN-D)\". If you have questions, comments, or feedback, please reach out to nodd@noaa.gov with GHCN in the subject line. <br /> <br >\\nGlobal Historical Climatology Network - Daily is a dataset from NOAA that contains daily observations over global land areas. It contains station-based measurements from land-based stations worldwide, about two thirds of which are for precipitation measurement only. Other meteorological elements include, but are not limited to, daily maximum and minimum temperature, temperature at the time of observation, snowfall and snow depth. It is a composite of climate records from numerous sources that were merged together and subjected to a common suite of quality assurance reviews.  Some data are more than 175 years old. The data is in CSV format. Each file corresponds to a year from 1763 to present and is named as such.'}\n",
      "{'name': 'OpenProteinSet', 'description': 'Multiple sequence alignments (MSAs) for 132,000 unique [Protein Data Bank](https://www.rcsb.org/) (PDB) chains, covering 640,000 PDB chains in total, and 4,850,000 [UniClust30](https://uniclust.mmseqs.com/) clusters. Template hits are also provided for the PDB chains and 270,000 UniClust30 clusters chosen for maximal diversity and MSA depth. MSAs were generated with HHBlits (-n3) and JackHMMER against MGnify, BFD, UniRef90, and UniClust30 while templates were identified from PDB70 with HHSearch, all according to procedures outlined in the supplement to the AlphaFold 2 Nature paper, [Jumper et al. 2021](https://www.nature.com/articles/s41586-021-03819-2). We expect the database to be broadly useful to structural biologists training or validating deep learning models for protein structure prediction and related tasks.'}\n",
      "{'name': 'BossDB Open Neuroimagery Datasets', 'description': 'This data ecosystem, Brain Observatory Storage Service & Database (BossDB), contains several neuro-imaging datasets across multiple modalities and scales, ranging from nanoscale (electron microscopy), to microscale (cleared lightsheet microscopy and array tomography), and mesoscale (structural and functional magnetic resonance imaging). Additionally, many of the datasets include dense segmentation and meshes.'}\n",
      "{'name': 'NOAA Atmospheric Climate Data Records', 'description': \"NOAA's Climate Data Records (CDRs) are robust, sustainable, and scientifically sound climate records that provide trustworthy information on how, where, and to what extent the land, oceans, atmosphere and ice sheets are changing. These datasets are thoroughly vetted time series measurements with the longevity, consistency, and continuity to assess and measure climate variability and change. NOAA CDRs are vetted using standards established by the National Research Council (NRC).<br/><br/>\\nClimate Data Records are created by merging data from surface, atmosphere, and space-based systems across decades. NOAA’s Climate Data Records provides authoritative and traceable long-term climate records. NOAA developed CDRs by applying modern data analysis methods to historical global satellite data. This process can clarify the underlying climate trends within the data and allows researchers and other users to identify economic and scientific value in these records. NCEI maintains and extends CDRs by applying the same methods to present-day and future satellite measurements.<br/><br/>\\nAtmospheric Climate Data Records are measurements of several global variables to help characterize the atmosphere at or just above the land and ocean surface as well as other upper air composition variables.\\n\"}\n",
      "{'name': 'Cell Painting Image Collection', 'description': 'The Cell Painting Image Collection is a collection of freely\\ndownloadable microscopy image sets. Cell Painting is an\\nunbiased high throughput imaging assay used to analyze\\nperturbations in cell models. In addition to the images\\nthemselves, each set includes a description of the biological\\napplication and some type of \"ground truth\" (expected results).\\nResearchers are encouraged to use these image sets as reference\\npoints when developing, testing, and publishing new image\\nanalysis algorithms for the life sciences. We hope that the\\nthis data set will lead to a better understanding of which\\nmethods are best for various biological image analysis\\napplications.\\n'}\n",
      "{'name': 'Sea Around Us Global Fisheries Catch Data', 'description': 'The project presents Sea Around Us Global Fisheries Catch Data aggregated at EEZ level. The data are computed from reconstructed catches from various official fisheries statistics, scientific, technical and policy reports about the fisheries, and includes estimation of discards, unreported and illegal catch data from all maritime countries and major territories of the world.\\n\\nThis project was the result of a work between Sea Around Us and the CIC programme, a collaborative programme between the University of British Columbia (UBC) and AWS.\\n'}\n",
      "{'name': 'Tabula Muris Senis', 'description': 'Tabula Muris Senis is a comprehensive compendium of single cell transcriptomic data from the model organism *Mus musculus* comprising more than 500,000 cells from 18 organs and tissues across the mouse lifespan. We discovered cell-specific changes occurring across multiple cell types and organs, as well as age related changes in the cellular composition of different organs. Using single-cell transcriptomic data we were able to assess cell type specific manifestations of different hallmarks of aging, such as senescence, changes in the activity of metabolic pathways, depletion of stem-cell populations, genomic instability and the role of inflammation as well as other changes in the organism’s immune system. Tabula Muris Senis provides a wealth of new molecular information about how the most significant hallmarks of aging are reflected in a broad range of tissues and cell types.See: https://www.biorxiv.org/content/10.1101/661728v1\\n'}\n",
      "{'name': 'RAPID NRT Flood Maps', 'description': 'Near Real-time and archival data of High-resolution (10 m) flood inundation dataset over the Contiguous United States, developed based on the Sentinel-1 SAR imagery (2016-current) archive, using an automated Radar Produced Inundation Diary (RAPID) algorithm.'}\n",
      "{'name': 'Image localization  - fast.ai datasets', 'description': 'Some of the most important datasets for image localization  research, including\\nCamvid and PASCAL VOC (2007 and 2012). This is part of the fast.ai datasets\\ncollection hosted by AWS for convenience of fast.ai students. See\\ndocumentation link for citation and license details for each dataset.\\n'}\n",
      "{'name': 'Common Crawl', 'description': 'A corpus of web crawl data composed of over 50 billion web pages.'}\n",
      "{'name': 'Ohio State Cardiac MRI Raw Data (OCMR)', 'description': 'OCMR is an open-access repository that provides multi-coil k-space data for cardiac cine.  The fully sampled MRI datasets are intended for quantitative comparison and evaluation of image reconstruction methods. The free-breathing, prospectively undersampled datasets are intended to evaluate their performance and generalizability qualitatively.'}\n",
      "{'name': 'Sudachi Language Resources', 'description': 'Japanese dictionaries and pre-trained models (word embeddings and language models) for natural language processing.\\n[SudachiDict](https://github.com/WorksApplications/SudachiDict) is the dictionary for a Japanese tokenizer (morphological analyzer) [Sudachi](https://github.com/WorksApplications/Sudachi).\\n[chiVe](https://github.com/WorksApplications/chiVe) is Japanese pretrained word embeddings (word vectors), trained using the ultra-large-scale web corpus NWJC by National Institute for Japanese Language and Linguistics, analyzed by Sudachi.\\n[chiTra](https://github.com/WorksApplications/SudachiTra) is a library for using large-scale pre-trained language models with the Japanese tokenizer SudachiPy.\\n'}\n",
      "{'name': 'NOAA Water-Column Sonar Data Archive', 'description': 'Water-column sonar data archived at the NOAA National Centers for Environmental Information.'}\n",
      "{'name': 'Japanese Tokenizer Dictionaries', 'description': 'Japanese Tokenizer Dictionaries for use with MeCab.'}\n",
      "{'name': 'REDASA COVID-19 Open Data', 'description': 'The REaltime DAta Synthesis and Analysis (REDASA) COVID-19 snapshot contains the output of the curation protocol produced by our curator community. A detailed description can be found in [our paper](https://www.jmir.org/2021/5/e25714). The first S3 bucket listed in Resources contains a large collection of medical documents in text format extracted from the [CORD-19 dataset](https://registry.opendata.aws/cord-19/), plus other sources deemed relevant by the REDASA consortium.  The second S3 bucket contains a series of documents surfaced by [Amazon Kendra](https://aws.amazon.com/kendra/) that were considered relevant for each medical question asked. The final S3 bucket contains the GroundTruth annotations created by our curator community.\\n'}\n",
      "{'name': 'The Cancer Genome Atlas', 'description': 'The Cancer Genome Atlas (TCGA), a collaboration between the National Cancer Institute (NCI) and National Human Genome Research Institute (NHGRI), aims to generate comprehensive, multi-dimensional maps of the key genomic changes in major types and subtypes of cancer. TCGA has analyzed matched tumor and normal tissues from 11,000 patients, allowing for the comprehensive characterization of 33 cancer types and subtypes, including 10 rare cancers.\\nThe dataset contains open Clinical Supplement, Biospecimen Supplement, RNA-Seq Gene Expression Quantification, miRNA-Seq Isoform Expression Quantification, miRNA Expression Quantification, Genotyping Array Copy Number Segment, Genotyping Array Masked Copy Number Segment, Genotyping Array Gene Level Copy Number Scores, and WXS Masked Somatic Mutation data from Genomic Data Commons (GDC).\\nThis dataset also contains controlled Whole Exome Sequencing (WXS), RNA-Seq, miRNA-Seq, ATAC-Seq Aligned Reads, WXS Annotated Somatic Mutation, WXS Raw Somatic Mutation, and WXS Aggregated Somatic Mutation data from GDC.\\nTCGA is made available on AWS via the [NIH STRIDES Initiative](https://aws.amazon.com/blogs/publicsector/aws-and-national-institutes-of-health-collaborate-to-accelerate-discoveries-with-strides-initiative/).\\n'}\n",
      "{'name': 'Physionet', 'description': 'PhysioNet offers free web access to large collections of recorded physiologic signals (PhysioBank) and related open-source software (PhysioToolkit).'}\n",
      "{'name': 'NOAA National Digital Forecast Database (NDFD)', 'description': 'The National Digital Forecast Database (NDFD) is a suite of gridded forecasts of sensible weather elements (e.g., cloud cover, maximum temperature).  Forecasts prepared by NWS field offices working in collaboration with the National Centers for Environmental Prediction (NCEP) are combined in the NDFD to create a seamless mosaic of digital forecasts from which operational NWS products are generated. The most recent data is under the opnl and expr prefixes. A copy is also placed under the wmo prefix. The wmo prefix is structured like so: wmo/&lt;parameter&gt;/&lt;year&gt;/&lt;month&gt;/&lt;day&gt;/&lt;wmo-file-name&gt; The wmo filename codes can be deciphered using the spreadsheet in the root of the bucket.\\n'}\n",
      "{'name': 'NOAA Global Forecast System (GFS)', 'description': 'NOTE - Upgrade NCEP Global Forecast System to v16.3.0 - Effective November 29, 2022 See notification [HERE](https://www.weather.gov/media/notification/pdf2/scn22-104_gfs.v16.3.0.pdf)\\n<br/>\\n<br/>\\nThe Global Forecast System (GFS) is a weather forecast model produced\\nby the National Centers for Environmental Prediction (NCEP). Dozens of\\natmospheric and land-soil variables are available through this dataset,\\nfrom temperatures, winds, and precipitation to soil moisture and\\natmospheric ozone concentration. The entire globe is covered by the GFS\\nat a base horizontal resolution of 18 miles (28 kilometers) between grid\\npoints, which is used by the operational forecasters who predict weather\\nout to 16 days in the future. Horizontal resolution drops to 44 miles\\n(70 kilometers) between grid point for forecasts between one week and two\\nweeks.\\n<br/>\\n<br/>\\nThe NOAA Global Forecast Systems (GFS) Warm Start Initial Conditions are \\nproduced by the National Centers for Environmental Prediction Center (NCEP) \\nto run operational deterministic medium-range numerical weather predictions.   \\nThe GFS is built with the GFDL Finite-Volume Cubed-Sphere Dynamical Core (FV3) \\nand the Grid-Point Statistical Interpolation (GSI) data assimilation system.  \\nPlease visit the links below in the Documentation section to find more details \\nabout the model and the data assimilation systems.   The current operational \\nGFS is run at 64 layers in the vertical extending from the surface to the upper\\nstratosphere and on six cubic-sphere tiles at  the C768 or 13-km horizontal\\nresolution.  A new version of the GFS that has 127 layers extending to the\\nmesopause will be implemented for operation on February 3, 2021.   These initial\\nconditions are made available four times per day for running forecasts at the\\n00Z, 06Z, 12Z and 18Z cycles, respectively.   For each cycle, the dataset\\ncontains the first guess of the atmosphere states found in the directory\\n./gdas.yyyymmdd/hh-6/RESTART, which are  6-hour GDAS forecast from the last\\ncycle,  and atmospheric analysis increments and surface analysis for the current\\ncycle  found in the directory ./gfs.yyyymmdd/hh, which are produced by the data\\nassimilation systems. \\n'}\n",
      "{'name': 'NOAA National Blend of Models (NBM)', 'description': 'The National Blend of Models (NBM) is a nationally consistent and skillful suite of calibrated forecast guidance based on a blend of both NWS and non-NWS numerical weather prediction model data and post-processed model guidance. The goal of the NBM is to create a highly accurate, skillful and consistent starting point for the gridded forecast. \\n'}\n",
      "{'name': 'CoMMpass from the Multiple Myeloma Research Foundation', 'description': 'The Relating Clinical Outcomes in Multiple Myeloma to Personal Assessment of Genetic Profile study is the Multiple Myeloma Research Foundation (MMRF)’s landmark personalized medicine initiative.  CoMMpass is a\\nlongitudinal observation study of around 1000 newly diagnosed myeloma patients receiving various\\nstandard approved treatments. The MMRF’s vision is to track the treatment and results for each\\nCoMMpass patient so that someday the information can be used to guide decisions for newly\\ndiagnosed patients. CoMMpass checked on patients every 6 months for 8 years, collecting tissue\\nsamples, genetic, information, quality of life and various disease and clinical outcomes. The\\nstudy has produced one of the largest genomic and clinical datasets of a single disease.\\n'}\n",
      "{'name': 'GATK Test Data', 'description': \"The GATK test data resource bundle is a collection of files for resequencing human genomic data with the\\nBroad Institute's [Genome Analysis Toolkit (GATK)](https://software.broadinstitute.org/gatk/).\\n\"}\n",
      "{'name': 'BodyM Dataset', 'description': 'The first large public body measurement dataset including 8978 frontal and lateral\\nsilhouettes for 2505 real subjects, paired with height, weight and 14 body\\nmeasurements. The following artifacts are made available for each subject.\\n  - Subject Height\\n  - Subject Weight\\n  - Subject Gender\\n  - Two black-and-white silhouette images of subject standing in frontal and side pose\\n  respectively with full body in view.\\n  - 14 body measurements in cm - {ankle girth, arm-length, bicep girth, calf girth,\\n                                chest girth, forearm girth, height, hip girth, leg-length,\\n                                shoulder-breadth, shoulder-to-crotch length, thigh girth,\\n                                waist girth, wrist girth}\\n\\nThe data is split into 3 sets - Training, Test Set A, Test Set B. For the training and\\nTest-A sets, subjects are photographed and 3D-scanned by in a lab by technicians. For\\nthe Test-B set, subjects are scanned in the lab, but photographed in a less-controlled\\nenvironment with diverse camera orientations and lighting conditions, to simulate\\nin-the-wild image capture. Some subjects have been photographed more than once with\\ndifferent clothing in order to test robustness of the dataset.\\n'}\n",
      "{'name': 'Answer Reformulation', 'description': 'Original StackExchange answers and their voice-friendly Reformulation.'}\n",
      "{'name': 'NOAA U.S. Climate Normals', 'description': 'The U.S. Climate Normals are a large suite of data products that provide information about typical climate conditions for thousands of locations across the United States. Normals act both as a ruler to compare today’s weather and tomorrow’s forecast, and as a predictor of conditions in the near future. The official normals are calculated for a uniform 30 year period, and consist of annual/seasonal, monthly, daily, and hourly averages and statistics of temperature, precipitation, and other climatological variables from almost 15,000 U.S. weather stations. <br/><br/>\\nNCEI generates the official U.S. normals every 10 years in keeping with the needs of our user community and the requirements of the World Meteorological Organization (WMO) and National Weather Service (NWS). The 1991–2020 U.S. Climate Normals are the latest in a series of decadal normals first produced in the 1950s. These data allow travelers to pack the right clothes, farmers to plant the best crop varieties, and utilities to plan for seasonal energy usage. Many other important economic decisions that are made beyond the predictive range of standard weather forecasts are either based on or influenced by climate normals.\\n'}\n",
      "{'name': 'ComStock', 'description': 'The commercial building sector stock model, or ComStock, is a highly\\ngranular, bottom-up model that uses multiple data sources, statistical\\nsampling methods, and advanced building energy simulations to estimate\\nthe annual sub-hourly energy consumption of the commercial building stock\\nacross the United States.\\n'}\n",
      "{'name': 'Human Cancer Models Initiative (HCMI) Cancer Model Development Center', 'description': 'The Human Cancer Models Initiative (HCMI) is an international consortium that is generating novel,\\nnext-generation, tumor-derived culture models annotated with genomic and clinical data.\\nHCMI-developed models and related data are available as a community resource. The NCI is\\ncontributing to the initiative by supporting four Cancer Model Development Centers (CMDCs).  CMDCs\\nare tasked with producing next-generation cancer models from clinical samples. The cancer models\\ninclude tumor types that are rare, originate from patients from underrepresented populations, lack\\nprecision therapy, or lack cancer model tools. Throughout the development process, the CMDCs\\nutilize stringent internal QC measures to ensure both clinical and molecular integrity. These\\nmodels are then annotated with clinical and genomic data and are available as a community\\nresource.\\n'}\n",
      "{'name': 'OpenSurfaces', 'description': 'A large database of annotated surfaces created from real-world consumer photographs.'}\n",
      "{'name': 'Cloud to Street - Microsoft Flood and Clouds Dataset', 'description': 'This dataset consists of chips of Sentinel-1 and Sentinel-2 satellite data. Each Sentinel-1 chip contains a corresponding label for water and each Sentinel-2 chip contains a corresponding label for water and clouds. Data is stored in folders by a unique event identifier as the folder name. Within each event folder there are subfolders for Sentinel-1 (s1) and Sentinel-2 (s2) data. Each chip is contained in its own sub-folder with the folder name being the source image id, followed by a unique chip identifier consisting of a hyphenated set of 5 numbers. All bands of the satellite data, as well as the labels, and overview images are contained within the chip folder.'}\n",
      "{'name': 'NOAA Coastal Lidar Data', 'description': 'Lidar (light detection and ranging) is a technology that can measure the 3-dimentional location of objects, including the solid earth surface. The data consists of a point cloud of the positions of solid objects that reflected a laser pulse, typically from an airborne platform. In addition to the position, each point may also be attributed by the type of object it reflected from, the intensity of the reflection, and other system dependent metadata. The NOAA Coastal Lidar Data is a collection of lidar projects from many different sources and agencies, geographically focused on the coastal areas of the United States of America. The data is provided in Entwine Point Tiles (https://entwine.io) format, which is a lossless streamable octree of the point cloud. Datasets are maintained in their original projects and care should be taken when merging projects. The coordinate reference system for the data is The NAD83(2011) UTM zone appropriate for the center of each data set and the orthometric datum appropriate for that area (for example, NAVD88 in the mainland United States, PRVD02 in Puerto Rico, or GUVD03 in Guam). The geoid model used is reflected in the data set resource name.'}\n",
      "{'name': 'Epoch of Reionization Dataset', 'description': 'The data are from observations with the Murchison Widefield Array (MWA) which is a\\nSquare Kilometer Array (SKA) precursor in Western Australia.  This particular\\ndataset is from the Epoch of Reionization project which is a key science driver\\nof the SKA. Nearly 2PB of such observations have been recorded to date, this is\\na small subset of that which has been exported from the MWA data archive in\\nPerth and made available to the public on AWS.  The data were taken to detect\\nsignatures of the first stars and galaxies forming and the effect of these early\\nstars and galaxies on the evolution of the universe.\\n'}\n",
      "{'name': 'U.S. Census ACS PUMS', 'description': 'U.S. Census Bureau American Community Survey (ACS) Public Use Microdata Sample (PUMS) available in a linked data format using the Resource Description Framework (RDF) data model.'}\n",
      "{'name': 'Allen Cell Imaging Collections', 'description': 'This bucket contains multiple datasets (as Quilt packages) created by the\\nAllen Institute for Cell Science (AICS). The imaging data in this bucket contains\\neither of the following:\\n\\n1) field of view images from glass plates\\n2) cell membrane, DNA, and structure segmentations\\n3) cell membrane, DNA and structure contours\\n4) machine learning imaging predictions of the previously listed modalities.\\n\\nIn addition, many of the datasets include CSVs that contain feature sets\\nrelated to that data.\\n'}\n",
      "{'name': 'NOAA Unified Forecast System Weather Model (UFS-WM) Regression Tests', 'description': 'The Unified Forecast System (UFS) is a community-based, coupled, comprehensive Earth Modeling System. The ufs-weather-model (UFS-WM) is the model source    of the UFS for NOAA’s operational numerical weather prediction applications. The UFS-WM Regression Test (RT) is the testing software to ensure that previously developed and tested capabilities in UFS-WM still work after code changes are integrated into the system. It is required that UFS-WM RTs are performed successfully on the required Tier-1 platforms whenever code changes are made to the UFS-WM.  The results of the UFS-WM RTs are summarized in log files and these files will be committed to the UFS-WM repository along with the code changes. Currently, the UFS-WM RTs have been developed to support several applications targeted for operational implementations including the global weather forecast, subseasonal to seasonal forecasts, hurricane forecast, regional rapid refresh forecast, and ocean analysis. <br/> <br/> At this time, there are 123 regression tests to support the UFS applications. The tests are evolving along with the development merged to the UFS-WM  code repository. The regression test framework has been developed in the UFS-WM to run these tests on tier-1 supported systems. Each of the regression tests require a set of input data files and configuration files. The configuration files include namelist and model configuration files residing within the UFS-WM code repository. The input data includes initial conditions, climatology data, and fixed data sets such as orographic data and grid specification data. In addition, the regression test framework maintains baseline data created from certain revisions of the UFS-WM code repository. When code changes are not expected to alter baseline results, regression tests will be performed against current baseline and as a result, the regression test log files are created – revealing a summary of no change within the results. If the code changes are expected to alter model results, impact to the regression tests will be specified in the pull request. The code changes and model results will be reviewed and confirmed. Once the model results are confirmed,  a new baseline will be generated. In some cases, new input data will need to be added or old data will need to be replaced, these data will be put in the input data location with proper timestamp, and regression tests will be performed with the updated data sets. <br/> <br/> The regression test framework serves as a test system to maintain the functionalities in the UFS-WM. The input data and the baselines need to be maintained and updated during the code integration to support the regression tests.'}\n",
      "{'name': 'CoversBR', 'description': 'CoversBR is the first large audio database with, predominantly, Brazilian music for the tasks of Covers Song\\nIdentification (CSI) and Live Song Identifications (LSI). Due to copyright restrictions audios of\\nthe songs cannot be made available, however metadata and files of features have public access. Audio\\nstreamings captured from radio and TV channels for the live song identification task will be made public.\\nCoversBR is composed of metadata and features extracted from 102298 songs, distributed in 26366\\ngroups of covers/versions, with an average of 3.88 versions per group. The entire collection adds up to a total of\\napproximately 7070 hours and the average song length is 240 seconds (4 minutes).\\n  \\n'}\n",
      "{'name': 'NASA Prediction of Worldwide Energy Resources (POWER)', 'description': \"NASA's goal in Earth science is to observe, understand, and model the Earth system to discover how it is changing, to better predict change, and to understand the consequences for life on Earth. The Applied Sciences Program serves NASA and Society by expanding and accelerating the realization of societal and economic benefits from Earth science, information, and technology research and development. <br/><br/>\\nThe NASA [Prediction Of Worldwide Energy Resources (POWER)](https://power.larc.nasa.gov/) Project, a NASA Applied Sciences program, improves the accessibility and usage NASA Earth Observations (EO) supporting community research in three focus areas: 1) renewable energy development, 2) building energy efficiency, and 3) agroclimatology applications. POWER can help communities be resilient amid observed climate variability through the easy access of solar and meteorological data via a verity of access methods. <br/><br/>\\nThe latest POWER version includes hourly-based source Analysis Ready Data (ARD), in addition to enhanced daily, monthly, annual, and climatology ARD. The daily time-series spans 40 years for meteorology available from 1981 and solar-based parameters start in 1984. The hourly source data are from Clouds and the Earth's Radiant Energy System (CERES) and Global Modeling and Assimilation Office (GMAO), spanning 20 years from 2001. The hourly data will provide users the ARD needed to model the energy performance of building systems, providing information directly amenable to decision support tools introducing the industry standard EPW (EnergyPlus Weather file). <br/><br/> \\nPOWER also provides parameters at daily, monthly, annual, and user-defined time periods, spanning from 1984 through to within a week of real time. Additionally, POWER provides are user-defined analytic capabilities, including custom climatologies and climatological-based reports for parameter anomalies, ASHRAE® compatible climate design condition statistics, and building climate zones. <br/><br/>\\nThe ARD and climate analytics will be readily accessible through POWER's integrated services suite, including the [Data Access Viewer (DAV)](https://power.larc.nasa.gov/data-access-viewer/). The DAV has recently been improved to incorporate updated parameter groupings, new analytical capabilities, and the new data formats.  POWER also provides a complete [API (Application Programming Interface)](https://power.larc.nasa.gov/api/pages/) that allows uses to obtain all the parameters in the DAV plus additional parameters for larger repetitive orders or from within the user\\\\'s own analytic tools. <br/><br/>\\n\"}\n",
      "{'name': 'WikiSum: Coherent Summarization Dataset for Efficient Human-Evaluation', 'description': 'This dataset provides how-to articles from [wikihow.com](https://www.wikihow.com) and their summaries,\\nwritten as a coherent paragraph.\\nThe dataset itself is available at [wikisum.zip](https://wikisum.s3.amazonaws.com/WikiSumDataset.zip),\\nand contains the article, the summary, the wikihow url, and an official fold (train, val, or test).\\nIn addition, human evaluation results are available at\\n[wikisum-human-eval.zip](https://wikisum.s3.amazonaws.com/HumanEvaluation.zip).\\nIt consists of human evaluation of the summary of the Pegasus system, annotators response regarding the difficulty\\nof the task, and words they marked as unknown.\\n'}\n",
      "{'name': 'Distributed Archives for Neurophysiology Data Integration (DANDI)', 'description': 'DANDI is a public archive of neurophysiology datasets, including raw and processed data,  and associated software containers. Datasets are shared according to a Creative Commons  CC0 or CC-BY licenses. The data archive provides a broad range of cellular neurophysiology data.  This includes electrode and optical recordings, and associated imaging data using a  set of community standards: [NWB:N - NWB:Neurophysiology](https://www.nwb.org/nwb-neurophysiology/), [BIDS - Brain Imaging Data Structure](https://bids.neuroimaging.io/), and  [NIDM - Neuro Imaging Data Model](http://nidm.nidash.org/). Development of DANDI is  supported by the National Institute of Mental Health.\\n'}\n",
      "{'name': 'The Human Connectome Project', 'description': 'The Human Connectome Project (HCP Young Adult, HCP-YA) is mapping the healthy human connectome by collecting and freely distributing neuroimaging and behavioral data on 1,200 normal young adults, aged 22-35.'}\n",
      "{'name': 'COVID-19 Data Lake', 'description': 'A centralized repository of up-to-date and curated datasets on or related to the spread and characteristics of the novel corona virus (SARS-CoV-2) and its associated illness, COVID-19. Globally, there are several efforts underway to gather this data, and we are working with partners to make this crucial data freely available and keep it up-to-date. Hosted on the AWS cloud, we have seeded our curated data lake with COVID-19 case tracking data from Johns Hopkins and The New York Times, hospital bed availability from Definitive Healthcare, and over 45,000 research articles about COVID-19 and related coronaviruses from the Allen Institute for AI.'}\n",
      "{'name': 'Swiss Public Transport Stops', 'description': 'The basic geo-data set for public transport stops comprises public transport stops in Switzerland and additional selected geo-referenced public transport locations that are of operational or structural importance (operating points).\\n'}\n",
      "{'name': 'Digital Earth Africa Global Mangrove Watch', 'description': 'The Global Mangrove Watch (GMW) dataset is a result of the collaboration between Aberystwyth University (U.K.), solo Earth Observation (soloEO; Japan), Wetlands International the World Conservation Monitoring Centre (UNEP-WCMC) and the Japan Aerospace Exploration Agency (JAXA). The primary objective of producing this dataset is to provide countries lacking a national mangrove monitoring system with first cut mangrove extent and change maps, to help safeguard against further mangrove forest loss and degradation.\\nThe Global Mangrove Watch dataset (version 2) consists of a global baseline map of mangroves for 2010 and changes from this baseline for six epochs i.e. 1996, 2007, 2008, 2009, 2015 and 2016. Annual maps are planned from 2018 and onwards. The dataset can be used to identify mangrove ecosystems and monitor changes in mangrove extent. This is important in applications such as quantifying ‘blue carbon’, mitigating risks from natural disasters, and prioritising restoration activities. For more information on the Global Watch Mangrove product see the Global Mangrove Watch website.\\n'}\n",
      "{'name': 'High-Order Accurate Direct Numerical Simulation of Flow over a MTU-T161 Low Pressure Turbine Blade', 'description': 'The archive comprises snapshot, point-probe, and time-average data produced via a high-fidelity computational simulation of turbulent air flow over a low pressure turbine blade, which is an important component in a jet engine. The simulation was undertaken using the open source PyFR flow solver on over 5000 Nvidia K20X GPUs of the Titan supercomputer at Oak Ridge National Laboratory under an INCITE award from the US DOE. The data can be used to develop an enhanced understanding of the complex three-dimensional unsteady air flow patterns over turbine blades in jet engines. This could in turn lead to design of greener more fuel efficient aircraft. It could also be used to train a next-generation of Reynolds Averaged Navier-Stokes turbulence models via a machine learning approach, which would have broad applicability to a wide range of science and engineering problems.'}\n",
      "{'name': 'USGS Landsat', 'description': 'This joint NASA/USGS program provides the longest continuous space-based record of \\nEarth’s land in existence. Every day, Landsat satellites provide essential information \\nto help land managers and policy makers make wise decisions about our resources and our environment.\\nData is provided for Landsats 1, 2, 3, 4, 5, 7, and 8.\\n'}\n",
      "{'name': 'DigitalCorpora', 'description': 'Disk images, memory dumps, network packet captures, and files for use in digital forensics research and education. All of this information is accessible through the digitalcorpora.org website, and made available at s3://digitalcorpora/. Some of these datasets implement scenarios that were performed by students, faculty, and others acting *in persona*. As such, the information is synthetic and may be used without prior authorization or IRB approval. Details of these datasets can be found at http://www.simson.net/clips/academic/2009.DFRWS.Corpora.pdf'}\n",
      "{'name': 'Multimedia Commons', 'description': 'The Multimedia Commons is a collection of audio and visual features computed for the nearly 100 million Creative Commons-licensed Flickr images and videos in the YFCC100M dataset from Yahoo! Labs, along with ground-truth annotations for selected subsets. The International Computer Science Institute (ICSI) and Lawrence Livermore National Laboratory are producing and distributing a core set of derived feature sets and annotations as part of an effort to enable large-scale video search capabilities. They have released this feature corpus into the public domain, under Creative Commons License 0, so it is free for anyone to use for any purpose.'}\n",
      "{'name': 'NYU Langone & FAIR FastMRI Dataset', 'description': 'This dataset contains deidentified raw k-space data and DICOM image files of over 1,500 knees and 6,970 brains.'}\n",
      "{'name': 'Multi-Scale Ultra High Resolution (MUR) Sea Surface Temperature (SST)', 'description': 'A global, gap-free, gridded, daily 1 km Sea Surface Temperature (SST) dataset created by merging multiple Level-2 satellite SST datasets. Those input datasets include the NASA Advanced Microwave Scanning Radiometer-EOS (AMSR-E), the JAXA Advanced Microwave Scanning Radiometer 2 (AMSR-2) on GCOM-W1, the Moderate Resolution Imaging Spectroradiometers (MODIS) on the NASA Aqua and Terra platforms, the US Navy microwave WindSat radiometer, the Advanced Very High Resolution Radiometer (AVHRR) on several NOAA satellites, and in situ SST observations from the NOAA iQuam project. Data are available from 2002 to present in Zarr format. The original source of the MUR data is the NASA JPL Physical Oceanography DAAC.'}\n",
      "{'name': 'Digital Earth Africa GeoMAD', 'description': 'GeoMAD is the Digital Earth Africa (DE Africa) surface reflectance geomedian and triple Median Absolute Deviation data service. It is a cloud-free composite of satellite data compiled over specific timeframes.\\nThe geomedian component combines measurements collected over the specified timeframe to produce one representative, multispectral measurement for every pixel unit of the African continent. The end result is a comprehensive dataset that can be used to generate true-colour images for visual inspection of anthropogenic or natural landmarks. The full spectral dataset can be used to develop more complex algorithms.\\nFor each pixel, invalid data is discarded, and remaining observations are mathematically summarised using the geomedian statistic. Flyover coverage provided by collecting data over a period of time also helps scope intermittently cloudy areas.\\nVariations between the geomedian and the individual measurements are captured by the three Median Absolute Deviation (MAD) layers. These are higher-order statistical measurements calculating variation relative to the geomedian. The MAD layers can be used on their own or together with geomedian to gain insights about the land surface, and understand change over time.\\nCalculating GeoMAD over different timeframes and sensors provides a range of insights to the environment. An annual timeframe allows better correction for cloud cover and reduces artifacts for comparison over multiple years. A semiannual timeframe, for example six-month blocks, better captures seasonal variation within one year, but can also be used to compare equivalent periods from different years. Likewise, Landsat sensors allows full utility of the surface reflectance archive dating back to 1984, while more recent Sentinel-2 data provides higher-frequency flyovers and better resolution.\\nThe Digital Earth Africa GeoMAD service currently provides annual and six-month semiannual datasets, with separate services for Landsat and Sentinel-2 sensors.\\n'}\n",
      "{'name': 'Low Context Name Entity Recognition (NER) Datasets with Gazetteer', 'description': 'See https://lowcontext-ner-gaz.s3.amazonaws.com/readme.html'}\n",
      "{'name': 'GEOS-Chem Input Data', 'description': 'Input data for the GEOS-Chem Chemical Transport Model. Including the NASA/GMAO MERRA-2 and GEOS-FP [meteorological products](http://wiki.seas.harvard.edu/geos-chem/index.php/Overview_of_GMAO_met_data_products), the [HEMCO emission inventories](http://wiki.seas.harvard.edu/geos-chem/index.php/HEMCO_data_directories), and other small data such as [model initial conditions](http://wiki.seas.harvard.edu/geos-chem/index.php/GEOS-Chem_basics#Restart_files).'}\n",
      "{'name': 'Geosnap Data, Center for Geospatial Sciences', 'description': 'This bucket contains multiple datasets (as Quilt packages) created by the\\nCenter for Geospatial Sciences (CGS) at the University of California-Riverside.\\nThe data in this bucket contains the following:\\n\\n1) Tabular and geographic data from the US Census\\n2) Land Cover imagery collected from [Multi-Resolution Land Characteristics Consortium](https://www.mrlc.gov/)\\n3) Road network data processed from OpenStreetMap\\n'}\n",
      "{'name': 'Normalized Difference Urban Index (NDUI)', 'description': 'NDUI is combined with cloud shadow-free Landsat Normalized Difference Vegetation Index (NDVI) composite and DMSP/OLS Night Time Light (NTL) to characterize global urban areas at a 30 m resolution,and it can greatly enhance urban areas, which can then be easily distinguished from bare lands including fallows and deserts. With the capability to delineate urban boundaries and, at the same time, to present sufficient spatial details within urban areas, the  NDUI has the potential for urbanization studies at regional and global scales.'}\n",
      "{'name': 'Coupled Model Intercomparison Project 6', 'description': 'The sixth phase of global coupled ocean-atmosphere general circulation model ensemble. \\n<br /><br />\\n'}\n",
      "{'name': 'Multiview Extended Video with Activities (MEVA)', 'description': 'The Multiview Extended Video with Activities (MEVA) dataset consists\\nvideo data of human activity, both scripted and unscripted,\\ncollected with roughly 100 actors over several weeks.  The data was\\ncollected with 29 cameras with overlapping and non-overlapping\\nfields of view. The current release consists of about 328 hours\\n(516GB, 4259 clips) of video data, as well as 4.6 hours (26GB) of\\nUAV data. Other data includes GPS tracks of actors, camera models,\\nand a site map. We have also released annotations for roughly 184 hours of\\ndata. Further updates are planned.\\n'}\n",
      "{'name': 'International Neuroimaging Data-Sharing Initiative (INDI)', 'description': 'This bucket contains multiple neuroimaging datasets that are part of the International Neuroimaging Data-Sharing Initiative. Raw human and non-human primate neuroimaging data include 1) Structural MRI; 2) Functional MRI; 3) Diffusion Tensor Imaging; 4) Electroencephalogram (EEG)\\nIn addition to the raw data, preprocessed data is also included for some datasets.\\nA complete list of the available datasets can be seen in the documentation lonk provided below. \\n'}\n",
      "{'name': 'Cell Organelle Segmentation in Electron Microscopy (COSEM) on AWS', 'description': 'High resolution images of subcellular structures.'}\n",
      "{'name': 'Toxicant Exposures and Responses by Genomic and Epigenomic Regulators of Transcription (TaRGET)', 'description': 'The TaRGET (Toxicant Exposures and Responses by Genomic and Epigenomic Regulators of Transcription) Program is a research consortium funded by the National Institute of Environmental Health Sciences (NIEHS). The goal of the collaboration is to address the role of environmental exposures in disease pathogenesis as a function of epigenome perturbation, including understanding the environmental control of epigenetic mechanisms and assessing the utility of surrogate tissue analysis in mouse models of disease-relevant environmental exposures.\\n'}\n",
      "{'name': 'MultiCoNER Dataset', 'description': 'MultiCoNER is a large multilingual dataset (11 languages) for Named Entity Recognition. It is designed to represent some of the contemporary challenges in NER, including low-context scenarios (short and uncased text), syntactically complex entities such as movie titles, and long-tail entity distributions.'}\n",
      "{'name': 'A2D2: Audi Autonomous Driving Dataset', 'description': 'An open multi-sensor dataset for autonomous driving research. This dataset comprises semantically segmented images, semantic point clouds, and 3D bounding boxes. In addition, it contains unlabelled 360 degree camera images, lidar, and bus data for three sequences. We hope this dataset will further facilitate active research and development in AI, computer vision, and robotics for autonomous driving.'}\n",
      "{'name': 'SpaceNet', 'description': 'SpaceNet, launched in August 2016 as an open innovation project offering a repository of freely available\\nimagery with co-registered map features. Before SpaceNet, computer vision researchers had minimal options\\nto obtain free, precision-labeled, and high-resolution satellite imagery. Today, SpaceNet hosts datasets\\ndeveloped by its own team, along with data sets from projects like IARPA’s Functional Map of the World (fMoW).\\n'}\n",
      "{'name': 'K2 Mission Data', 'description': 'The K2 mission observed 100 square degrees for 80 days each across 20 different pointings along the ecliptic, collecting high-precision photometry for a selection of targets within each field. The mission began when the original Kepler mission ended due to loss of the second reaction wheel in 2011. More information about the K2 mission is available at [MAST](https://archive.stsci.edu/k2/).\\n'}\n",
      "{'name': 'RADARSAT-1', 'description': \"Developed and operated by the Canadian Space Agency, it is Canada's first commercial Earth observation satellite.\"}\n",
      "{'name': 'UCSC Genome Browser Sequence and Annotations', 'description': \"The UCSC Genome Browser is an online graphical viewer for genomes, a genome browser, hosted by the University of California, Santa Cruz (UCSC). The interactive website offers access to genome sequence data from a variety of vertebrate and invertebrate species and major model organisms, integrated with a large collection of aligned annotations. This dataset is a copy of the MySQL tables in MyISAM binary and tab-sep format and all binary files in custom formats, sometimes referred as 'gbdb'-files. Data from the UCSC Genome Browser is free and open for use by anyone. However, every genome annotation track has been created by an academic research group, or, in a few cases, by commercial companies. Please acknowledge them by citing them. The information can be found by going to https://genome.ucsc.edu, selecting the respective genome assembly and clicking on the data track. At the end of the documentation, we provide a list of references and acknowledgements.\"}\n",
      "{'name': 'Natural Scenes Dataset', 'description': 'Here, we collected and pre-processed a massive, high-quality 7T fMRI dataset that can be used to advance our understanding of how the brain works. A unique feature of this dataset is the massive amount of data available per individual subject. The data were acquired using ultra-high-field fMRI (7T, whole-brain, 1.8-mm resolution, 1.6-s TR). We measured fMRI responses while each of 8 participants viewed 9,000–10,000 distinct, color natural scenes (22,500–30,000 trials) in 30–40 weekly scan sessions over the course of a year. Additional measures were collected including resting-state data, retinotopy, category localizers, anatomical data (T1, T2, diffusion, venogram, angiogram), physiological data (pulse, respiration), eye-tracking data, and additional behavioral assessments outside the scanner. Because of its unprecedented scale and richness, NSD can be used to explore diverse neuroscientific questions with high power at the level of individual subjects. In particular, the number of images sampled in this dataset is sufficiently large that the dataset may be of high interest for computer vision, machine learning, and other data-driven applications.'}\n",
      "{'name': 'Pacific Ocean Sound Recordings', 'description': 'This project offers passive acoustic data (sound recordings) from a deep-ocean environment off central California.  Recording began in July 2015, has been nearly continuous, and is ongoing.  These resources are intended for applications\\nin ocean soundscape research, education, and the arts.\\n'}\n",
      "{'name': 'YouTube 8 Million - Data Lakehouse Ready', 'description': \"This both the original .tfrecords and a Parquet representation of the [YouTube 8 Million dataset](https://research.google.com/youtube8m/). YouTube-8M is a large-scale labeled video dataset that consists of millions of YouTube video IDs, with high-quality machine-generated annotations from a diverse vocabulary of 3,800+ visual entities. It comes with precomputed audio-visual features from billions of frames and audio segments, designed to fit on a single hard disk. This dataset also includes the YouTube-8M Segments data from June 2019. \\nThis dataset is 'Lakehouse Ready'. Meaning, you can query this data in-place straight out of the Registry of Open Data S3 bucket. [Deploy this dataset's corresponding CloudFormation template](https://us-west-2.console.aws.amazon.com/cloudformation/home?region=us-west-2#/stacks/quickcreate?templateUrl=https://aws-roda-ml-datalake.s3.us-west-2.amazonaws.com/YT8MRodaTemplate.RodaTemplate.json&stackName=YT8M-RODA) to create the  AWS Glue Catalog entries into your account in about 30 seconds. That one step will enable you to interact with the data with AWS Athena, AWS SageMaker, AWS EMR, or join into your AWS Redshift clusters. More detail in (the documentation)[https://github.com/aws-samples/data-lake-as-code/blob/roda-ml/README.md.\\n\"}\n",
      "{'name': 'NOAA High-Resolution Rapid Refresh (HRRR) Model', 'description': 'The HRRR is a NOAA real-time 3-km resolution, hourly updated, cloud-resolving, convection-allowing atmospheric model, initialized by 3km grids with 3km radar assimilation. Radar data is assimilated in the HRRR every 15 min over a 1-h period adding further detail to that provided by the hourly data assimilation from the 13km radar-enhanced Rapid Refresh.\\n'}\n",
      "{'name': 'Copernicus Digital Elevation Model (DEM)', 'description': 'The Copernicus DEM is a Digital Surface Model (DSM) which represents the surface of the Earth including buildings, infrastructure and vegetation. We provide two instances of Copernicus DEM named GLO-30 Public and GLO-90. GLO-90 provides worldwide coverage at 90 meters. GLO-30 Public provides limited worldwide coverage at 30 meters because a small subset of tiles covering specific countries are not yet released to the public by the Copernicus Programme. Note that in both cases ocean areas do not have tiles, there one can assume height values equal to zero. Data is provided as Cloud Optimized GeoTIFFs and comes from Copernicus DEM 2021 release.\\n'}\n",
      "{'name': 'CCAFS-Climate Data', 'description': 'High resolution climate data to help assess the impacts of climate change primarily on agriculture. These open access datasets of climate projections will help researchers make climate change impact assessments.'}\n",
      "{'name': 'Amazon Bin Image Dataset', 'description': 'The Amazon Bin Image Dataset contains over 500,000 images and metadata from bins of a pod in an operating Amazon Fulfillment Center. The bin images in this dataset are captured as robot units carry pods as part of normal Amazon Fulfillment Center operations.'}\n",
      "{'name': 'EPA Risk-Screening Environmental Indicators', 'description': 'Detailed air model results from EPA’s Risk-Screening Environmental Indicators (RSEI) model.'}\n",
      "{'name': 'NOAA Global Real-Time Ocean Forecast System (Global RTOFS)', 'description': \"NOAA's Global Real-Time Ocean Forecast System (Global RTOFS) provides users with nowcasts (analyses of near present conditions) and forecast guidance up to eight days of ocean temperature and salinity, water velocity, sea surface elevation, sea ice coverage and sea ice thickness. <br /><br />\\n\\nThe Global Operational Real-Time Ocean Forecast System (Global RTOFS) is based on an eddy resolving 1/12° global HYCOM (HYbrid Coordinates Ocean Model) (https://www.hycom.org/), which is coupled to the Community Ice CodE (CICE) Version 4 (https://www.arcus.org/witness-the-arctic/2018/5/highlight/1). The RTOFS grid has a 1/12 degree horizontal resolution and 41 hybrid vertical levels on a global tripolar grid. <br /><br />\\n\\nSince 2020, the RTOFS system implements a multivariate, multi-scale 3DVar data assimilation algorithm (Cummings and Smedstad, 2014) using a 24-hour update cycle. The data types presently assimilated include <br /><br />\\n\\n(1) satellite Sea Surface Temperature (SST) from METOP-B, JPSS-VIIRS, and  in-Situ SST, from ships, fixed and drifting buoys <br />\\n(2) Sea Surface Salinity (SSS) from SMAP, SMOS, and buoys <br />\\n(3) profiles of Temperature and Salinity from Animal-borne, Alamo floats, Argo floats, CTD, fixed buoys, gliders, TESAC, and XBT <br />\\n(4) Absolute Dynamic Topography (ADT) from Altika, Cryosat, Jason-3, Sentinel 3a, 3b, 6a <br />\\n(5) sea ice concentration from SSMI/S, AMSR2 <br /> <br />\\n\\nThe system is designed to incorporate new observing systems as the data becomes available. <br /> <br />\\n\\nOnce the observations go through a fully automated quality control and thinning process, the increments, or corrections, are obtained by executing the 3D variational algorithm. The increments are then added to the 24-hours forecast fields using a 6-hourly incremental analysis update. An earlier version of the system is described in Garraffo et al (2020). <br /> <br />\\n\\nGarraffo, Z.D., J.A. Cummings, S. Paturi, Y. Hao, D. Iredell, T. Spindler, B. Balasubramanian, I. Rivin, H-C. Kim, A. Mehra, 2020. Real Time Ocean-Sea Ice Coupled Three Dimensional Variational Global Data Assimilative Ocean Forecast System. In Research Activities in Earth System Modeling, edited by E. Astakhova, WMO, World Climate Research Program Report No.6, July 2020. <br /> <br />\\n\\nCummings, J. A. and O. M. Smedstad. 2013. Variational Data Assimilation for the Global Ocean.\\nData Assimilation for Atmospheric, Oceanic and Hydrologic Applications (Vol II)\\nS. Park and L. Xu (eds), Springer, Chapter 13, 303-343. <br /> <br />\\n\\nGlobal Real Time Ocean Forecasting System RTOFS output available here include NetCDF and Hycom binary a/b files. Please see README documentation for more details. \\n\"}\n",
      "{'name': 'Cancer Cell Line Encyclopedia (CCLE)', 'description': 'The Cancer Cell Line Encyclopedia (CCLE) project is an effort to conduct a detailed genetic\\ncharacterization of a large panel of human cancer cell lines. The CCLE provides public access to\\ngenomic data, visualization and analysis for over 1100 cancer cell lines. This dataset contains\\nRNA-Seq Aligned Reads, WXS Aligned Reads, and WGS Aligned Reads data.\\n'}\n",
      "{'name': 'PubSeq - Public Sequence Resource', 'description': 'COVID-19 PubSeq is a free and open online bioinformatics public sequence resource with on-the-fly analysis of sequenced SARS-CoV-2 samples that allows for a quick turnaround in identification of new virus strains. PubSeq allows anyone to upload sequence material in the form of FASTA or FASTQ files with accompanying metadata through the web interface or REST API.'}\n",
      "{'name': 'DNAStack COVID19 SRA Data', 'description': 'The [Sequence Read Archive (SRA)](https://www.ncbi.nlm.nih.gov/sra/) is the primary archive of high-throughput sequencing data, hosted by the National Institutes of Health (NIH). The SRA represents the largest publicly available repository of SARS-CoV-2 sequencing data. This dataset was created by DNAstack using SARS-CoV-2 sequencing data sourced from the SRA. Where possible, raw sequence data were processed by DNAstack through a unified bioinformatics pipeline to produce genome assemblies and variant calls. The use of a standardized workflow to produce this harmonized dataset allows public data generated using different methodologies to be combined and compared for a more powerful global analysis of available SARS-CoV-2 data, allowing researchers rapid access to aggregated downstream results for accelerated insight generation. Methodology: Reads from the SRA were extracted in FASTQ format, then entered into a different pipeline depending on the sequencing technology used to create the reads: the [ARTIC protocol](https://artic.network/ncov-2019/ncov2019-bioinformatics-sop.html) for Oxford Nanopore-derived reads; the [SIGNAL pipeline](https://github.com/jaleezyy/covid-19-signal) for paired-end Illumina reads; and the [CoSA pipeline](https://github.com/PacificBiosciences/CoSA) (using [DeepVariant](https://github.com/google/deepvariant) for variant calling) for PacBio reads. Briefly, reads were primer-trimmed and aligned to the SARS-CoV-2 reference genome, following which contiguous regions were assembled and variant sites were called. [Pangolin](https://github.com/cov-lineages/pangolin) was then used to assign viral lineage based on the assembled genome.\\n'}\n",
      "{'name': 'Sentinel-1 SLC dataset for South and Southeast Asia, Taiwan, Korea and Japan', 'description': 'The S1 Single Look Complex (SLC) dataset contains Synthetic Aperture Radar (SAR) data in the C-Band wavelength. The SAR sensors are installed on a two-satellite (Sentinel-1A and Sentinel-1B) constellation orbiting the Earth with a combined revisit time of six days, operated by the European Space Agency. The S1 SLC data are a Level-1 product that collects radar amplitude and phase information in all-weather, day or night conditions, which is ideal for studying natural hazards and emergency response, land applications, oil spill monitoring, sea-ice conditions, and associated climate change effects.\\n'}\n",
      "{'name': 'The Klarna Product-Page Dataset', 'description': 'A collection of 51,701 product pages from 8175 e-commerce websites across 8 markets (US, GB, SE, NL, FI, NO, DE, AT) with 5 manually labelled elements, specifically, the product price, name and image, add-to-cart and go-to-cart buttons.\\nThe dataset was collected between 2018 and 2019 and is made available has MHTML and as WebTraversalLibrary-format snapshots.\\n'}\n",
      "{'name': 'AWS iGenomes', 'description': 'Common reference genomes hosted on AWS S3. Can be used when aligning and analysing raw DNA sequencing data.'}\n",
      "{'name': 'FashionLocalTriplets', 'description': 'Fine-grained localized visual similarity and search for fashion.'}\n",
      "{'name': 'stdpopsim species resources', 'description': 'Contains all resources (genome specifications, recombination maps, etc.) required for species specific simulation with the stdpopsim package. These resources are originally from a variety of other consortium and published work but are consolidated here for ease of access and use. If you are interested in adding a new species to the stdpopsim resource please raise an issue on the stdpopsim GitHub page to have the necessary files added here.'}\n",
      "{'name': 'Hecatomb Databases', 'description': 'Preprocessed databases for use with the Hecatomb pipeline for viral and phage sequence annotation.'}\n",
      "{'name': 'Deutsche Börse Public Dataset', 'description': \"The Deutsche Börse Public Data Set consists of trade data aggregated to one minute intervals from the Eurex and Xetra trading systems. It provides the initial price, lowest price, highest price, final price and volume for every minute of the trading day, and for every tradeable security.  If you need higher resolution data, including untraded price movements, please refer to our historical market data product [here](http://www.eurexchange.com/exchange-en/market-data/historical-data).  Also, be sure to check out our [developer's portal](https://console.developer.deutsche-boerse.com/).\"}\n",
      "{'name': 'SILAM Air Quality', 'description': 'Air Quality is a global SILAM atmospheric composition and air quality forecast performed on a daily basis for > 100 species and covering the troposphere and the stratosphere. The output produces 3D concentration fields and aerosol optical thickness. The data are unique: 20km resolution for global AQ models is unseen worldwide.'}\n",
      "{'name': 'UK Met Office Global and Regional Weather Forecasts', 'description': 'Data from two models is available: MOEGREPS-UK, a high resolution weather forecast covering the United Kingdom, and MOGREPS-G, a global weather forecast.'}\n",
      "{'name': 'Homeland Security and Infrastructure US Cities', 'description': 'The U.S. Cities elevation data collection program supported the US Department of Homeland Security Homeland Security and Infrastructure Program (HSIP). As part of the HSIP Program, there were 133+ U.S. cities that had imagery and LiDAR collected to provide the Homeland Security, Homeland Defense, and Emergency Preparedness, Response and Recovery (EPR&R) community with common operational, geospatially enabled baseline data needed to analyze threat, support critical infrastructure protection and expedite readiness, response and recovery in the event of a man-made or natural disaster. As a part of that, for some time, recurring LiDAR data was also being collected by a joint agreement of NGA and other federal agencies and the HIFLDS Working Group. The publicly released data excluded US Military Installation coverage, but it is provided in as is. These collects were acquired by contract using commercial collection companies. Some metadata information about the collection can be found at USGS at https://rockyweb.usgs.gov/vdelivery/Datasets/Staged/Elevation/Non_Standard_Contributed/NGA_US_Cities/Topeka_KS/NGA%20133%20US%20Cities%20Data%20Disclaimer%20and%20Explanation%20Readme.pdf'}\n",
      "{'name': 'Corn Kernel Counting Dataset', 'description': 'Dataset associated with the March 2021 Frontiers in Robotics and AI paper \"Broad Dataset and Methods for Counting and Localization of On-Ear Corn Kernels\", DOI: 10.3389/frobt.2021.627009'}\n",
      "{'name': 'Smithsonian Open Access', 'description': 'The Smithsonian’s mission is the \"increase and diffusion of knowledge\" and has been collecting since 1846. The Smithsonian, through its efforts to digitize its multidisciplinary collections, has created millions of digital assets and related metadata describing the collection objects. On February 25th, 2020, the Smithsonian released over 2.8 million CC0 interdisciplinary 2-D and 3-D images, related metadata, and additionally, research data from researches across the Smithsonian. The 2.8 million \"open access\" collections are a subset of the Smithsonian’s 155 million objects, 2.1 million library volumes and 156,000 cubic feet of archival collections held in 19 museums, 9 research centers, libraries, archives and the National Zoo. Digitization of collections is ongoing.\\n'}\n",
      "{'name': 'Sentinel-1', 'description': '[Sentinel-1](https://sentinel.esa.int/web/sentinel/missions/sentinel-1) is a pair of European radar imaging (SAR) satellites launched in 2014 and 2016. Its 6 days revisit cycle and ability to observe through clouds makes it perfect for sea and land monitoring, emergency response due to environmental disasters, and economic applications. This dataset represents the global Sentinel-1 GRD archive, from beginning to the present, converted to [cloud-optimized GeoTIFF format](https://www.cogeo.org/).'}\n",
      "{'name': 'Maxar Open Data Program', 'description': 'Pre and post event high-resolution satellite imagery in support of emergency planning, risk assessment, monitoring of staging areas and emergency response, damage assessment, and recovery. Also includes crowdsourced damage assessments for major, sudden onset disasters.\\n'}\n",
      "{'name': 'OpenStreetMap on AWS', 'description': 'OSM is a free, editable map of the world, created and maintained by volunteers. Regular OSM data archives are made available in Amazon S3.'}\n",
      "{'name': 'NREL National Solar Radiation Database', 'description': \"Released to the public as part of the Department of Energy's Open Energy Data Initiative,\\nthe [National Solar Radiation Database (NSRDB)](https://nsrdb.nrel.gov/) is\\na serially complete collection of hourly and half-hourly values of the three\\nmost common measurements of solar radiation – global horizontal, direct\\nnormal, and diffuse horizontal irradiance — and meteorological data. These\\ndata have been collected at a sufficient number of locations and temporal and\\nspatial scales to accurately represent regional solar radiation climates.\\n\"}\n",
      "{'name': 'AI2 TabMCQ: Multiple Choice Questions aligned with the Aristo Tablestore', 'description': '9092 crowd-sourced science questions and 68 tables of curated facts'}\n",
      "{'name': 'NOAA S-102 Bathymetric Surface Data', 'description': 'S-102 is a data and metadata encoding specification that is part of the [S-100 Universal Hydrographic Data Model](https://iho.int/en/s100-project), an international standard for hydrographic data exchange. This collection of data contains bathymetric surfaces from [NOAA/NOS/OCS National Bathymetric Source](https://nauticalcharts.noaa.gov/updates/building-the-national-bathymetry/), for various U.S. coastal and offshore waters and the great lakes. These datasets are encoded as HDF5 files conforming to the S-102 specification.'}\n",
      "{'name': 'Cloud Indexes for Bowtie, Kraken, HISAT, and Centrifuge', 'description': 'Genomic tools use reference databases as indexes to operate quickly and efficiently, analogous to how web search engines use indexes for fast querying. Here, we aggregate genomic, pan-genomic and metagenomic indexes for analysis of sequencing data.'}\n",
      "{'name': 'CMAS Data Warehouse', 'description': 'CMAS Data Warehouse on AWS collects and disseminates meteorology, emissions and air quality model input and output for Community Multiscale Air Quality (CMAQ) Model Applications'}\n",
      "{'name': 'Galaxy Evolution Explorer Satellite (GALEX)', 'description': 'The Galaxy Evolution Explorer Satellite (GALEX) was a NASA mission led by the California Institute of Technology, whose primary goal was to investigates how star formation in galaxies evolved from the early Universe up to the present. GALEX used microchannel plate detectors to obtain direct images in the near-UV (NUV) and far-UV (FUV), and a grism to disperse light for low resolution spectroscopy. More information about GALEX is available at [MAST](https://archive.stsci.edu/missions-and-data/galex)\\n'}\n",
      "{'name': 'NOAA Global Ensemble Forecast System (GEFS) Re-forecast', 'description': 'NOAA has generated a multi-decadal reanalysis and reforecast data set to accompany the next-generation version of its ensemble prediction system, the Global Ensemble Forecast System, version 12 (GEFSv12). Accompanying the real-time forecasts are “reforecasts” of the weather, that is, retrospective forecasts spanning the period 2000-2019. These reforecasts are not as numerous as the real-time data; they were generated only once per day, from 00 UTC initial conditions, and only 5 members were provided, with the following exception. Once weekly, an 11-member reforecast was generated, and these extend in lead time to +35 days. \\n'}\n",
      "{'name': 'Voices Obscured in Complex Environmental Settings (VOiCES)', 'description': 'VOiCES is a speech corpus recorded in acoustically challenging settings,\\nusing distant microphone recording. Speech was recorded in real rooms with various\\nacoustic features (reverb, echo, HVAC systems, outside noise, etc.). Adversarial noise,\\neither television, music, or babble, was concurrently played with clean speech.\\nData was recorded using multiple microphones strategically placed\\nthroughout the room. The corpus includes audio recordings, orthographic transcriptions,\\nand speaker labels.\\n'}\n",
      "{'name': 'Allen Ivy Glioblastoma Atlas', 'description': 'This dataset consists of images of glioblastoma human brain tumor tissue sections that have been probed for expression of particular genes believed to play a role in development of the cancer.  Each tissue section is adjacent to another section that was stained with a reagent useful for identifying histological features of the tumor.  Each of these types of images has been completely annotated for tumor features by a machine learning process trained by expert medical doctors.\\n'}\n",
      "{'name': 'Virginia Coastal Resilience Master Plan, Phase 1 - December 2021', 'description': 'The Virginia Coastal Resilience Master Plan builds on the 2020 Virginia Coastal Resilience Master Planning Framework, which outlined the goals and principles of the Commonwealth’s statewide coastal resilience strategy. Recognizing the urgent challenge flooding already poses, the Commonwealth developed Phase One of the Master Plan on an accelerated timeline and focused this first assessment on the impacts of tidal and storm surge coastal flooding on coastal Virginia.  The Master Plan leveraged the combined efforts of more than two thousand stakeholders, subject matter experts, and government personnel. We centered the development of this plan around three core components:\\n<br/>\\n<br/>\\nA Technical Study compiled essential data, research, processes, products, and resilience efforts in the Coastal Resilience Database, which forms much of basis of this plan and the Coastal Resilience Web Explorer;\\n<br/>\\n<br/>\\nA Technical Advisory Committee supported coordination across key stakeholders and ensured the incorporation of the best available subject matter knowledge, data, and methods into this plan; and\\n<br/>\\n<br/>\\nStakeholder Engagement captured diverse resilience perspectives from residents, local and regional officials, and other stakeholders across Virginia’s coastal communities to drive regionally specific resilience priorities.Data products used and generated for the Virginia Coastal Resilience.\\n<br/>\\n<br/>\\nThis dataset represents the data that was developed for the technical study.  Appendix F - Data Product List provides a list of available data.  Other Appendix documents provide the input data sources and processing methods to develop this data. Data products used and generated for the Virginia Coastal Resilience Master Plan\\n'}\n",
      "{'name': 'NOAA Global Ensemble Forecast System (GEFS)', 'description': 'The Global Ensemble Forecast System (GEFS), previously known as the GFS Global ENSemble (GENS), is a weather forecast model made up of 21 separate forecasts, or ensemble members.  The National Centers for Environmental Prediction (NCEP) started the GEFS to address the nature of uncertainty in weather observations, which is used to initialize weather forecast models. The GEFS attempts to quantify the amount of uncertainty in a forecast by generating an ensemble of multiple forecasts, each minutely different, or perturbed, from the original observations. With global coverage, GEFS is produced four times a day with weather forecasts going out to 16 days.'}\n",
      "{'name': '3DCoMPaT: Composition of Materials on Parts of 3D Things', 'description': '3D CoMPaT is a richly annotated large-scale dataset of rendered compositions of Materials on Parts of thousands of unique 3D Models.\\nThis dataset primarily focuses on stylizing 3D shapes at part-level with compatible materials.\\nEach object with the applied part-material compositions is rendered from four equally spaced views as well as four randomized views.\\nWe introduce a new task, called Grounded CoMPaT Recognition (GCR), to collectively recognize and ground compositions of materials on parts of 3D objects.\\nWe present two variations of this task and adapt state-of-art 2D/3D deep learning methods to solve the problem as baselines for future research.\\nWe hope our work will help ease future research on compositional 3D Vision.\\n'}\n",
      "{'name': 'Encyclopedia of DNA Elements (ENCODE)', 'description': 'The Encyclopedia of DNA Elements (ENCODE) Consortium is an international collaboration of\\nresearch groups funded by the National Human Genome Research Institute (NHGRI). The goal\\nof ENCODE is to build a comprehensive parts list of functional elements in the human genome,\\nincluding elements that act at the protein and RNA levels, and regulatory elements that\\ncontrol cells and circumstances in which a gene is active. ENCODE investigators employ a\\nvariety of assays and methods to identify functional elements. The discovery and annotation\\nof gene elements is accomplished primarily by sequencing a diverse range of RNA sources,\\ncomparative genomics, integrative bioinformatic methods, and human curation. Regulatory\\nelements are typically investigated through DNA hypersensitivity assays, assays of\\nDNA methylation, and immunoprecipitation (IP) of proteins that interact with DNA and RNA,\\ni.e., modified histones, transcription factors, chromatin regulators, and\\nRNA-binding proteins, followed by sequencing.\\n'}\n",
      "{'name': 'Allen Brain Observatory - Visual Coding AWS Public Data Set', 'description': 'The Allen Brain Observatory – Visual Coding is a large-scale, standardized survey of physiological activity across the mouse visual cortex, hippocampus, and thalamus. It includes datasets collected with both two-photon imaging and Neuropixels probes, two complementary techniques for measuring the activity of neurons in vivo. The two-photon imaging dataset features visually evoked calcium responses from GCaMP6-expressing neurons in a range of cortical layers, visual areas, and Cre lines. The Neuropixels dataset features spiking activity from distributed cortical and subcortical brain regions, collected under analogous conditions to the two-photon imaging experiments. We hope that experimentalists and modelers will use these comprehensive, open datasets as a testbed for theories of visual information processing.\\n'}\n",
      "{'name': 'NOAA Global Extratropical Surge and Tide Operational Forecast System (Global ESTOFS)', 'description': 'NOTICE - The Coast Survey Development Laboratory (CSDL) in NOAA/National Ocean Service (NOS)/Office of Coast Survey is proposing to upgrade the Surge and Tide Operational Forecast System (STOFS, formerly ESTOFS) to Version 1.0.1 in late fall of 2022. CSDL is seeking comments on this proposed upgrade through September 1, 2022. If approved, a Service Change Notice (SCN) will be issued at least 30 days before implementation of STOFS V1.0.1 with more detailed information. More details on the Public Information Statement can be found \"[HERE](https://www.weather.gov/media/notification/pdf2/pns22-37_stofs_dec_2022_upgrade_aaa.pdf)\"<br /><br />\\n\\nNOAA\\'s Global Extratropical Surge and Tide Operational Forecast System (Global ESTOFS) provides users with nowcasts (analyses of near present conditions) and forecast guidance of water level conditions for the entire globe. Global ESTOFS has been developed to serve the marine navigation, weather forecasting, and disaster mitigation user communities. Global ESTOFS was developed in a collaborative effort between the NOAA/National Ocean Service (NOS)/Office of Coast Survey, the NOAA/National Weather Service (NWS)/National Centers for Environmental Prediction (NCEP) Central Operations (NCO), the University of Notre Dame, the University of North Carolina, and The Water Institute of the Gulf. The model generates forecasts out to 180 hours four times per day; forecast output includes water levels caused by the combined effects of storm surge and tides, by astronomical tides alone, and by sub-tidal water levels (isolated storm surge). <br /><br />\\n\\nThe hydrodynamic model employed by Global ESTOFS is the ADvanced CIRCulation (ADCIRC) finite element model. The model is forced by GFS winds, mean sea level pressure, and sea ice. The unstructured grid used by Global ESTOFS consists of 8,452,486 nodes and 16,226,163 triangular elements. Coastal resolution is up to 80 m for Hawaii and the U.S. West Coast; up to 90-120 m for the Pacific Islands including Guam, American Samoa, Marianas, Wake Island, Marshall Islands, and Palau; and up to 120 m for the U.S. East Coast, Puerto Rico, Micronesia, and Alaska. The flood plain extends overland to approximately 6 m elevation ASL for the U.S. East Coast, and up to 20 m elevation ASL for the Pacific Islands. Global ESTOFS a) reduces bias and errors due to the removal of the open ocean boundaries that were included in previous ESTOFS regional domains (ESTOFS-Atlantic, -Pacific, -Micronesia); b) includes internal tide-induced dissipation in the deep ocean; c) includes sea ice effect on wind drag, and d) incorporates a bias correction using 2-day average water level observations from CO-OPS tide stations that are interpolated spatially across the Global ESTOFS mesh. <br /><br />\\n\\nGlobal ESTOFS water level forecast guidance output available here include NetCDF, GRIB2, and SHEF files. Please see README documentation for more details.\\n'}\n",
      "{'name': 'Digital Earth Africa Water Observations from Space', 'description': 'Water Observations from Space (WOfS) is a service that draws on satellite imagery to provide historical surface water observations of the whole African continent. WOfS allows users to understand the location and movement of inland and coastal water present in the African landscape. It shows where water is usually present; where it is seldom observed; and where inundation of the surface has been observed by satellite.\\nThey are generated using the WOfS classification algorithm on Landsat satellite data. There are several WOfS products available for the African continent including scene-level data and annual or all time summaries.\\n'}\n",
      "{'name': 'Unidata NOAA Global Forecast System (GFS) Model', 'description': 'The Global Forecast System (GFS) is a weather forecast model produced by the National Centers for Environmental Prediction (NCEP). Dozens of atmospheric and land-soil variables are available through this dataset, from temperatures, winds, and precipitation to soil moisture and atmospheric ozone concentration. The entire globe is covered by the GFS at a base horizontal resolution of 18 miles (28 kilometers) between grid points, which is used by the operational forecasters who predict weather out to 16 days in the future. Horizontal resolution drops to 44 miles (70 kilometers) between grid point for forecasts between one week and two weeks.'}\n",
      "{'name': 'Prefeitura Municipal de São Paulo (PMSP) LiDAR Point Cloud', 'description': 'The objective of the Mapa 3D Digital da Cidade (M3DC) of the São Paulo City Hall is to publish LiDAR point cloud data. The initial data was acquired in 2017 by aerial surveying and future data will be added. This publicly accessible dataset is provided in the [Entwine Point Tiles](https://entwine.io/entwine-point-tile.html) format as a lossless octree, full density, based on [LASzip](https://laszip.org) (LAZ) encoding.'}\n",
      "{'name': 'Learning to Rank and Filter - community question answering', 'description': \"This dataset provides product related questions and answers, including answers' quality labels, as as part of the paper 'IR Evaluation and Learning in the Presence of Forbidden Documents'.\"}\n",
      "{'name': 'NOAA North American Mesoscale Forecast System (NAM)', 'description': 'The North American Mesoscale Forecast System (NAM) is one of the National Centers For Environmental Prediction’s (NCEP) major models for producing weather forecasts. NAM generates multiple grids (or domains) of weather forecasts over the North American continent at various horizontal resolutions. Each grid contains data for dozens of weather parameters, including temperature, precipitation, lightning, and turbulent kinetic energy. NAM uses additional numerical weather models to generate high-resolution forecasts over fixed regions, and occasionally to follow significant weather events like hurricanes.'}\n",
      "{'name': 'Gabriella Miller Kids First Pediatric Research Program (Kids First)', 'description': \"The NIH Common Fund's Gabriella Miller Kids First Pediatric Research Program’s (“Kids First”) vision is to “alleviate suffering from childhood cancer and structural birth defects by fostering collaborative research to uncover the etiology of these diseases and by supporting data sharing within the pediatric research community.” The program continues to generate and share whole genome sequence data from thousands of children affected by these conditions, ranging from rare pediatric cancers, such as osteosarcoma, to more prevalent diagnoses, such as congenital heart defects. In 2018, Kids First launched the Gabriella Miller Kids First Data Resource Center, charged with building a large-scale data platform supporting clinical and genetic data from these patients and their families in order to accelerate discovery and ultimately clinical impact. Researchers can search, access, aggregate, and analyze these data through the Kids First Data Resource Portal. Additionally, by using cloud-based individual workspaces in CAVATICA, a data analysis and sharing computation platform, researchers can cross-analyze Kids First data with data from other efforts, such as NCI’s TARGET program and consortia-based datasets like the Children’s Brain Tumor Tissue Consortium (CBTTC).\\nKids First is made available on AWS via the [NIH STRIDES Initiative](https://aws.amazon.com/blogs/publicsector/aws-and-national-institutes-of-health-collaborate-to-accelerate-discoveries-with-strides-initiative/).\\n\"}\n",
      "{'name': 'Rapid7 FDNS ANY Dataset', 'description': 'This dataset has been deprecated. Please see this [Rapid7 blog post](https://www.rapid7.com/blog/post/2022/02/10/evolving-how-we-share-rapid7-research-data-2/) for details.\\n'}\n",
      "{'name': 'Tabula Sapiens', 'description': 'Tabula Sapiens will be a benchmark, first-draft human cell atlas of two million cells from 25 organs of eight normal human subjects. \\nTaking the organs from the same individual controls for genetic background, age, environment, and epigenetic effects, and allows detailed analysis and comparison of cell types that are shared between tissues. \\nOur work creates a detailed portrait of cell types as well as their distribution and variation in gene expression across tissues and within the endothelial, epithelial, stromal and immune compartments. \\nA critical factor in the Tabula projects is our large collaborative network of PI’s with deep expertise at preparation of diverse organs, enabling all organs from a subject to be successfully processed within a single day. \\nTabula Sapiens leverages our network of human tissue experts and a close collaboration with a Donor Network West, a not-for-profit organ procurement organization. \\nWe use their experience to balance and assign cell types from each tissue compartment and optimally mix high-quality plate-seq data and high-volume droplet-based data to provide a broad and deep benchmark atlas. \\nOur goal is to make sequence data rapidly and broadly available to the scientific community as a community resource. Before you use our data, please take note of our Data Release Policy below.</br></br>\\n\\nData Release Policy</br></br>\\n\\nOur goal is to make sequence data rapidly and broadly available to the scientific community as a community resource. It is our intention to publish the work of this project in a timely fashion, and we welcome collaborative interaction on the project and analyses. \\nHowever, considerable investment was made in generating these data and we ask that you respect rights of first publication and acknowledgment as outlined in the [Toronto agreement](https://www.nature.com/articles/461168a). \\nBy accessing these data, you agree not to publish any articles containing analyses of genes, cell types or transcriptomic data on a whole atlas or tissue scale prior to initial publication by the Tabula Sapiens Consortium and its collaborating scientists. \\nIf you wish to make use of restricted data for publication or are interested in collaborating on the analyses of these data, please use email or contact form available from the portal. \\nRedistribution of these data should include the full text of the data use policy.\\n'}\n",
      "{'name': 'KITTI Vision Benchmark Suite', 'description': 'Dataset and benchmarks for computer vision research in the context of autonomous driving. The dataset has been recorded in and around the city of Karlsruhe, Germany using the mobile platform AnnieWay (VW station wagon) which has been equipped with several RGB and monochrome cameras, a Velodyne HDL 64 laser scanner as well as an accurate RTK corrected GPS/IMU localization unit. The dataset has been created for computer vision and machine learning research on stereo, optical flow, visual odometry, semantic segmentation, semantic instance segmentation, road segmentation, single image depth prediction, depth map completion, 2D and 3D object detection and object tracking. In addition, several raw data recordings are provided. The datasets are captured by driving around the mid-size city of Karlsruhe, in rural areas and on highways. Up to 15 cars and 30 pedestrians are visible per image.'}\n",
      "{'name': 'SiPeCaM (Sitios Permanentes de la Calibración y Monitoreo de la Biodiversidad)', 'description': 'The SiPeCaM goal is to create a data source that allows to evaluate changes in the biodiversity state, considering key aspect of how does the ecosystem behaves.'}\n",
      "{'name': 'Helpful Sentences from Reviews', 'description': 'A collection of sentences extracted from customer reviews labeled with their helpfulness score.'}\n",
      "{'name': 'Variant Effect Predictor (VEP) and the Loss-Of-Function Transcript Effect Estimator (LOFTEE) Plugin', 'description': 'VEP determines the effect of genetic variants (SNPs, insertions, deletions, CNVs or structural variants) on genes, transcripts, and protein sequence, as well as regulatory regions. The European Bioinformatics Institute produces the VEP tool/db and releases updates every 1 - 6 months. The latest release contains 267 genomes from 232 species containing 5567663 protein coding genes. This dataset hosts the last 5 releases for human, rat, and zebrafish. Also, it hosts the required reference files for the Loss-Of-Function Transcript Effect Estimator (LOFTEE) plugin as it is commonly used with VEP.'}\n",
      "{'name': 'Defense Meteorology Satellite Program (DMSP) Auroral Particle Flux', 'description': 'The United States Air Force (USAF) Defense Meteorological Satellite Program (DMSP) SSJ precipitating particle instrument measures in-situ total flux and energy distribution of electrons and ions at low earth orbit. These precipitating particles are of interest for space weather operations and research, in part because they produce aurora during normal and very strong geomagnetic storms. This dataset contains both sensor-level raw data (as detailed in Redmon et al. 2017) and a high-level machine-learning-ready data product.'}\n",
      "{'name': 'COVID-19 Genome Sequence Dataset', 'description': 'A centralized sequence repository for all records containing sequence associated with the novel corona virus (SARS-CoV-2) submitted to the National Center for Biotechnology Information (NCBI) Sequence Read Archive (SRA). Included are both the original sequences submitted by the principal investigator as well as SRA-processed sequences that require the SRA Toolkit for analysis. Additionally, submitter provided metadata included in associated BioSample and BioProject records is available alongside NCBI calculated data, such k-mer based taxonomy analysis results, contiguous assemblies (contigs) and associated statistics such as contig length, blast results for the assembled contigs, contig annotation, blast databases of contigs and their annotated peptides, and VCF files generated for each record relative to the SARS-CoV-2 RefSeq record. Finally, metadata is additionally made available in parquet format to facilitate search and filtering using the AWS Athena Service.'}\n",
      "{'name': 'HIRLAM Weather Model', 'description': 'HIRLAM (High Resolution Limited Area Model) is an operational synoptic and mesoscale weather prediction model managed by the Finnish Meteorological Institute.'}\n",
      "{'name': 'Covid Job Impacts - US Hiring Data Since March 1 2020', 'description': 'This dataset provides daily updates on the volume of US job listings filtered by geography industry job family and role; normalized to pre-covid levels.\\n\\nThese data files feed the business intelligence visuals at covidjobimpacts.greenwich.hr, a public-facing site hosted by Greenwich.HR and OneModel Inc.\\nData is derived from online job listings tracked continuously, calculated daily and published nightly.  On average data from 70% of all new US jobs are captured,\\nand the dataset currently contains data from 3.3 million hiring organizations.\\n\\nData for each filter segment is represented as the 7-day average of new job listings for a specific date, expressed as a percentage of the corresponding value \\non March 1, 2020.\\n'}\n",
      "{'name': 'Oxford Nanopore Technologies Benchmark Datasets', 'description': 'The ont-open-data registry provides reference sequencing data from Oxford Nanopore Technologies to support, 1) Exploration of the characteristics of nanopore sequence data. 2) Assessment and reproduction of performance benchmarks 3) Development of tools and methods. The data deposited showcases DNA sequences from a representative subset of sequencing chemistries. The datasets correspond to publicly-available reference samples (e.g. Genome In A Bottle reference cell lines). Raw data are provided with metadata and scripts to describe sample and data provenance.\\n'}\n",
      "{'name': 'NOAA Space Weather Forecast and Observation Data', 'description': \"Space weather forecast and observation data is collected and disseminated by NOAA’s Space Weather Prediction Center (SWPC) in Boulder, CO. SWPC produces forecasts for multiple space weather phenomenon types and the resulting impacts to Earth and human activities.  A variety of products are available that provide these forecast expectations, and their respective measurements, in formats that range from detailed technical forecast discussions to NOAA Scale values to simple bulletins that give information in laymen's terms. Forecasting is the prediction of future events, based on analysis and modeling of the past and present conditions of the environment you are interested in. In Space Weather, persistence and recurrence of active regions on the sun over the 27-day solar rotational period play an important role in accurately forecasting the space environment.\\n\"}\n",
      "{'name': 'Sentinel-2 Cloud-Optimized GeoTIFFs', 'description': \"The [Sentinel-2 mission](https://sentinel.esa.int/web/sentinel/missions/sentinel-2) is\\na land monitoring constellation of two satellites that provide high resolution\\noptical imagery and provide continuity for the current SPOT and Landsat missions.\\nThe mission provides a global coverage of the Earth's land surface every 5 days,\\nmaking the data of great use in ongoing studies.\\nThis dataset is the same as the [Sentinel-2](https://registry.opendata.aws/sentinel-2/)\\ndataset, except the JP2K files were converted into Cloud-Optimized GeoTIFFs (COGs).\\nAdditionally, SpatioTemporal Asset Catalog metadata has were in a JSON file\\nalongside the data, and a STAC API called [Earth-search](https://earth-search.aws.element84.com/v0)\\nis freely available to search the archive. This dataset contains all of the scenes in the\\noriginal Sentinel-2 Public Dataset and will grow as that does.\\nL2A data are available from April 2017 over wider Europe region and globally since December 2018.\\n\"}\n",
      "{'name': 'CMIP6 GCMs downscaled using WRF', 'description': 'High-resolution historical and future climate simulations from 1980-2100'}\n",
      "{'name': 'ESA WorldCover', 'description': 'The European Space Agency (ESA) WorldCover product provides global land cover maps for 2020 & 2021 at 10 m resolution based on Copernicus Sentinel-1 and Sentinel-2 data. The WorldCover product comes with 11 land cover classes and has been generated in the framework of the ESA WorldCover project, part of the 5th Earth Observation Envelope Programme (EOEP-5) of the European Space Agency. A first version of the product (v100), containing the 2020 map was released in October 2021. The 2021 map was released in October 2022 using an improved algorithm (v200). The WorldCover 2020 and 2021 maps were generated with different algorithm versions and therefore changes between the maps should be treated with caution as these contain both real land cover changes as well as changes due to the used algorithms.'}\n",
      "{'name': '1000 Genomes', 'description': 'The 1000 Genomes Project is an international collaboration which has established the most detailed catalogue of human genetic variation, including SNPs, structural variants, and their haplotype context. The final phase of the project sequenced more than 2500 individuals from 26 different populations around the world and produced an integrated set of phased haplotypes with more than 80 million variants for these individuals.'}\n",
      "{'name': 'Digital Earth Africa Sentinel-1 Radiometrically Terrain Corrected', 'description': 'DE Africa’s Sentinel-1 backscatter product is developed to be compliant with the CEOS Analysis Ready Data for Land (CARD4L) specifications.\\nThe Sentinel-1 mission, composed of a constellation of two C-band Synthetic Aperture Radar (SAR) satellites, are operated by European Space Agency (ESA) as part of the Copernicus Programme. The mission currently collects data every 12 days over Africa at a spatial resolution of approximately 20 m.\\nRadar backscatter measures the amount of microwave radiation reflected back to the sensor from the ground surface. This measurement is sensitive to surface roughness, moisture content and viewing geometry. DE Africa provides Sentinel-1 backscatter as Radiometrically Terrain Corrected (RTC) gamma-0 (γ0) where variation due to changing observation geometries has been mitigated.\\nThe dual polarisation backcastter time series can be used in applications for forests, agriculture, wetlands and land cover classification. SAR’s ability to ‘see through’ clouds makes it critical for mapping and monitoring land cover changes in the wet tropics.\\n'}\n",
      "{'name': 'Automatic Speech Recognition (ASR) Error Robustness', 'description': 'Sentence classification datatasets with ASR Errors.'}\n",
      "{'name': 'NOAA National Water Model Short-Range Forecast', 'description': 'The National Water Model (NWM) is a water resources model that simulates and forecasts water\\nbudget variables, including snowpack, evapotranspiration, soil moisture and streamflow, over\\nthe entire continental United States (CONUS). The model, launched in August 2016, is designed\\nto improve the ability of NOAA to meet the needs of its stakeholders (forecasters, emergency\\nmanagers, reservoir operators, first responders, recreationists, farmers, barge operators, and\\necosystem and floodplain managers) by providing expanded accuracy, detail, and frequency of water\\ninformation. It is operated by NOAA’s Office of Water Prediction. This bucket contains a four-week\\nrollover of the Short Range Forecast model output and the corresponding forcing data for the\\nmodel. The model is forced with meteorological data from the High Resolution Rapid Refresh (HRRR)\\nand the Rapid Refresh (RAP) models. The Short Range Forecast configuration cycles hourly and produces\\nhourly deterministic forecasts of streamflow and hydrologic states out to 18 hours.\\n'}\n",
      "{'name': 'Binding DB - Data Lakehouse Ready', 'description': \"This a parquet representation of The Binding Database's [Full BindingDB Database Dump](https://www.bindingdb.org/bind/chemsearch/marvin/SDFdownload.jsp?all_download=yes) that you can query straight from Athena in under 60 seconds (no Oracle database required). The Binding Database projects aims to make experimental data on the noncovalent association of molecules in solution searchable via the world wide web. The initial focus is on biomolecular systems, but data on host-guest and supramolecular systems are also important and being included over time. It is expected that the enhanced access to data provided by this resource will facilitate drug-discovery, the design of self-assembling systems, and the development of predictive computer models of binding. The Binding Database is based at the Skaggs School of Pharmacy and Pharmaceutical Sciences at the University of California, San Diego, La Jolla, CA.\"}\n",
      "{'name': 'Radiant MLHub', 'description': \"Radiant MLHub is an open library for geospatial training data that hosts datasets generated by [Radiant Earth Foundation](https://www.radiant.earth/)'s team as well as other training data catalogs contributed by Radiant Earth’s partners. Radiant MLHub is open to anyone to access, store, register and/or share their training datasets for high-quality Earth observations. All of the training datasets are stored using a [SpatioTemporal Asset Catalog (STAC)](https://stacspec.org/) compliant catalog and exposed through a common API. Training datasets include pairs of imagery and labels for different types of machine learning problems including image classification, object detection, and semantic segmentation. Labels are generated from ground reference data and/or image annotation.\"}\n",
      "{'name': 'Digital Earth Africa ALOS PALSAR, ALOS-2 PALSAR-2 and JERS-1', 'description': 'The ALOS/PALSAR annual mosaic is a global 25 m resolution dataset that combines data from many images captured by JAXA’s PALSAR and PALSAR-2 sensors on ALOS-1 and ALOS-2 satellites respectively. This product contains radar measurement in L-band and in HH and HV polarizations. It has a spatial resolution of 25 m and is available annually for 2007 to 2010 (ALOS/PALSAR) and 2015 to 2020 (ALOS-2/PALSAR-2).\\nThe JERS annual mosaic is generated from images acquired by the SAR sensor on the Japanese Earth Resources Satellite-1 (JERS-1) satellite. This product contains radar measurement in L-band and HH polarization. It has a spatial resolution of 25 m and is available for 1996.\\nThis mosaic data is part of a global dataset provided by the Japan Aerospace Exploration Agency (JAXA) Earth Observation Research Center.\\n'}\n",
      "{'name': 'Community Earth System Model Large Ensemble (CESM LENS)', 'description': 'The Community Earth System Model (CESM) Large Ensemble Numerical Simulation (LENS) dataset includes a 40-member ensemble of climate simulations for the period 1920-2100 using historical data (1920-2005) or assuming the RCP8.5 greenhouse gas concentration scenario (2006-2100), as well as longer control runs based on pre-industrial conditions. The data comprise both surface (2D) and volumetric (3D) variables in the atmosphere, ocean, land, and ice domains. The total data volume of the original dataset is ~500TB, which has traditionally been stored as ~150,000 individual CF/NetCDF files on disk or magnetic tape made available through the NCAR Climate Data Gateway for download or via web services.\\n\\n   NCAR has copied a subset (currently ~70 TB) of CESM LENS data to Amazon S3 as part of the AWS Public Datasets Program. To optimize for large-scale analytics we have represented the data as ~275 Zarr stores format accessible through the Python Xarray library. Each Zarr store contains a single physical variable for a given model run type and temporal frequency (monthly, daily, 6-hourly).\\n'}\n",
      "{'name': 'NOAA Geostationary Operational Environmental Satellites (GOES) 16, 17 & 18', 'description': 'NEW GOES-18 Data!!! GOES-18 is now provisional and data has began streaming. Data files will be available between Provisional and the Operational Declaration of the satellite, however, data will have the caveat GOES-18 Preliminary, Non-Operational Data. The exception is during the interleave period when ABI Radiances and Cloud and Moisture Imagery data will be shared operationally via the NOAA Open Data Dissemination Program.\\n<br/>\\n<br/> \\nGOES satellites (GOES-16, GOES-17, & GOES-18) provide continuous weather imagery and\\nmonitoring of meteorological and space environment data across North America.\\nGOES satellites provide the kind of continuous monitoring necessary for\\nintensive data analysis. They hover continuously over one position on the surface.\\nThe satellites orbit high enough to allow for a full-disc view of the Earth. Because\\nthey stay above a fixed spot on the surface, they provide a constant vigil for the\\natmospheric \"triggers\" for severe weather conditions such as tornadoes, flash floods,\\nhailstorms, and hurricanes. When these conditions develop, the GOES satellites are able\\nto monitor storm development and track their movements. SUVI products available in both NetCDF and FITS. \\n'}\n",
      "{'name': 'OpenCell on AWS', 'description': 'The OpenCell project is a proteome-scale effort to measure the localization and interactions of human proteins\\nusing high-throughput genome engineering to endogenously tag thousands of proteins in the human proteome.\\nThis dataset consists of the raw confocal fluorescence microscopy images for all tagged cell lines in the OpenCell library. \\nThese images can be interpreted both individually, to determine the localization of particular proteins of interest,\\nand in aggregate, by training machine learning models to classify or quantify subcellular localization patterns. \\n'}\n",
      "{'name': 'Image classification - fast.ai datasets', 'description': 'Some of the most important datasets for image classification research, including\\nCIFAR 10 and 100, Caltech 101, MNIST, Food-101, Oxford-102-Flowers, Oxford-IIIT-Pets,\\nand Stanford-Cars.  This is part of the fast.ai datasets collection hosted by\\nAWS for convenience of fast.ai students. See documentation link for citation and\\nlicense details for each dataset.\\n'}\n",
      "{'name': '1000 Genomes Phase 3 Reanalysis with DRAGEN 3.5 - Data Lakehouse Ready', 'description': 'The 1000 Genomes Project is an international collaboration which has established the most detailed catalogue of human genetic variation, including SNPs, structural variants, and their haplotype context. There were a total of 3202 individuals sequenced as part of Phase 3 of this project. The high coverage samples were processed using the Illumina DRAGEN v3.5.7b pipeline and are available at s3://1000genomes-dragen/. This dataset contains the VCFs transformed to Parquet/ORC in 3 different schemas - partitioned by samples, partitioned by chromosome and a nested data format. These representations of the 1000 Genomes DRAGEN data are stored in Parquet/ORC format and can be queried through [Amazon Athena](https://aws.amazon.com/athena/?whats-new-cards.sort-by=item.additionalFields.postDateTime&whats-new-cards.sort-order=desc). To add these tables to your Glue Data Catalog and for sample queries on this dataset, please refer to the link in our Documentation.'}\n",
      "{'name': 'NOAA Real-Time Mesoscale Analysis (RTMA)', 'description': 'The Real-Time Mesoscale Analysis (RTMA) is a NOAA National Centers For Environmental Prediction (NCEP) high-spatial and temporal resolution analysis/assimilation system for near-surf ace weather conditions. Its main component is the NCEP/EMC Gridpoint Statistical Interpolation (GSI) system applied in two-dimensional variational mode to assimilate conventional and satellite-derived observations.<br/><br/>\\nThe RTMA was developed to support NDFD operations and provide field forecasters with high quality analyses for nowcasting, situational awareness, and forecast verification purposes. The system produces hourly analyses at 2. 5 km resolution for the Conus NDFD grid, 3 km for the Alaska NDFD grid and 2.5 km for the Hawaii, Puerto-Rico and Guam NDFD grids.<br/><br/>\\nData is available from the start of 2019 until present.\\n'}\n"
     ]
    }
   ],
   "source": [
    "# add data to weaviate\n",
    "with client.batch as batch:\n",
    "    for dataset in datasets:\n",
    "        print(dataset)\n",
    "        batch.add_data_object(dataset, class_name='Dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query\n",
    "result = client.query.get(\"Dataset\", [\"name\", \"description\"]).do()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'description': \"Imagery acquired\\nby the China-Brazil Earth Resources Satellite (CBERS), 4 and 4A.\\nThe\\nimage files are recorded and processed by Instituto Nacional de Pesquisas\\nEspaciais (INPE) and are converted to Cloud Optimized Geotiff\\nformat in order to optimize its use for cloud based applications.\\nContains all CBERS-4 MUX, AWFI, PAN5M and\\nPAN10M scenes acquired since\\nthe start of the satellite mission and is daily updated with\\nnew scenes.\\nCBERS-4 PAN5M and PAN10M starting from 05-2022 are temporarily not ingested\\ndue to an error in the bands identification on INPE's catalog.\\nCBERS-4A MUX Level 4 (Orthorectified) scenes are being\\ningested starting from 04-13-2021. CBERS-4A WFI Level 4 (Orthorectified)\\nscenes are being ingested starting from 10-12-2022.\\n\",\n",
       " 'name': 'CBERS on AWS'}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['data']['Get']['Dataset'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"\"\"\n",
    "# {\n",
    "#     Get {\n",
    "#         Dataset {\n",
    "#             name,\n",
    "#             description\n",
    "#         }\n",
    "#     }\n",
    "# }\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"\"\"\n",
    "# {\n",
    "#     Get {\n",
    "#         Dataset (\n",
    "#             nearText: {\n",
    "#                 concepts: [\"whales\"]\n",
    "#                 certainty: 0.5\n",
    "#                 moveAwayFrom: {\n",
    "#                     concepts: [\"satellites\", \"European Space Agency\", \"Sentinel\"]\n",
    "#                     force: 0.45\n",
    "#                 }\n",
    "#                 moveTo: {\n",
    "#                 concepts: [\"oceans\", \"marine\", \"NOAA\", \"water\"],\n",
    "#                 force: 0.85\n",
    "#                 }\n",
    "#             }\n",
    "#         ) {\n",
    "#             name\n",
    "#             description\n",
    "#             _additional {\n",
    "#                 certainty # only works if distance==cosine\n",
    "#                 distance  # always works\n",
    "#             }\n",
    "#         }\n",
    "#     }\n",
    "# }\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# genomics datasets\n",
    "query = \"\"\"\n",
    "{\n",
    "    Get {\n",
    "        Dataset (\n",
    "            nearText: {\n",
    "                concepts: [\"genomics\", \"metabolism\", \"proteins\", \"bioinformatics\", \"metagenomics\"]\n",
    "                certainty: 0.5\n",
    "            }\n",
    "        ) {\n",
    "            name\n",
    "            description\n",
    "            _additional {\n",
    "                certainty\n",
    "                distance \n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# legal datasets\n",
    "query = \"\"\"\n",
    "{\n",
    "    Get {\n",
    "        Dataset (\n",
    "            nearText: {\n",
    "                concepts: [\"legal\", \"law\", \"court\", \"judicial\", \"judiciary\", \"finance\", \"corporate\", \"business\"]\n",
    "                certainty: 0.6\n",
    "            }\n",
    "        ) {\n",
    "            name\n",
    "            description\n",
    "            _additional {\n",
    "                certainty\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = client.query.raw(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result['data']['Get']['Dataset'])\n",
    "# result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result['data']['Get']['Dataset'][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Amazon-PQA\n",
      "Description: Amazon product questions and their answers, along with the public product information.\n",
      "Certainty: 0.6178012490272522\n",
      "\n",
      "\n",
      "\n",
      "Name: Legal Entity Identifier (LEI) and Legal Entity Reference Data (LE-RD)\n",
      "Description: The Legal Entity Identifier (LEI) is a 20-character, alpha-numeric code based on the ISO 17442 standard developed by the International Organization for Standardization (ISO). It connects to key reference information that enables clear and unique identification of legal entities participating in financial transactions. Each LEI contains information about an entity’s ownership structure and thus answers the questions of 'who is who’ and ‘who owns whom’. Simply put, the publicly available LEI data pool can be regarded as a global directory, which greatly enhances transparency in the global marketplace. The Financial Stability Board (FSB) has reiterated that global LEI adoption underpins “multiple financial stability objectives” such as improved risk management in firms as well as better assessment of micro and macro prudential risks. As a result, it promotes market integrity while containing market abuse and financial fraud. Last but not least, LEI rollout “supports higher quality and accuracy of financial data overall”. The publicly available LEI data pool is a unique key to standardized information on legal entities globally. The data is registered and regularly verified according to protocols and procedures established by the Regulatory Oversight Committee. In cooperation with its partners in the Global LEI System, the Global Legal Entity Identifier Foundation (GLEIF) continues to focus on further optimizing the quality, reliability and usability of LEI data, empowering market participants to benefit from the wealth of information available with the LEI population. The drivers of the LEI initiative, i.e. the Group of 20, the FSB and many regulators around the world, have emphasized the need to make the LEI a broad public good. The Global LEI Index, made available by GLEIF, greatly contributes to meeting this objective. It puts the complete LEI data at the disposal of any interested party, conveniently and free of charge. The benefits for the wider business community to be generated with the Global LEI Index grow in line with the rate of LEI adoption. To maximize the benefits of entity identification across financial markets and beyond, firms are therefore encouraged to engage in the process and get their own LEI. Obtaining an LEI is easy. Registrants simply contact their preferred business partner from the list of LEI issuing organizations available on the GLEIF website.\n",
      "\n",
      "Certainty: 0.616974264383316\n",
      "\n",
      "\n",
      "\n",
      "Name: Clinical Proteomic Tumor Analysis Consortium 3 (CPTAC-3)\n",
      "Description: The Clinical Proteomic Tumor Analysis Consortium (CPTAC) is a national effort to accelerate the\n",
      "understanding of the molecular basis of cancer through the application of large-scale proteome and\n",
      "genome analysis, or proteogenomics. CPTAC-3 is the Phase III of the CPTAC Initiative. The dataset\n",
      "contains open RNA-Seq Gene Expression Quantification data.\n",
      "\n",
      "Certainty: 0.6041261255741119\n",
      "\n",
      "\n",
      "\n",
      "Name: Clinical Proteomic Tumor Analysis Consortium 2 (CPTAC-2)\n",
      "Description: The Clinical Proteomic Tumor Analysis Consortium (CPTAC) is a national effort to accelerate the\n",
      "understanding of the molecular basis of cancer through the application of large-scale proteome and\n",
      "genome analysis, or proteogenomics. CPTAC-2 is the Phase II of the CPTAC Initiative (2011-2016).\n",
      "Datasets contain open RNA-Seq Gene Expression Quantification, miRNA-Seq Isoform Expression\n",
      "Quantification, and miRNA Expression Quantification data.\n",
      "\n",
      "Certainty: 0.6027506291866302\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for result in result['data']['Get']['Dataset'][:25]:\n",
    "    print(f'Name: {result[\"name\"]}')\n",
    "    print(f'Description: {result[\"description\"]}')\n",
    "    print(f'Certainty: {result[\"_additional\"][\"certainty\"]}')\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('aws-open-data-registry-neural-search-4l16sXU8-py3.10')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 (main, Oct 13 2022, 10:17:43) [Clang 14.0.0 (clang-1400.0.29.102)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ddda627f0aa5a2016e69119ee9732a94264c055a92310a2df2f2b6f4c7282b9d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
